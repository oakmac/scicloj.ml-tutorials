<!DOCTYPE html>
<html lang="en-US">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <link href="https://stackpath.bootstrapcdn.com/bootswatch/4.5.0/sandstone/bootstrap.min.css" rel="stylesheet" type="text/css">
        <link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.6.0/styles/solarized-light.min.css" rel="stylesheet" type="text/css">
        <link href="https://cdnjs.cloudflare.com/ajax/libs/ag-grid/24.0.0/styles/ag-grid.min.css" rel="stylesheet" type="text/css">
        <link href="https://cdnjs.cloudflare.com/ajax/libs/ag-grid/24.0.0/styles/ag-theme-balham.min.css" rel="stylesheet" type="text/css">
        <link href="https://unpkg.com/leaflet@1.6.0/dist/leaflet.css" rel="stylesheet" type="text/css">

        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
        <!-- The loading of KaTeX is deferred to speed up page rendering -->
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
    </head>
    <body>
        <p id="loading">Loading ...</p>
        <div id="app"></div>
    </body>
    <script id="state" type="text">"{:options {:reverse-notes? false, :header? false, :notes-in-cards? false, :initially-collapse? false, :auto-scroll? false, :port 5678, :custom-header [:div {:style {:font-style \"italic\", :font-family \"\\\"Lucida Console\\\", Courier, monospace\"}} \"(notespace)\" [:p \"Mon Oct 04 14:06:13 CEST 2021\"] nil [:hr]], :custom-footer [:div [:hr] [:hr]]}, :ids [\"1178\" \"1180\" \"1182\" \"1184\" \"1186\" \"1188\" \"1190\" \"1192\" \"1194\" \"1196\" \"1198\" \"1200\" \"1202\" \"1204\" \"1206\" \"1208\" \"1210\" \"1212\" \"1214\" \"1216\"], :id->content {\"1186\" [:div [:p] [:div [:p/code {:code \"(require '[scicloj.ml.core :as ml]\\n         '[scicloj.ml.metamorph :as mm]\\n         '[scicloj.ml.dataset :as ds]\\n         '[tech.v3.dataset.tensor :as dst]\\n         '[scicloj.sklearn-clj]\\n         '[scicloj.metamorph.ml.toydata :as toydata]\\n         )\", :bg-class \"bg-light\"}]] nil nil], \"1188\" [:div [:p] [:div [:p/code {:code \"(def ds (-> (dst/tensor->dataset [[0 0 0 ] [1 1 1 ] [2 2 2]])\\n\\n\\n\\n            ))\", :bg-class \"bg-light\"}]] nil nil], \"1204\" [:div [:p] [:div [:p/code {:code \"(def iris\\n  (ds/dataset\\n   \\\"https://raw.githubusercontent.com/scicloj/metamorph.ml/main/test/data/iris.csv\\\" {:key-fn keyword})\\n  )\", :bg-class \"bg-light\"}]] nil nil], \"1178\" [:div [:p] [:div [:p/code {:code \"(comment\\n  (note/init-with-browser)\\n  (note/eval-this-notespace)\\n  (note/reread-this-notespace)\\n  (note/render-static-html \\\"docs/userguide-sklearnclj.html\\\")\\n  (note/init)\\n\\n  )\", :bg-class \"bg-light\"}]] nil [:p/code {:code \"nil\\n\"}]], \"1214\" [:div [:p] nil nil [:p/markdown \"## Sklearn regression\"]], \"1212\" [:div [:p] nil nil ([:div [:h3 \":sklearn.classification/ada-boost-classifier\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [5 2]:\\n\\n|           :name | :default |\\n|-----------------|----------|\\n|      :algorithm |  SAMME.R |\\n| :base-estimator |          |\\n|  :learning-rate |    1.000 |\\n|   :n-estimators |       50 |\\n|   :random-state |          |\\n\"]]] [:span [:p/markdown \"An AdaBoost classifier.\\n\\n    An AdaBoost [1] classifier is a meta-estimator that begins by fitting a\\n    classifier on the original dataset and then fits additional copies of the\\n    classifier on the same dataset but where the weights of incorrectly\\n    classified instances are adjusted such that subsequent classifiers focus\\n    more on difficult cases.\\n\\n    This class implements the algorithm known as AdaBoost-SAMME [2].\\n\\n    Read more in the User Guide: `adaboost`.\\n\\n    *Added in 0.14*\\n\\n    Parameters\\n    ----------\\n- `base_estimator`: object, default=None\\n        The base estimator from which the boosted ensemble is built.\\n        Support for sample weighting is required, as well as proper\\n        ``classes_`` and ``n_classes_`` attributes. If ``None``, then\\n        the base estimator is ``DecisionTreeClassifier(max_depth=1)``.\\n\\n- `n_estimators`: int, default=50\\n        The maximum number of estimators at which boosting is terminated.\\n        In case of perfect fit, the learning procedure is stopped early.\\n\\n- `learning_rate`: float, default=1.\\n        Learning rate shrinks the contribution of each classifier by\\n        ``learning_rate``. There is a trade-off between ``learning_rate`` and\\n        ``n_estimators``.\\n\\n- `algorithm`: {'SAMME', 'SAMME.R'}, default='SAMME.R'\\n        If 'SAMME.R' then use the SAMME.R real boosting algorithm.\\n        ``base_estimator`` must support calculation of class probabilities.\\n        If 'SAMME' then use the SAMME discrete boosting algorithm.\\n        The SAMME.R algorithm typically converges faster than SAMME,\\n        achieving a lower test error with fewer boosting iterations.\\n\\n- `random_state`: int or RandomState, default=None\\n        Controls the random seed given at each `base_estimator` at each\\n        boosting iteration.\\n        Thus, it is only used when `base_estimator` exposes a `random_state`.\\n        Pass an int for reproducible output across multiple function calls.\\n        See :term:`Glossary <random_state>`.\\n\\n    Attributes\\n    ----------\\n- `base_estimator_`: estimator\\n        The base estimator from which the ensemble is grown.\\n\\n- `estimators_`: list of classifiers\\n        The collection of fitted sub-estimators.\\n\\n- `classes_`: ndarray of shape (n_classes,)\\n        The classes labels.\\n\\n- `n_classes_`: int\\n        The number of classes.\\n\\n- `estimator_weights_`: ndarray of floats\\n        Weights for each estimator in the boosted ensemble.\\n\\n- `estimator_errors_`: ndarray of floats\\n        Classification error for each estimator in the boosted\\n        ensemble.\\n\\n- `feature_importances_`: ndarray of shape (n_features,)\\n        The impurity-based feature importances if supported by the\\n        ``base_estimator`` (when based on decision trees).\\n\\n        Warning: impurity-based feature importances can be misleading for\\n        high cardinality features (many unique values). See\\n        `sklearn.inspection.permutation_importance` as an alternative.\\n\\n    See Also\\n    --------\\n    AdaBoostRegressor\\n        An AdaBoost regressor that begins by fitting a regressor on the\\n        original dataset and then fits additional copies of the regressor\\n        on the same dataset but where the weights of instances are\\n        adjusted according to the error of the current prediction.\\n\\n    GradientBoostingClassifier\\n        GB builds an additive model in a forward stage-wise fashion. Regression\\n        trees are fit on the negative gradient of the binomial or multinomial\\n        deviance loss function. Binary classification is a special case where\\n        only a single regression tree is induced.\\n\\n    sklearn.tree.DecisionTreeClassifier\\n        A non-parametric supervised learning method used for classification.\\n        Creates a model that predicts the value of a target variable by\\n        learning simple decision rules inferred from the data features.\\n\\n    References\\n    ----------\\n - [1] Y. Freund, R. Schapire, \\\"A Decision-Theoretic Generalization of\\n           on-Line Learning and an Application to Boosting\\\", 1995.\\n\\n - [2] J. Zhu, H. Zou, S. Rosset, T. Hastie, \\\"Multi-class AdaBoost\\\", 2009.\\n\\n    Examples\\n    --------\\n    >>> from sklearn.ensemble import AdaBoostClassifier\\n    >>> from sklearn.datasets import make_classification\\n    >>> X, y = make_classification(n_samples=1000, n_features=4,\\n    ...                            n_informative=2, n_redundant=0,\\n    ...                            random_state=0, shuffle=False)\\n    >>> clf = AdaBoostClassifier(n_estimators=100, random_state=0)\\n    >>> clf.fit(X, y)\\n    AdaBoostClassifier(n_estimators=100, random_state=0)\\n    >>> clf.predict([[0, 0, 0, 0]])\\n    array([1])\\n    >>> clf.score(X, y)\\n    0.983...\\n    \"]] [:hr] [:hr]] [:div [:h3 \":sklearn.classification/bagging-classifier\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [11 2]:\\n\\n|               :name | :default |\\n|---------------------|----------|\\n|          :bootstrap |     true |\\n| :bootstrap-features |    false |\\n|             :n-jobs |          |\\n|       :random-state |          |\\n|          :oob-score |    false |\\n|     :base-estimator |          |\\n|       :max-features |    1.000 |\\n|         :warm-start |    false |\\n|       :n-estimators |       10 |\\n|        :max-samples |    1.000 |\\n|            :verbose |        0 |\\n\"]]] [:span [:p/markdown \"A Bagging classifier.\\n\\n    A Bagging classifier is an ensemble meta-estimator that fits base\\n    classifiers each on random subsets of the original dataset and then\\n    aggregate their individual predictions (either by voting or by averaging)\\n    to form a final prediction. Such a meta-estimator can typically be used as\\n    a way to reduce the variance of a black-box estimator (e.g., a decision\\n    tree), by introducing randomization into its construction procedure and\\n    then making an ensemble out of it.\\n\\n    This algorithm encompasses several works from the literature. When random\\n    subsets of the dataset are drawn as random subsets of the samples, then\\n    this algorithm is known as Pasting [1]_. If samples are drawn with\\n    replacement, then the method is known as Bagging [2]_. When random subsets\\n    of the dataset are drawn as random subsets of the features, then the method\\n    is known as Random Subspaces [3]_. Finally, when base estimators are built\\n    on subsets of both samples and features, then the method is known as\\n    Random Patches [4]_.\\n\\n    Read more in the User Guide: `bagging`.\\n\\n    *Added in 0.15*\\n\\n    Parameters\\n    ----------\\n- `base_estimator`: object, default=None\\n        The base estimator to fit on random subsets of the dataset.\\n        If None, then the base estimator is a decision tree.\\n\\n- `n_estimators`: int, default=10\\n        The number of base estimators in the ensemble.\\n\\n- `max_samples`: int or float, default=1.0\\n        The number of samples to draw from X to train each base estimator (with\\n        replacement by default, see `bootstrap` for more details).\\n\\n        - If int, then draw `max_samples` samples.\\n        - If float, then draw `max_samples * X.shape[0]` samples.\\n\\n- `max_features`: int or float, default=1.0\\n        The number of features to draw from X to train each base estimator (\\n        without replacement by default, see `bootstrap_features` for more\\n        details).\\n\\n        - If int, then draw `max_features` features.\\n        - If float, then draw `max_features * X.shape[1]` features.\\n\\n- `bootstrap`: bool, default=True\\n        Whether samples are drawn with replacement. If False, sampling\\n        without replacement is performed.\\n\\n- `bootstrap_features`: bool, default=False\\n        Whether features are drawn with replacement.\\n\\n- `oob_score`: bool, default=False\\n        Whether to use out-of-bag samples to estimate\\n        the generalization error.\\n\\n- `warm_start`: bool, default=False\\n        When set to True, reuse the solution of the previous call to fit\\n        and add more estimators to the ensemble, otherwise, just fit\\n        a whole new ensemble. See :term:`the Glossary <warm_start>`.\\n\\n        *Added in 0.17*\\n           *warm_start* constructor parameter.\\n\\n- `n_jobs`: int, default=None\\n        The number of jobs to run in parallel for both `fit` and\\n        `predict`. ``None`` means 1 unless in a\\n        :obj:`joblib.parallel_backend` context. ``-1`` means using all\\n        processors. See :term:`Glossary <n_jobs>` for more details.\\n\\n- `random_state`: int or RandomState, default=None\\n        Controls the random resampling of the original dataset\\n        (sample wise and feature wise).\\n        If the base estimator accepts a `random_state` attribute, a different\\n        seed is generated for each instance in the ensemble.\\n        Pass an int for reproducible output across multiple function calls.\\n        See :term:`Glossary <random_state>`.\\n\\n- `verbose`: int, default=0\\n        Controls the verbosity when fitting and predicting.\\n\\n    Attributes\\n    ----------\\n- `base_estimator_`: estimator\\n        The base estimator from which the ensemble is grown.\\n\\n- `n_features_`: int\\n        The number of features when `fit` is performed.\\n\\n- `estimators_`: list of estimators\\n        The collection of fitted base estimators.\\n\\n- `estimators_samples_`: list of arrays\\n        The subset of drawn samples (i.e., the in-bag samples) for each base\\n        estimator. Each subset is defined by an array of the indices selected.\\n\\n- `estimators_features_`: list of arrays\\n        The subset of drawn features for each base estimator.\\n\\n- `classes_`: ndarray of shape (n_classes,)\\n        The classes labels.\\n\\n- `n_classes_`: int or list\\n        The number of classes.\\n\\n- `oob_score_`: float\\n        Score of the training dataset obtained using an out-of-bag estimate.\\n        This attribute exists only when ``oob_score`` is True.\\n\\n- `oob_decision_function_`: ndarray of shape (n_samples, n_classes)\\n        Decision function computed with out-of-bag estimate on the training\\n        set. If n_estimators is small it might be possible that a data point\\n        was never left out during the bootstrap. In this case,\\n        `oob_decision_function_` might contain NaN. This attribute exists\\n        only when ``oob_score`` is True.\\n\\n    Examples\\n    --------\\n    >>> from sklearn.svm import SVC\\n    >>> from sklearn.ensemble import BaggingClassifier\\n    >>> from sklearn.datasets import make_classification\\n    >>> X, y = make_classification(n_samples=100, n_features=4,\\n    ...                            n_informative=2, n_redundant=0,\\n    ...                            random_state=0, shuffle=False)\\n    >>> clf = BaggingClassifier(base_estimator=SVC(),\\n    ...                         n_estimators=10, random_state=0).fit(X, y)\\n    >>> clf.predict([[0, 0, 0, 0]])\\n    array([1])\\n\\n    References\\n    ----------\\n\\n - [1] L. Breiman, \\\"Pasting small votes for classification in large\\n           databases and on-line\\\", Machine Learning, 36(1), 85-103, 1999.\\n\\n - [2] L. Breiman, \\\"Bagging predictors\\\", Machine Learning, 24(2), 123-140,\\n           1996.\\n\\n - [3] T. Ho, \\\"The random subspace method for constructing decision\\n           forests\\\", Pattern Analysis and Machine Intelligence, 20(8), 832-844,\\n           1998.\\n\\n - [4] G. Louppe and P. Geurts, \\\"Ensembles on Random Patches\\\", Machine\\n           Learning and Knowledge Discovery in Databases, 346-361, 2012.\\n    \"]] [:hr] [:hr]] [:div [:h3 \":sklearn.classification/bernoulli-nb\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [4 2]:\\n\\n|        :name | :default |\\n|--------------|----------|\\n|       :alpha |    1.000 |\\n|    :binarize |    0.000 |\\n| :class-prior |          |\\n|   :fit-prior |     true |\\n\"]]] [:span [:p/markdown \"Naive Bayes classifier for multivariate Bernoulli models.\\n\\n    Like MultinomialNB, this classifier is suitable for discrete data. The\\n    difference is that while MultinomialNB works with occurrence counts,\\n    BernoulliNB is designed for binary/boolean features.\\n\\n    Read more in the User Guide: `bernoulli_naive_bayes`.\\n\\n    Parameters\\n    ----------\\n- `alpha`: float, default=1.0\\n        Additive (Laplace/Lidstone) smoothing parameter\\n        (0 for no smoothing).\\n\\n- `binarize`: float or None, default=0.0\\n        Threshold for binarizing (mapping to booleans) of sample features.\\n        If None, input is presumed to already consist of binary vectors.\\n\\n- `fit_prior`: bool, default=True\\n        Whether to learn class prior probabilities or not.\\n        If false, a uniform prior will be used.\\n\\n- `class_prior`: array-like of shape (n_classes,), default=None\\n        Prior probabilities of the classes. If specified the priors are not\\n        adjusted according to the data.\\n\\n    Attributes\\n    ----------\\n- `class_count_`: ndarray of shape (n_classes)\\n        Number of samples encountered for each class during fitting. This\\n        value is weighted by the sample weight when provided.\\n\\n- `class_log_prior_`: ndarray of shape (n_classes)\\n        Log probability of each class (smoothed).\\n\\n- `classes_`: ndarray of shape (n_classes,)\\n        Class labels known to the classifier\\n\\n- `feature_count_`: ndarray of shape (n_classes, n_features)\\n        Number of samples encountered for each (class, feature)\\n        during fitting. This value is weighted by the sample weight when\\n        provided.\\n\\n- `feature_log_prob_`: ndarray of shape (n_classes, n_features)\\n        Empirical log probability of features given a class, P(x_i|y).\\n\\n- `n_features_`: int\\n        Number of features of each sample.\\n\\n    Examples\\n    --------\\n    >>> import numpy as np\\n    >>> rng = np.random.RandomState(1)\\n    >>> X = rng.randint(5, size=(6, 100))\\n    >>> Y = np.array([1, 2, 3, 4, 4, 5])\\n    >>> from sklearn.naive_bayes import BernoulliNB\\n    >>> clf = BernoulliNB()\\n    >>> clf.fit(X, Y)\\n    BernoulliNB()\\n    >>> print(clf.predict(X[2:3]))\\n    [3]\\n\\n    References\\n    ----------\\n    C.D. Manning, P. Raghavan and H. Schuetze (2008). Introduction to\\n    Information Retrieval. Cambridge University Press, pp. 234-265.\\n    https://nlp.stanford.edu/IR-book/html/htmledition/the-bernoulli-model-1.html\\n\\n    A. McCallum and K. Nigam (1998). A comparison of event models for naive\\n    Bayes text classification. Proc. AAAI/ICML-98 Workshop on Learning for\\n    Text Categorization, pp. 41-48.\\n\\n    V. Metsis, I. Androutsopoulos and G. Paliouras (2006). Spam filtering with\\n    naive Bayes -- Which naive Bayes? 3rd Conf. on Email and Anti-Spam (CEAS).\\n    \"]] [:hr] [:hr]] [:div [:h3 \":sklearn.classification/calibrated-classifier-cv\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [3 2]:\\n\\n|           :name | :default |\\n|-----------------|----------|\\n| :base-estimator |          |\\n|             :cv |          |\\n|         :method |  sigmoid |\\n\"]]] [:span [:p/markdown \"Probability calibration with isotonic regression or logistic regression.\\n\\n    The calibration is based on the :term:`decision_function` method of the\\n    `base_estimator` if it exists, else on :term:`predict_proba`.\\n\\n    Read more in the User Guide: `calibration`.\\n\\n    Parameters\\n    ----------\\n- `base_estimator`: instance BaseEstimator\\n        The classifier whose output need to be calibrated to provide more\\n        accurate `predict_proba` outputs.\\n\\n- `method`: 'sigmoid' or 'isotonic'\\n        The method to use for calibration. Can be 'sigmoid' which\\n        corresponds to Platt's method (i.e. a logistic regression model) or\\n        'isotonic' which is a non-parametric approach. It is not advised to\\n        use isotonic calibration with too few calibration samples\\n        ``(<<1000)`` since it tends to overfit.\\n\\n- `cv`: integer, cross-validation generator, iterable or \\\"prefit\\\", optional\\n        Determines the cross-validation splitting strategy.\\n        Possible inputs for cv are:\\n\\n        - None, to use the default 5-fold cross-validation,\\n        - integer, to specify the number of folds.\\n        - :term:`CV splitter`,\\n        - An iterable yielding (train, test) splits as arrays of indices.\\n\\n        For integer/None inputs, if ``y`` is binary or multiclass,\\n        `sklearn.model_selection.StratifiedKFold` is used. If ``y`` is\\n        neither binary nor multiclass, `sklearn.model_selection.KFold`\\n        is used.\\n\\n        Refer User Guide: `cross_validation` for the various\\n        cross-validation strategies that can be used here.\\n\\n        If \\\"prefit\\\" is passed, it is assumed that `base_estimator` has been\\n        fitted already and all data is used for calibration.\\n\\n        *Changed in 0.22*\\n            ``cv`` default value if None changed from 3-fold to 5-fold.\\n\\n    Attributes\\n    ----------\\n- `classes_`: array, shape (n_classes)\\n        The class labels.\\n\\n- `calibrated_classifiers_`: list (len() equal to cv or 1 if cv == \\\"prefit\\\")\\n        The list of calibrated classifiers, one for each cross-validation fold,\\n        which has been fitted on all but the validation fold and calibrated\\n        on the validation fold.\\n\\n    References\\n    ----------\\n - [1] Obtaining calibrated probability estimates from decision trees\\n           and naive Bayesian classifiers, B. Zadrozny & C. Elkan, ICML 2001\\n\\n - [2] Transforming Classifier Scores into Accurate Multiclass\\n           Probability Estimates, B. Zadrozny & C. Elkan, (KDD 2002)\\n\\n - [3] Probabilistic Outputs for Support Vector Machines and Comparisons to\\n           Regularized Likelihood Methods, J. Platt, (1999)\\n\\n - [4] Predicting Good Probabilities with Supervised Learning,\\n           A. Niculescu-Mizil & R. Caruana, ICML 2005\\n    \"]] [:hr] [:hr]] [:div [:h3 \":sklearn.classification/categorical-nb\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [3 2]:\\n\\n|        :name | :default |\\n|--------------|----------|\\n|       :alpha |    1.000 |\\n| :class-prior |          |\\n|   :fit-prior |     true |\\n\"]]] [:span [:p/markdown \"Naive Bayes classifier for categorical features\\n\\n    The categorical Naive Bayes classifier is suitable for classification with\\n    discrete features that are categorically distributed. The categories of\\n    each feature are drawn from a categorical distribution.\\n\\n    Read more in the User Guide: `categorical_naive_bayes`.\\n\\n    Parameters\\n    ----------\\n- `alpha`: float, default=1.0\\n        Additive (Laplace/Lidstone) smoothing parameter\\n        (0 for no smoothing).\\n\\n- `fit_prior`: bool, default=True\\n        Whether to learn class prior probabilities or not.\\n        If false, a uniform prior will be used.\\n\\n- `class_prior`: array-like of shape (n_classes,), default=None\\n        Prior probabilities of the classes. If specified the priors are not\\n        adjusted according to the data.\\n\\n    Attributes\\n    ----------\\n- `category_count_`: list of arrays of shape (n_features,)\\n        Holds arrays of shape (n_classes, n_categories of respective feature)\\n        for each feature. Each array provides the number of samples\\n        encountered for each class and category of the specific feature.\\n\\n- `class_count_`: ndarray of shape (n_classes,)\\n        Number of samples encountered for each class during fitting. This\\n        value is weighted by the sample weight when provided.\\n\\n- `class_log_prior_`: ndarray of shape (n_classes,)\\n        Smoothed empirical log probability for each class.\\n\\n- `classes_`: ndarray of shape (n_classes,)\\n        Class labels known to the classifier\\n\\n- `feature_log_prob_`: list of arrays of shape (n_features,)\\n        Holds arrays of shape (n_classes, n_categories of respective feature)\\n        for each feature. Each array provides the empirical log probability\\n        of categories given the respective feature and class, ``P(x_i|y)``.\\n\\n- `n_features_`: int\\n        Number of features of each sample.\\n\\n    Examples\\n    --------\\n    >>> import numpy as np\\n    >>> rng = np.random.RandomState(1)\\n    >>> X = rng.randint(5, size=(6, 100))\\n    >>> y = np.array([1, 2, 3, 4, 5, 6])\\n    >>> from sklearn.naive_bayes import CategoricalNB\\n    >>> clf = CategoricalNB()\\n    >>> clf.fit(X, y)\\n    CategoricalNB()\\n    >>> print(clf.predict(X[2:3]))\\n    [3]\\n    \"]] [:hr] [:hr]] [:div [:h3 \":sklearn.classification/complement-nb\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [4 2]:\\n\\n|        :name | :default |\\n|--------------|----------|\\n|       :alpha |    1.000 |\\n| :class-prior |          |\\n|   :fit-prior |     true |\\n|        :norm |    false |\\n\"]]] [:span [:p/markdown \"The Complement Naive Bayes classifier described in Rennie et al. (2003).\\n\\n    The Complement Naive Bayes classifier was designed to correct the \\\"severe\\n    assumptions\\\" made by the standard Multinomial Naive Bayes classifier. It is\\n    particularly suited for imbalanced data sets.\\n\\n    Read more in the User Guide: `complement_naive_bayes`.\\n\\n    *Added in 0.20*\\n\\n    Parameters\\n    ----------\\n- `alpha`: float, default=1.0\\n        Additive (Laplace/Lidstone) smoothing parameter (0 for no smoothing).\\n\\n- `fit_prior`: bool, default=True\\n        Only used in edge case with a single class in the training set.\\n\\n- `class_prior`: array-like of shape (n_classes,), default=None\\n        Prior probabilities of the classes. Not used.\\n\\n- `norm`: bool, default=False\\n        Whether or not a second normalization of the weights is performed. The\\n        default behavior mirrors the implementations found in Mahout and Weka,\\n        which do not follow the full algorithm described in Table 9 of the\\n        paper.\\n\\n    Attributes\\n    ----------\\n- `class_count_`: ndarray of shape (n_classes,)\\n        Number of samples encountered for each class during fitting. This\\n        value is weighted by the sample weight when provided.\\n\\n- `class_log_prior_`: ndarray of shape (n_classes,)\\n        Smoothed empirical log probability for each class. Only used in edge\\n        case with a single class in the training set.\\n\\n- `classes_`: ndarray of shape (n_classes,)\\n        Class labels known to the classifier\\n\\n- `feature_all_`: ndarray of shape (n_features,)\\n        Number of samples encountered for each feature during fitting. This\\n        value is weighted by the sample weight when provided.\\n\\n- `feature_count_`: ndarray of shape (n_classes, n_features)\\n        Number of samples encountered for each (class, feature) during fitting.\\n        This value is weighted by the sample weight when provided.\\n\\n- `feature_log_prob_`: ndarray of shape (n_classes, n_features)\\n        Empirical weights for class complements.\\n\\n- `n_features_`: int\\n        Number of features of each sample.\\n\\n    Examples\\n    --------\\n    >>> import numpy as np\\n    >>> rng = np.random.RandomState(1)\\n    >>> X = rng.randint(5, size=(6, 100))\\n    >>> y = np.array([1, 2, 3, 4, 5, 6])\\n    >>> from sklearn.naive_bayes import ComplementNB\\n    >>> clf = ComplementNB()\\n    >>> clf.fit(X, y)\\n    ComplementNB()\\n    >>> print(clf.predict(X[2:3]))\\n    [3]\\n\\n    References\\n    ----------\\n    Rennie, J. D., Shih, L., Teevan, J., & Karger, D. R. (2003).\\n    Tackling the poor assumptions of naive bayes text classifiers. In ICML\\n    (Vol. 3, pp. 616-623).\\n    https://people.csail.mit.edu/jrennie/papers/icml03-nb.pdf\\n    \"]] [:hr] [:hr]] [:div [:h3 \":sklearn.classification/decision-tree-classifier\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [14 2]:\\n\\n|                     :name |   :default |\\n|---------------------------|------------|\\n| :min-weight-fraction-leaf |      0.000 |\\n|           :max-leaf-nodes |            |\\n|    :min-impurity-decrease |      0.000 |\\n|        :min-samples-split |      2.000 |\\n|                  :presort | deprecated |\\n|                :ccp-alpha |      0.000 |\\n|                 :splitter |       best |\\n|             :random-state |            |\\n|         :min-samples-leaf |          1 |\\n|             :max-features |            |\\n|       :min-impurity-split |            |\\n|                :max-depth |            |\\n|             :class-weight |            |\\n|                :criterion |       gini |\\n\"]]] [:span [:p/markdown \"A decision tree classifier.\\n\\n    Read more in the User Guide: `tree`.\\n\\n    Parameters\\n    ----------\\n- `criterion`: {\\\"gini\\\", \\\"entropy\\\"}, default=\\\"gini\\\"\\n        The function to measure the quality of a split. Supported criteria are\\n        \\\"gini\\\" for the Gini impurity and \\\"entropy\\\" for the information gain.\\n\\n- `splitter`: {\\\"best\\\", \\\"random\\\"}, default=\\\"best\\\"\\n        The strategy used to choose the split at each node. Supported\\n        strategies are \\\"best\\\" to choose the best split and \\\"random\\\" to choose\\n        the best random split.\\n\\n- `max_depth`: int, default=None\\n        The maximum depth of the tree. If None, then nodes are expanded until\\n        all leaves are pure or until all leaves contain less than\\n        min_samples_split samples.\\n\\n- `min_samples_split`: int or float, default=2\\n        The minimum number of samples required to split an internal node:\\n\\n        - If int, then consider `min_samples_split` as the minimum number.\\n        - If float, then `min_samples_split` is a fraction and\\n          `ceil(min_samples_split * n_samples)` are the minimum\\n          number of samples for each split.\\n\\n        *Changed in 0.18*\\n           Added float values for fractions.\\n\\n- `min_samples_leaf`: int or float, default=1\\n        The minimum number of samples required to be at a leaf node.\\n        A split point at any depth will only be considered if it leaves at\\n        least ``min_samples_leaf`` training samples in each of the left and\\n        right branches.  This may have the effect of smoothing the model,\\n        especially in regression.\\n\\n        - If int, then consider `min_samples_leaf` as the minimum number.\\n        - If float, then `min_samples_leaf` is a fraction and\\n          `ceil(min_samples_leaf * n_samples)` are the minimum\\n          number of samples for each node.\\n\\n        *Changed in 0.18*\\n           Added float values for fractions.\\n\\n- `min_weight_fraction_leaf`: float, default=0.0\\n        The minimum weighted fraction of the sum total of weights (of all\\n        the input samples) required to be at a leaf node. Samples have\\n        equal weight when sample_weight is not provided.\\n\\n- `max_features`: int, float or {\\\"auto\\\", \\\"sqrt\\\", \\\"log2\\\"}, default=None\\n        The number of features to consider when looking for the best split:\\n\\n            - If int, then consider `max_features` features at each split.\\n            - If float, then `max_features` is a fraction and\\n              `int(max_features * n_features)` features are considered at each\\n              split.\\n            - If \\\"auto\\\", then `max_features=sqrt(n_features)`.\\n            - If \\\"sqrt\\\", then `max_features=sqrt(n_features)`.\\n            - If \\\"log2\\\", then `max_features=log2(n_features)`.\\n            - If None, then `max_features=n_features`.\\n\\n        Note: the search for a split does not stop until at least one\\n        valid partition of the node samples is found, even if it requires to\\n        effectively inspect more than ``max_features`` features.\\n\\n- `random_state`: int, RandomState instance, default=None\\n        Controls the randomness of the estimator. The features are always\\n        randomly permuted at each split, even if ``splitter`` is set to\\n        ``\\\"best\\\"``. When ``max_features < n_features``, the algorithm will\\n        select ``max_features`` at random at each split before finding the best\\n        split among them. But the best found split may vary across different\\n        runs, even if ``max_features=n_features``. That is the case, if the\\n        improvement of the criterion is identical for several splits and one\\n        split has to be selected at random. To obtain a deterministic behaviour\\n        during fitting, ``random_state`` has to be fixed to an integer.\\n        See :term:`Glossary <random_state>` for details.\\n\\n- `max_leaf_nodes`: int, default=None\\n        Grow a tree with ``max_leaf_nodes`` in best-first fashion.\\n        Best nodes are defined as relative reduction in impurity.\\n        If None then unlimited number of leaf nodes.\\n\\n- `min_impurity_decrease`: float, default=0.0\\n        A node will be split if this split induces a decrease of the impurity\\n        greater than or equal to this value.\\n\\n        The weighted impurity decrease equation is the following\\n\\n```python\\nN_t / N * (impurity - N_t_R / N_t * right_impurity\\n                    - N_t_L / N_t * left_impurity)\\n\\ne ``N`` is the total number of samples, ``N_t`` is the number of\\nles at the current node, ``N_t_L`` is the number of samples in the\\n child, and ``N_t_R`` is the number of samples in the right child.\\n\\n`, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\\n`sample_weight`` is passed.\\n\\nersionadded:: 0.19\\n\\nrity_split : float, default=0\\nshold for early stopping in tree growth. A node will split\\nts impurity is above the threshold, otherwise it is a leaf.\\n\\neprecated:: 0.19\\n`min_impurity_split`` has been deprecated in favor of\\n`min_impurity_decrease`` in 0.19. The default value of\\n`min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\\nill be removed in 0.25. Use ``min_impurity_decrease`` instead.\\n\\night : dict, list of dict or \\\"balanced\\\", default=None\\nhts associated with classes in the form ``{class_label: weight}``.\\none, all classes are supposed to have weight one. For\\ni-output problems, a list of dicts can be provided in the same\\nr as the columns of y.\\n\\n that for multioutput (including multilabel) weights should be\\nned for each class of every column in its own dict. For example,\\nfour-class multilabel classification weights should be\\n 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\\n1}, {2:5}, {3:1}, {4:1}].\\n\\n\\\"balanced\\\" mode uses the values of y to automatically adjust\\nhts inversely proportional to class frequencies in the input data\\n`n_samples / (n_classes * np.bincount(y))``\\n\\nmulti-output, the weights of each column of y will be multiplied.\\n\\n that these weights will be multiplied with sample_weight (passed\\nugh the fit method) if sample_weight is specified.\\n\\n: deprecated, default='deprecated'\\n parameter is deprecated and will be removed in v0.24.\\n\\neprecated:: 0.22\\n\\na : non-negative float, default=0.0\\nlexity parameter used for Minimal Cost-Complexity Pruning. The\\nree with the largest cost complexity that is smaller than\\np_alpha`` will be chosen. By default, no pruning is performed. See\\n:`minimal_cost_complexity_pruning` for details.\\n\\nersionadded:: 0.22\\n\\nes\\n--\\n : ndarray of shape (n_classes,) or list of ndarray\\nclasses labels (single output problem),\\n list of arrays of class labels (multi-output problem).\\n\\nimportances_ : ndarray of shape (n_features,)\\nimpurity-based feature importances.\\nhigher, the more important the feature.\\nimportance of a feature is computed as the (normalized)\\nl reduction of the criterion brought by that feature.  It is also\\nn as the Gini importance [4]_.\\n\\ning: impurity-based feature importances can be misleading for\\n cardinality features (many unique values). See\\nc:`sklearn.inspection.permutation_importance` as an alternative.\\n\\nures_ : int\\ninferred value of max_features.\\n\\ns_ : int or list of int\\nnumber of classes (for single output problems),\\n list containing the number of classes for each\\nut (for multi-output problems).\\n\\nes_ : int\\nnumber of features when ``fit`` is performed.\\n\\ns_ : int\\nnumber of outputs when ``fit`` is performed.\\n\\nTree\\nunderlying Tree object. Please refer to\\nlp(sklearn.tree._tree.Tree)`` for attributes of Tree object and\\n:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py`\\nbasic usage of these attributes.\\n\\n\\n\\nTreeRegressor : A decision tree regressor.\\n\\n\\n\\nult values for the parameters controlling the size of the trees\\nmax_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\\n trees which can potentially be very large on some data sets. To\\nemory consumption, the complexity and size of the trees should be\\ned by setting those parameter values.\\n\\nes\\n--\\n\\nttps://en.wikipedia.org/wiki/Decision_tree_learning\\n\\n. Breiman, J. Friedman, R. Olshen, and C. Stone, \\\"Classification\\nnd Regression Trees\\\", Wadsworth, Belmont, CA, 1984.\\n\\n. Hastie, R. Tibshirani and J. Friedman. \\\"Elements of Statistical\\nearning\\\", Springer, 2009.\\n\\n. Breiman, and A. Cutler, \\\"Random Forests\\\",\\nttps://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm\\n\\n\\n\\n sklearn.datasets import load_iris\\n sklearn.model_selection import cross_val_score\\n sklearn.tree import DecisionTreeClassifier\\n= DecisionTreeClassifier(random_state=0)\\n = load_iris()\\ns_val_score(clf, iris.data, iris.target, cv=10)\\n                        # doctest: +SKIP\\n\\n1.     ,  0.93...,  0.86...,  0.93...,  0.93...,\\n0.93...,  0.93...,  1.     ,  0.93...,  1.      ])\\n```\\n\"]] [:hr] [:hr]] [:div [:h3 \":sklearn.classification/dummy-classifier\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [3 2]:\\n\\n|         :name | :default |\\n|---------------|----------|\\n|     :constant |          |\\n| :random-state |          |\\n|     :strategy |     warn |\\n\"]]] [:span [:p/markdown \"\\n    DummyClassifier is a classifier that makes predictions using simple rules.\\n\\n    This classifier is useful as a simple baseline to compare with other\\n    (real) classifiers. Do not use it for real problems.\\n\\n    Read more in the User Guide: `dummy_estimators`.\\n\\n    *Added in 0.13*\\n\\n    Parameters\\n    ----------\\n- `strategy`: str, default=\\\"stratified\\\"\\n        Strategy to use to generate predictions.\\n\\n        * \\\"stratified\\\": generates predictions by respecting the training\\n          set's class distribution.\\n        * \\\"most_frequent\\\": always predicts the most frequent label in the\\n          training set.\\n        * \\\"prior\\\": always predicts the class that maximizes the class prior\\n          (like \\\"most_frequent\\\") and ``predict_proba`` returns the class prior.\\n        * \\\"uniform\\\": generates predictions uniformly at random.\\n        * \\\"constant\\\": always predicts a constant label that is provided by\\n          the user. This is useful for metrics that evaluate a non-majority\\n          class\\n\\n          *Changed in 0.22*\\n             The default value of `strategy` will change to \\\"prior\\\" in version\\n             0.24. Starting from version 0.22, a warning will be raised if\\n             `strategy` is not explicitly set.\\n\\n          *Added in 0.17*\\n             Dummy Classifier now supports prior fitting strategy using\\n             parameter *prior*.\\n\\n- `random_state`: int, RandomState instance or None, optional, default=None\\n        Controls the randomness to generate the predictions when\\n        ``strategy='stratified'`` or ``strategy='uniform'``.\\n        Pass an int for reproducible output across multiple function calls.\\n        See :term:`Glossary <random_state>`.\\n\\n- `constant`: int or str or array-like of shape (n_outputs,)\\n        The explicit constant as predicted by the \\\"constant\\\" strategy. This\\n        parameter is useful only for the \\\"constant\\\" strategy.\\n\\n    Attributes\\n    ----------\\n- `classes_`: array or list of array of shape (n_classes,)\\n        Class labels for each output.\\n\\n- `n_classes_`: array or list of array of shape (n_classes,)\\n        Number of label for each output.\\n\\n- `class_prior_`: array or list of array of shape (n_classes,)\\n        Probability of each class for each output.\\n\\n- `n_outputs_`: int,\\n        Number of outputs.\\n\\n- `sparse_output_`: bool,\\n        True if the array returned from predict is to be in sparse CSC format.\\n        Is automatically set to True if the input y is passed in sparse format.\\n\\n    Examples\\n    --------\\n    >>> import numpy as np\\n    >>> from sklearn.dummy import DummyClassifier\\n    >>> X = np.array([-1, 1, 1, 1])\\n    >>> y = np.array([0, 1, 1, 1])\\n    >>> dummy_clf = DummyClassifier(strategy=\\\"most_frequent\\\")\\n    >>> dummy_clf.fit(X, y)\\n    DummyClassifier(strategy='most_frequent')\\n    >>> dummy_clf.predict(X)\\n    array([1, 1, 1, 1])\\n    >>> dummy_clf.score(X, y)\\n    0.75\\n    \"]] [:hr] [:hr]] [:div [:h3 \":sklearn.classification/extra-tree-classifier\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [13 2]:\\n\\n|                     :name | :default |\\n|---------------------------|----------|\\n| :min-weight-fraction-leaf |    0.000 |\\n|           :max-leaf-nodes |          |\\n|    :min-impurity-decrease |    0.000 |\\n|        :min-samples-split |    2.000 |\\n|                :ccp-alpha |    0.000 |\\n|                 :splitter |   random |\\n|             :random-state |          |\\n|         :min-samples-leaf |        1 |\\n|             :max-features |     auto |\\n|       :min-impurity-split |          |\\n|                :max-depth |          |\\n|             :class-weight |          |\\n|                :criterion |     gini |\\n\"]]] [:span [:p/markdown \"An extremely randomized tree classifier.\\n\\n    Extra-trees differ from classic decision trees in the way they are built.\\n    When looking for the best split to separate the samples of a node into two\\n    groups, random splits are drawn for each of the `max_features` randomly\\n    selected features and the best split among those is chosen. When\\n    `max_features` is set 1, this amounts to building a totally random\\n    decision tree.\\n\\n    Warning: Extra-trees should only be used within ensemble methods.\\n\\n    Read more in the User Guide: `tree`.\\n\\n    Parameters\\n    ----------\\n- `criterion`: {\\\"gini\\\", \\\"entropy\\\"}, default=\\\"gini\\\"\\n        The function to measure the quality of a split. Supported criteria are\\n        \\\"gini\\\" for the Gini impurity and \\\"entropy\\\" for the information gain.\\n\\n- `splitter`: {\\\"random\\\", \\\"best\\\"}, default=\\\"random\\\"\\n        The strategy used to choose the split at each node. Supported\\n        strategies are \\\"best\\\" to choose the best split and \\\"random\\\" to choose\\n        the best random split.\\n\\n- `max_depth`: int, default=None\\n        The maximum depth of the tree. If None, then nodes are expanded until\\n        all leaves are pure or until all leaves contain less than\\n        min_samples_split samples.\\n\\n- `min_samples_split`: int or float, default=2\\n        The minimum number of samples required to split an internal node:\\n\\n        - If int, then consider `min_samples_split` as the minimum number.\\n        - If float, then `min_samples_split` is a fraction and\\n          `ceil(min_samples_split * n_samples)` are the minimum\\n          number of samples for each split.\\n\\n        *Changed in 0.18*\\n           Added float values for fractions.\\n\\n- `min_samples_leaf`: int or float, default=1\\n        The minimum number of samples required to be at a leaf node.\\n        A split point at any depth will only be considered if it leaves at\\n        least ``min_samples_leaf`` training samples in each of the left and\\n        right branches.  This may have the effect of smoothing the model,\\n        especially in regression.\\n\\n        - If int, then consider `min_samples_leaf` as the minimum number.\\n        - If float, then `min_samples_leaf` is a fraction and\\n          `ceil(min_samples_leaf * n_samples)` are the minimum\\n          number of samples for each node.\\n\\n        *Changed in 0.18*\\n           Added float values for fractions.\\n\\n- `min_weight_fraction_leaf`: float, default=0.0\\n        The minimum weighted fraction of the sum total of weights (of all\\n        the input samples) required to be at a leaf node. Samples have\\n        equal weight when sample_weight is not provided.\\n\\n- `max_features`: int, float, {\\\"auto\\\", \\\"sqrt\\\", \\\"log2\\\"} or None, default=\\\"auto\\\"\\n        The number of features to consider when looking for the best split:\\n\\n            - If int, then consider `max_features` features at each split.\\n            - If float, then `max_features` is a fraction and\\n              `int(max_features * n_features)` features are considered at each\\n              split.\\n            - If \\\"auto\\\", then `max_features=sqrt(n_features)`.\\n            - If \\\"sqrt\\\", then `max_features=sqrt(n_features)`.\\n            - If \\\"log2\\\", then `max_features=log2(n_features)`.\\n            - If None, then `max_features=n_features`.\\n\\n        Note: the search for a split does not stop until at least one\\n        valid partition of the node samples is found, even if it requires to\\n        effectively inspect more than ``max_features`` features.\\n\\n- `random_state`: int, RandomState instance, default=None\\n        Used to pick randomly the `max_features` used at each split.\\n        See :term:`Glossary <random_state>` for details.\\n\\n- `max_leaf_nodes`: int, default=None\\n        Grow a tree with ``max_leaf_nodes`` in best-first fashion.\\n        Best nodes are defined as relative reduction in impurity.\\n        If None then unlimited number of leaf nodes.\\n\\n- `min_impurity_decrease`: float, default=0.0\\n        A node will be split if this split induces a decrease of the impurity\\n        greater than or equal to this value.\\n\\n        The weighted impurity decrease equation is the following\\n\\n```python\\nN_t / N * (impurity - N_t_R / N_t * right_impurity\\n                    - N_t_L / N_t * left_impurity)\\n\\ne ``N`` is the total number of samples, ``N_t`` is the number of\\nles at the current node, ``N_t_L`` is the number of samples in the\\n child, and ``N_t_R`` is the number of samples in the right child.\\n\\n`, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\\n`sample_weight`` is passed.\\n\\nersionadded:: 0.19\\n\\nrity_split : float, (default=0)\\nshold for early stopping in tree growth. A node will split\\nts impurity is above the threshold, otherwise it is a leaf.\\n\\neprecated:: 0.19\\n`min_impurity_split`` has been deprecated in favor of\\n`min_impurity_decrease`` in 0.19. The default value of\\n`min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\\nill be removed in 0.25. Use ``min_impurity_decrease`` instead.\\n\\night : dict, list of dict or \\\"balanced\\\", default=None\\nhts associated with classes in the form ``{class_label: weight}``.\\none, all classes are supposed to have weight one. For\\ni-output problems, a list of dicts can be provided in the same\\nr as the columns of y.\\n\\n that for multioutput (including multilabel) weights should be\\nned for each class of every column in its own dict. For example,\\nfour-class multilabel classification weights should be\\n 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\\n1}, {2:5}, {3:1}, {4:1}].\\n\\n\\\"balanced\\\" mode uses the values of y to automatically adjust\\nhts inversely proportional to class frequencies in the input data\\n`n_samples / (n_classes * np.bincount(y))``\\n\\nmulti-output, the weights of each column of y will be multiplied.\\n\\n that these weights will be multiplied with sample_weight (passed\\nugh the fit method) if sample_weight is specified.\\n\\na : non-negative float, default=0.0\\nlexity parameter used for Minimal Cost-Complexity Pruning. The\\nree with the largest cost complexity that is smaller than\\np_alpha`` will be chosen. By default, no pruning is performed. See\\n:`minimal_cost_complexity_pruning` for details.\\n\\nersionadded:: 0.22\\n\\nes\\n--\\n : ndarray of shape (n_classes,) or list of ndarray\\nclasses labels (single output problem),\\n list of arrays of class labels (multi-output problem).\\n\\nures_ : int\\ninferred value of max_features.\\n\\ns_ : int or list of int\\nnumber of classes (for single output problems),\\n list containing the number of classes for each\\nut (for multi-output problems).\\n\\nimportances_ : ndarray of shape (n_features,)\\nimpurity-based feature importances.\\nhigher, the more important the feature.\\nimportance of a feature is computed as the (normalized)\\nl reduction of the criterion brought by that feature.  It is also\\nn as the Gini importance.\\n\\ning: impurity-based feature importances can be misleading for\\n cardinality features (many unique values). See\\nc:`sklearn.inspection.permutation_importance` as an alternative.\\n\\nes_ : int\\nnumber of features when ``fit`` is performed.\\n\\ns_ : int\\nnumber of outputs when ``fit`` is performed.\\n\\nTree\\nunderlying Tree object. Please refer to\\nlp(sklearn.tree._tree.Tree)`` for attributes of Tree object and\\n:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py`\\nbasic usage of these attributes.\\n\\n\\n\\neRegressor : An extremely randomized tree regressor.\\nensemble.ExtraTreesClassifier : An extra-trees classifier.\\nensemble.ExtraTreesRegressor : An extra-trees regressor.\\n\\n\\n\\nult values for the parameters controlling the size of the trees\\nmax_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\\n trees which can potentially be very large on some data sets. To\\nemory consumption, the complexity and size of the trees should be\\ned by setting those parameter values.\\n\\nes\\n--\\n\\n. Geurts, D. Ernst., and L. Wehenkel, \\\"Extremely randomized trees\\\",\\nachine Learning, 63(1), 3-42, 2006.\\n\\n\\n\\n sklearn.datasets import load_iris\\n sklearn.model_selection import train_test_split\\n sklearn.ensemble import BaggingClassifier\\n sklearn.tree import ExtraTreeClassifier\\n = load_iris(return_X_y=True)\\nain, X_test, y_train, y_test = train_test_split(\\n, y, random_state=0)\\na_tree = ExtraTreeClassifier(random_state=0)\\n= BaggingClassifier(extra_tree, random_state=0).fit(\\n_train, y_train)\\nscore(X_test, y_test)\\n.\\n```\\n\"]] [:hr] [:hr]] [:div [:h3 \":sklearn.classification/extra-trees-classifier\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [19 2]:\\n\\n|                     :name | :default |\\n|---------------------------|----------|\\n| :min-weight-fraction-leaf |    0.000 |\\n|           :max-leaf-nodes |          |\\n|    :min-impurity-decrease |    0.000 |\\n|        :min-samples-split |    2.000 |\\n|                :bootstrap |    false |\\n|                :ccp-alpha |    0.000 |\\n|                   :n-jobs |          |\\n|             :random-state |          |\\n|                :oob-score |    false |\\n|         :min-samples-leaf |        1 |\\n|             :max-features |     auto |\\n|       :min-impurity-split |          |\\n|               :warm-start |    false |\\n|                :max-depth |          |\\n|             :class-weight |          |\\n|             :n-estimators |      100 |\\n|              :max-samples |          |\\n|                :criterion |     gini |\\n|                  :verbose |        0 |\\n\"]]] [:span [:p/markdown \"\\n    An extra-trees classifier.\\n\\n    This class implements a meta estimator that fits a number of\\n    randomized decision trees (a.k.a. extra-trees) on various sub-samples\\n    of the dataset and uses averaging to improve the predictive accuracy\\n    and control over-fitting.\\n\\n    Read more in the User Guide: `forest`.\\n\\n    Parameters\\n    ----------\\n- `n_estimators`: int, default=100\\n        The number of trees in the forest.\\n\\n        *Changed in 0.22*\\n           The default value of ``n_estimators`` changed from 10 to 100\\n           in 0.22.\\n\\n- `criterion`: {\\\"gini\\\", \\\"entropy\\\"}, default=\\\"gini\\\"\\n        The function to measure the quality of a split. Supported criteria are\\n        \\\"gini\\\" for the Gini impurity and \\\"entropy\\\" for the information gain.\\n\\n- `max_depth`: int, default=None\\n        The maximum depth of the tree. If None, then nodes are expanded until\\n        all leaves are pure or until all leaves contain less than\\n        min_samples_split samples.\\n\\n- `min_samples_split`: int or float, default=2\\n        The minimum number of samples required to split an internal node:\\n\\n        - If int, then consider `min_samples_split` as the minimum number.\\n        - If float, then `min_samples_split` is a fraction and\\n          `ceil(min_samples_split * n_samples)` are the minimum\\n          number of samples for each split.\\n\\n        *Changed in 0.18*\\n           Added float values for fractions.\\n\\n- `min_samples_leaf`: int or float, default=1\\n        The minimum number of samples required to be at a leaf node.\\n        A split point at any depth will only be considered if it leaves at\\n        least ``min_samples_leaf`` training samples in each of the left and\\n        right branches.  This may have the effect of smoothing the model,\\n        especially in regression.\\n\\n        - If int, then consider `min_samples_leaf` as the minimum number.\\n        - If float, then `min_samples_leaf` is a fraction and\\n          `ceil(min_samples_leaf * n_samples)` are the minimum\\n          number of samples for each node.\\n\\n        *Changed in 0.18*\\n           Added float values for fractions.\\n\\n- `min_weight_fraction_leaf`: float, default=0.0\\n        The minimum weighted fraction of the sum total of weights (of all\\n        the input samples) required to be at a leaf node. Samples have\\n        equal weight when sample_weight is not provided.\\n\\n- `max_features`: {\\\"auto\\\", \\\"sqrt\\\", \\\"log2\\\"}, int or float, default=\\\"auto\\\"\\n        The number of features to consider when looking for the best split:\\n\\n        - If int, then consider `max_features` features at each split.\\n        - If float, then `max_features` is a fraction and\\n          `int(max_features * n_features)` features are considered at each\\n          split.\\n        - If \\\"auto\\\", then `max_features=sqrt(n_features)`.\\n        - If \\\"sqrt\\\", then `max_features=sqrt(n_features)`.\\n        - If \\\"log2\\\", then `max_features=log2(n_features)`.\\n        - If None, then `max_features=n_features`.\\n\\n        Note: the search for a split does not stop until at least one\\n        valid partition of the node samples is found, even if it requires to\\n        effectively inspect more than ``max_features`` features.\\n\\n- `max_leaf_nodes`: int, default=None\\n        Grow trees with ``max_leaf_nodes`` in best-first fashion.\\n        Best nodes are defined as relative reduction in impurity.\\n        If None then unlimited number of leaf nodes.\\n\\n- `min_impurity_decrease`: float, default=0.0\\n        A node will be split if this split induces a decrease of the impurity\\n        greater than or equal to this value.\\n\\n        The weighted impurity decrease equation is the following\\n\\n```python\\nN_t / N * (impurity - N_t_R / N_t * right_impurity\\n                    - N_t_L / N_t * left_impurity)\\n\\ne ``N`` is the total number of samples, ``N_t`` is the number of\\nles at the current node, ``N_t_L`` is the number of samples in the\\n child, and ``N_t_R`` is the number of samples in the right child.\\n\\n`, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\\n`sample_weight`` is passed.\\n\\nersionadded:: 0.19\\n\\nrity_split : float, default=None\\nshold for early stopping in tree growth. A node will split\\nts impurity is above the threshold, otherwise it is a leaf.\\n\\neprecated:: 0.19\\n`min_impurity_split`` has been deprecated in favor of\\n`min_impurity_decrease`` in 0.19. The default value of\\n`min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\\nill be removed in 0.25. Use ``min_impurity_decrease`` instead.\\n\\np : bool, default=False\\nher bootstrap samples are used when building trees. If False, the\\ne dataset is used to build each tree.\\n\\ne : bool, default=False\\nher to use out-of-bag samples to estimate\\ngeneralization accuracy.\\n\\n int, default=None\\nnumber of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\\nh:`decision_path` and :meth:`apply` are all parallelized over the\\ns. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\\next. ``-1`` means using all processors. See :term:`Glossary\\nobs>` for more details.\\n\\ntate : int, RandomState, default=None\\nrols 3 sources of randomness:\\n\\ne bootstrapping of the samples used when building trees\\nf ``bootstrap=True``)\\ne sampling of the features to consider when looking for the best\\nlit at each node (if ``max_features < n_features``)\\ne draw of the splits for each of the `max_features`\\n\\n:term:`Glossary <random_state>` for details.\\n\\n: int, default=0\\nrols the verbosity when fitting and predicting.\\n\\nrt : bool, default=False\\n set to ``True``, reuse the solution of the previous call to fit\\nadd more estimators to the ensemble, otherwise, just fit a whole\\nforest. See :term:`the Glossary <warm_start>`.\\n\\night : {\\\"balanced\\\", \\\"balanced_subsample\\\"}, dict or list of dicts,             default=None\\nhts associated with classes in the form ``{class_label: weight}``.\\not given, all classes are supposed to have weight one. For\\ni-output problems, a list of dicts can be provided in the same\\nr as the columns of y.\\n\\n that for multioutput (including multilabel) weights should be\\nned for each class of every column in its own dict. For example,\\nfour-class multilabel classification weights should be\\n 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\\n1}, {2:5}, {3:1}, {4:1}].\\n\\n\\\"balanced\\\" mode uses the values of y to automatically adjust\\nhts inversely proportional to class frequencies in the input data\\n`n_samples / (n_classes * np.bincount(y))``\\n\\n\\\"balanced_subsample\\\" mode is the same as \\\"balanced\\\" except that\\nhts are computed based on the bootstrap sample for every tree\\nn.\\n\\nmulti-output, the weights of each column of y will be multiplied.\\n\\n that these weights will be multiplied with sample_weight (passed\\nugh the fit method) if sample_weight is specified.\\n\\na : non-negative float, default=0.0\\nlexity parameter used for Minimal Cost-Complexity Pruning. The\\nree with the largest cost complexity that is smaller than\\np_alpha`` will be chosen. By default, no pruning is performed. See\\n:`minimal_cost_complexity_pruning` for details.\\n\\nersionadded:: 0.22\\n\\nles : int or float, default=None\\nootstrap is True, the number of samples to draw from X\\nrain each base estimator.\\n\\n None (default), then draw `X.shape[0]` samples.\\n int, then draw `max_samples` samples.\\n float, then draw `max_samples * X.shape[0]` samples. Thus,\\nax_samples` should be in the interval `(0, 1)`.\\n\\nersionadded:: 0.22\\n\\nes\\n--\\nimator_ : ExtraTreesClassifier\\nchild estimator template used to create the collection of fitted\\nestimators.\\n\\nrs_ : list of DecisionTreeClassifier\\ncollection of fitted sub-estimators.\\n\\n : ndarray of shape (n_classes,) or a list of such arrays\\nclasses labels (single output problem), or a list of arrays of\\ns labels (multi-output problem).\\n\\ns_ : int or list\\nnumber of classes (single output problem), or a list containing the\\ner of classes for each output (multi-output problem).\\n\\nimportances_ : ndarray of shape (n_features,)\\nimpurity-based feature importances.\\nhigher, the more important the feature.\\nimportance of a feature is computed as the (normalized)\\nl reduction of the criterion brought by that feature.  It is also\\nn as the Gini importance.\\n\\ning: impurity-based feature importances can be misleading for\\n cardinality features (many unique values). See\\nc:`sklearn.inspection.permutation_importance` as an alternative.\\n\\nes_ : int\\nnumber of features when ``fit`` is performed.\\n\\ns_ : int\\nnumber of outputs when ``fit`` is performed.\\n\\ne_ : float\\ne of the training dataset obtained using an out-of-bag estimate.\\n attribute exists only when ``oob_score`` is True.\\n\\nsion_function_ : ndarray of shape (n_samples, n_classes)\\nsion function computed with out-of-bag estimate on the training\\n If n_estimators is small it might be possible that a data point\\nnever left out during the bootstrap. In this case,\\n_decision_function_` might contain NaN. This attribute exists\\n when ``oob_score`` is True.\\n\\n\\n\\ntree.ExtraTreeClassifier : Base classifier for this ensemble.\\nrestClassifier : Ensemble Classifier based on trees with optimal\\nts.\\n\\n\\n\\nult values for the parameters controlling the size of the trees\\nmax_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\\n trees which can potentially be very large on some data sets. To\\nemory consumption, the complexity and size of the trees should be\\ned by setting those parameter values.\\n\\nes\\n--\\n. Geurts, D. Ernst., and L. Wehenkel, \\\"Extremely randomized\\nrees\\\", Machine Learning, 63(1), 3-42, 2006.\\n\\n\\n\\n sklearn.ensemble import ExtraTreesClassifier\\n sklearn.datasets import make_classification\\n = make_classification(n_features=4, random_state=0)\\n= ExtraTreesClassifier(n_estimators=100, random_state=0)\\nfit(X, y)\\nesClassifier(random_state=0)\\npredict([[0, 0, 0, 0]])\\n])\\n```\\n\"]] [:hr] [:hr]] [:div [:h3 \":sklearn.classification/gaussian-nb\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [2 2]:\\n\\n|          :name | :default |\\n|----------------|---------:|\\n|        :priors |          |\\n| :var-smoothing |  1.0E-09 |\\n\"]]] [:span [:p/markdown \"\\n    Gaussian Naive Bayes (GaussianNB)\\n\\n    Can perform online updates to model parameters via `partial_fit`.\\n    For details on algorithm used to update feature means and variance online,\\n    see Stanford CS tech report STAN-CS-79-773 by Chan, Golub, and LeVeque:\\n\\n        http://i.stanford.edu/pub/cstr/reports/cs/tr/79/773/CS-TR-79-773.pdf\\n\\n    Read more in the User Guide: `gaussian_naive_bayes`.\\n\\n    Parameters\\n    ----------\\n- `priors`: array-like of shape (n_classes,)\\n        Prior probabilities of the classes. If specified the priors are not\\n        adjusted according to the data.\\n\\n- `var_smoothing`: float, default=1e-9\\n        Portion of the largest variance of all features that is added to\\n        variances for calculation stability.\\n\\n        *Added in 0.20*\\n\\n    Attributes\\n    ----------\\n- `class_count_`: ndarray of shape (n_classes,)\\n        number of training samples observed in each class.\\n\\n- `class_prior_`: ndarray of shape (n_classes,)\\n        probability of each class.\\n\\n- `classes_`: ndarray of shape (n_classes,)\\n        class labels known to the classifier\\n\\n- `epsilon_`: float\\n        absolute additive value to variances\\n\\n- `sigma_`: ndarray of shape (n_classes, n_features)\\n        variance of each feature per class\\n\\n- `theta_`: ndarray of shape (n_classes, n_features)\\n        mean of each feature per class\\n\\n    Examples\\n    --------\\n    >>> import numpy as np\\n    >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\\n    >>> Y = np.array([1, 1, 1, 2, 2, 2])\\n    >>> from sklearn.naive_bayes import GaussianNB\\n    >>> clf = GaussianNB()\\n    >>> clf.fit(X, Y)\\n    GaussianNB()\\n    >>> print(clf.predict([[-0.8, -1]]))\\n    [1]\\n    >>> clf_pf = GaussianNB()\\n    >>> clf_pf.partial_fit(X, Y, np.unique(Y))\\n    GaussianNB()\\n    >>> print(clf_pf.predict([[-0.8, -1]]))\\n    [1]\\n    \"]] [:hr] [:hr]] [:div [:h3 \":sklearn.classification/gaussian-process-classifier\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [9 2]:\\n\\n|                 :name |      :default |\\n|-----------------------|---------------|\\n|               :kernel |               |\\n|            :optimizer | fmin_l_bfgs_b |\\n|          :multi-class |   one_vs_rest |\\n|               :n-jobs |               |\\n|         :random-state |               |\\n|     :max-iter-predict |           100 |\\n|         :copy-x-train |          true |\\n| :n-restarts-optimizer |             0 |\\n|           :warm-start |         false |\\n\"]]] [:span [:p/markdown \"Gaussian process classification (GPC) based on Laplace approximation.\\n\\n    The implementation is based on Algorithm 3.1, 3.2, and 5.1 of\\n    Gaussian Processes for Machine Learning (GPML) by Rasmussen and\\n    Williams.\\n\\n    Internally, the Laplace approximation is used for approximating the\\n    non-Gaussian posterior by a Gaussian.\\n\\n    Currently, the implementation is restricted to using the logistic link\\n    function. For multi-class classification, several binary one-versus rest\\n    classifiers are fitted. Note that this class thus does not implement\\n    a true multi-class Laplace approximation.\\n\\n    Read more in the User Guide: `gaussian_process`.\\n\\n    Parameters\\n    ----------\\n- `kernel`: kernel instance, default=None\\n        The kernel specifying the covariance function of the GP. If None is\\n        passed, the kernel \\\"1.0 * RBF(1.0)\\\" is used as default. Note that\\n        the kernel's hyperparameters are optimized during fitting.\\n\\n- `optimizer`: 'fmin_l_bfgs_b' or callable, default='fmin_l_bfgs_b'\\n        Can either be one of the internally supported optimizers for optimizing\\n        the kernel's parameters, specified by a string, or an externally\\n        defined optimizer passed as a callable. If a callable is passed, it\\n        must have the  signature\\n\\n```python\\ndef optimizer(obj_func, initial_theta, bounds):\\n    # * 'obj_func' is the objective function to be maximized, which\\n    #   takes the hyperparameters theta as parameter and an\\n    #   optional flag eval_gradient, which determines if the\\n    #   gradient is returned additionally to the function value\\n    # * 'initial_theta': the initial value for theta, which can be\\n    #   used by local optimizers\\n    # * 'bounds': the bounds on the values of theta\\n    ....\\n    # Returned are the best found hyperparameters theta and\\n    # the corresponding value of the target function.\\n    return theta_opt, func_min\\n\\ndefault, the 'L-BFGS-B' algorithm from scipy.optimize.minimize\\nsed. If None is passed, the kernel's parameters are kept fixed.\\nlable internal optimizers are::\\n\\n'fmin_l_bfgs_b'\\n\\nts_optimizer : int, default=0\\nnumber of restarts of the optimizer for finding the kernel's\\nmeters which maximize the log-marginal likelihood. The first run\\nhe optimizer is performed from the kernel's initial parameters,\\nremaining ones (if any) from thetas sampled log-uniform randomly\\n the space of allowed theta-values. If greater than 0, all bounds\\n be finite. Note that n_restarts_optimizer=0 implies that one\\nis performed.\\n\\n_predict : int, default=100\\nmaximum number of iterations in Newton's method for approximating\\nposterior during predict. Smaller values will reduce computation\\n at the cost of worse results.\\n\\nrt : bool, default=False\\narm-starts are enabled, the solution of the last Newton iteration\\nhe Laplace approximation of the posterior mode is used as\\nialization for the next call of _posterior_mode(). This can speed\\nonvergence when _posterior_mode is called several times on similar\\nlems as in hyperparameter optimization. See :term:`the Glossary\\nm_start>`.\\n\\nrain : bool, default=True\\nrue, a persistent copy of the training data is stored in the\\nct. Otherwise, just a reference to the training data is stored,\\nh might cause predictions to change if the data is modified\\nrnally.\\n\\ntate : int or RandomState, default=None\\nrmines random number generation used to initialize the centers.\\n an int for reproducible results across multiple function calls.\\n:term: `Glossary <random_state>`.\\n\\nass : {'one_vs_rest', 'one_vs_one'}, default='one_vs_rest'\\nifies how multi-class classification problems are handled.\\norted are 'one_vs_rest' and 'one_vs_one'. In 'one_vs_rest',\\nbinary Gaussian process classifier is fitted for each class, which\\nrained to separate this class from the rest. In 'one_vs_one', one\\nry Gaussian process classifier is fitted for each pair of classes,\\nh is trained to separate these two classes. The predictions of\\ne binary predictors are combined into multi-class predictions.\\n that 'one_vs_one' does not support predicting probability\\nmates.\\n\\n int, default=None\\nnumber of jobs to use for the computation.\\nne`` means 1 unless in a :obj:`joblib.parallel_backend` context.\\n`` means using all processors. See :term:`Glossary <n_jobs>`\\nmore details.\\n\\nes\\n--\\n: kernel instance\\nkernel used for prediction. In case of binary classification,\\nstructure of the kernel is the same as the one passed as parameter\\nwith optimized hyperparameters. In case of multi-class\\nsification, a CompoundKernel is returned which consists of the\\nerent kernels used in the one-versus-rest classifiers.\\n\\ninal_likelihood_value_ : float\\nlog-marginal-likelihood of ``self.kernel_.theta``\\n\\n : array-like of shape (n_classes,)\\nue class labels.\\n\\ns_ : int\\nnumber of classes in the training data\\n\\n\\n\\n sklearn.datasets import load_iris\\n sklearn.gaussian_process import GaussianProcessClassifier\\n sklearn.gaussian_process.kernels import RBF\\n = load_iris(return_X_y=True)\\nel = 1.0 * RBF(1.0)\\n= GaussianProcessClassifier(kernel=kernel,\\n    random_state=0).fit(X, y)\\nscore(X, y)\\n.\\npredict_proba(X[:2,:])\\n0.83548752, 0.03228706, 0.13222543],\\n0.79064206, 0.06525643, 0.14410151]])\\n\\nonadded:: 0.18\\n```\\n\"]] [:hr] [:hr]] [:div [:h3 \":sklearn.classification/gradient-boosting-classifier\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [22 2]:\\n\\n|                     :name |     :default |\\n|---------------------------|--------------|\\n|         :n-iter-no-change |              |\\n|            :learning-rate |       0.1000 |\\n| :min-weight-fraction-leaf |        0.000 |\\n|           :max-leaf-nodes |              |\\n|    :min-impurity-decrease |        0.000 |\\n|        :min-samples-split |        2.000 |\\n|                      :tol |    0.0001000 |\\n|                  :presort |   deprecated |\\n|                :subsample |        1.000 |\\n|                :ccp-alpha |        0.000 |\\n|             :random-state |              |\\n|         :min-samples-leaf |            1 |\\n|             :max-features |              |\\n|                     :init |              |\\n|       :min-impurity-split |              |\\n|               :warm-start |        false |\\n|                :max-depth |            3 |\\n|      :validation-fraction |       0.1000 |\\n|             :n-estimators |          100 |\\n|                :criterion | friedman_mse |\\n|                     :loss |     deviance |\\n|                  :verbose |            0 |\\n\"]]] [:span [:p/markdown \"Gradient Boosting for classification.\\n\\n    GB builds an additive model in a\\n    forward stage-wise fashion; it allows for the optimization of\\n    arbitrary differentiable loss functions. In each stage ``n_classes_``\\n    regression trees are fit on the negative gradient of the\\n    binomial or multinomial deviance loss function. Binary classification\\n    is a special case where only a single regression tree is induced.\\n\\n    Read more in the User Guide: `gradient_boosting`.\\n\\n    Parameters\\n    ----------\\n- `loss`: {'deviance', 'exponential'}, default='deviance'\\n        loss function to be optimized. 'deviance' refers to\\n        deviance (= logistic regression) for classification\\n        with probabilistic outputs. For loss 'exponential' gradient\\n        boosting recovers the AdaBoost algorithm.\\n\\n- `learning_rate`: float, default=0.1\\n        learning rate shrinks the contribution of each tree by `learning_rate`.\\n        There is a trade-off between learning_rate and n_estimators.\\n\\n- `n_estimators`: int, default=100\\n        The number of boosting stages to perform. Gradient boosting\\n        is fairly robust to over-fitting so a large number usually\\n        results in better performance.\\n\\n- `subsample`: float, default=1.0\\n        The fraction of samples to be used for fitting the individual base\\n        learners. If smaller than 1.0 this results in Stochastic Gradient\\n        Boosting. `subsample` interacts with the parameter `n_estimators`.\\n        Choosing `subsample < 1.0` leads to a reduction of variance\\n        and an increase in bias.\\n\\n- `criterion`: {'friedman_mse', 'mse', 'mae'}, default='friedman_mse'\\n        The function to measure the quality of a split. Supported criteria\\n        are 'friedman_mse' for the mean squared error with improvement\\n        score by Friedman, 'mse' for mean squared error, and 'mae' for\\n        the mean absolute error. The default value of 'friedman_mse' is\\n        generally the best as it can provide a better approximation in\\n        some cases.\\n\\n        *Added in 0.18*\\n\\n- `min_samples_split`: int or float, default=2\\n        The minimum number of samples required to split an internal node:\\n\\n        - If int, then consider `min_samples_split` as the minimum number.\\n        - If float, then `min_samples_split` is a fraction and\\n          `ceil(min_samples_split * n_samples)` are the minimum\\n          number of samples for each split.\\n\\n        *Changed in 0.18*\\n           Added float values for fractions.\\n\\n- `min_samples_leaf`: int or float, default=1\\n        The minimum number of samples required to be at a leaf node.\\n        A split point at any depth will only be considered if it leaves at\\n        least ``min_samples_leaf`` training samples in each of the left and\\n        right branches.  This may have the effect of smoothing the model,\\n        especially in regression.\\n\\n        - If int, then consider `min_samples_leaf` as the minimum number.\\n        - If float, then `min_samples_leaf` is a fraction and\\n          `ceil(min_samples_leaf * n_samples)` are the minimum\\n          number of samples for each node.\\n\\n        *Changed in 0.18*\\n           Added float values for fractions.\\n\\n- `min_weight_fraction_leaf`: float, default=0.0\\n        The minimum weighted fraction of the sum total of weights (of all\\n        the input samples) required to be at a leaf node. Samples have\\n        equal weight when sample_weight is not provided.\\n\\n- `max_depth`: int, default=3\\n        maximum depth of the individual regression estimators. The maximum\\n        depth limits the number of nodes in the tree. Tune this parameter\\n        for best performance; the best value depends on the interaction\\n        of the input variables.\\n\\n- `min_impurity_decrease`: float, default=0.0\\n        A node will be split if this split induces a decrease of the impurity\\n        greater than or equal to this value.\\n\\n        The weighted impurity decrease equation is the following\\n\\n```python\\nN_t / N * (impurity - N_t_R / N_t * right_impurity\\n                    - N_t_L / N_t * left_impurity)\\n\\ne ``N`` is the total number of samples, ``N_t`` is the number of\\nles at the current node, ``N_t_L`` is the number of samples in the\\n child, and ``N_t_R`` is the number of samples in the right child.\\n\\n`, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\\n`sample_weight`` is passed.\\n\\nersionadded:: 0.19\\n\\nrity_split : float, default=None\\nshold for early stopping in tree growth. A node will split\\nts impurity is above the threshold, otherwise it is a leaf.\\n\\neprecated:: 0.19\\n`min_impurity_split`` has been deprecated in favor of\\n`min_impurity_decrease`` in 0.19. The default value of\\n`min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\\nill be removed in 0.25. Use ``min_impurity_decrease`` instead.\\n\\nstimator or 'zero', default=None\\nstimator object that is used to compute the initial predictions.\\nit`` has to provide :meth:`fit` and :meth:`predict_proba`. If\\no', the initial raw predictions are set to zero. By default, a\\nmmyEstimator`` predicting the classes priors is used.\\n\\ntate : int or RandomState, default=None\\nrols the random seed given to each Tree estimator at each\\nting iteration.\\nddition, it controls the random permutation of the features at\\n split (see Notes for more details).\\nlso controls the random spliting of the training data to obtain a\\ndation set if `n_iter_no_change` is not None.\\n an int for reproducible output across multiple function calls.\\n:term:`Glossary <random_state>`.\\n\\nures : {'auto', 'sqrt', 'log2'}, int or float, default=None\\nnumber of features to consider when looking for the best split:\\n\\n int, then consider `max_features` features at each split.\\n float, then `max_features` is a fraction and\\nnt(max_features * n_features)` features are considered at each\\nlit.\\n 'auto', then `max_features=sqrt(n_features)`.\\n 'sqrt', then `max_features=sqrt(n_features)`.\\n 'log2', then `max_features=log2(n_features)`.\\n None, then `max_features=n_features`.\\n\\nsing `max_features < n_features` leads to a reduction of variance\\nan increase in bias.\\n\\n: the search for a split does not stop until at least one\\nd partition of the node samples is found, even if it requires to\\nctively inspect more than ``max_features`` features.\\n\\n: int, default=0\\nle verbose output. If 1 then it prints progress and performance\\n in a while (the more trees the lower the frequency). If greater\\n 1 then it prints progress and performance for every tree.\\n\\n_nodes : int, default=None\\n trees with ``max_leaf_nodes`` in best-first fashion.\\n nodes are defined as relative reduction in impurity.\\none then unlimited number of leaf nodes.\\n\\nrt : bool, default=False\\n set to ``True``, reuse the solution of the previous call to fit\\nadd more estimators to the ensemble, otherwise, just erase the\\nious solution. See :term:`the Glossary <warm_start>`.\\n\\n: deprecated, default='deprecated'\\n parameter is deprecated and will be removed in v0.24.\\n\\neprecated :: 0.22\\n\\non_fraction : float, default=0.1\\nproportion of training data to set aside as validation set for\\ny stopping. Must be between 0 and 1.\\n used if ``n_iter_no_change`` is set to an integer.\\n\\nersionadded:: 0.20\\n\\no_change : int, default=None\\niter_no_change`` is used to decide if early stopping will be used\\nerminate training when validation score is not improving. By\\nult it is set to None to disable early stopping. If set to a\\ner, it will set aside ``validation_fraction`` size of the training\\n as validation and terminate training when validation score is not\\noving in all of the previous ``n_iter_no_change`` numbers of\\nations. The split is stratified.\\n\\nersionadded:: 0.20\\n\\noat, default=1e-4\\nrance for the early stopping. When the loss is not improving\\nt least tol for ``n_iter_no_change`` iterations (if set to a\\ner), the training stops.\\n\\nersionadded:: 0.20\\n\\na : non-negative float, default=0.0\\nlexity parameter used for Minimal Cost-Complexity Pruning. The\\nree with the largest cost complexity that is smaller than\\np_alpha`` will be chosen. By default, no pruning is performed. See\\n:`minimal_cost_complexity_pruning` for details.\\n\\nersionadded:: 0.22\\n\\nes\\n--\\ntors_ : int\\nnumber of estimators as selected by early stopping (if\\niter_no_change`` is specified). Otherwise it is set to\\nestimators``.\\n\\nersionadded:: 0.20\\n\\nimportances_ : ndarray of shape (n_features,)\\nimpurity-based feature importances.\\nhigher, the more important the feature.\\nimportance of a feature is computed as the (normalized)\\nl reduction of the criterion brought by that feature.  It is also\\nn as the Gini importance.\\n\\ning: impurity-based feature importances can be misleading for\\n cardinality features (many unique values). See\\nc:`sklearn.inspection.permutation_importance` as an alternative.\\n\\novement_ : ndarray of shape (n_estimators,)\\nimprovement in loss (= deviance) on the out-of-bag samples\\ntive to the previous iteration.\\nb_improvement_[0]`` is the improvement in\\n of the first stage over the ``init`` estimator.\\n available if ``subsample < 1.0``\\n\\nore_ : ndarray of shape (n_estimators,)\\ni-th score ``train_score_[i]`` is the deviance (= loss) of the\\nl at iteration ``i`` on the in-bag sample.\\n`subsample == 1`` this is the deviance on the training data.\\n\\nLossFunction\\nconcrete ``LossFunction`` object.\\n\\nestimator\\nestimator that provides the initial predictions.\\nvia the ``init`` argument or ``loss.init_estimator``.\\n\\nrs_ : ndarray of DecisionTreeRegressor of shape (n_estimators, ``loss_.K``)\\ncollection of fitted sub-estimators. ``loss_.K`` is 1 for binary\\nsification, otherwise n_classes.\\n\\n : ndarray of shape (n_classes,)\\nclasses labels.\\n\\nes_ : int\\nnumber of data features.\\n\\ns_ : int\\nnumber of classes.\\n\\nures_ : int\\ninferred value of max_features.\\n\\n\\n\\nures are always randomly permuted at each split. Therefore,\\n found split may vary, even with the same training data and\\natures=n_features``, if the improvement of the criterion is\\nl for several splits enumerated during the search of the best\\no obtain a deterministic behaviour during fitting,\\n_state`` has to be fixed.\\n\\n\\n\\n sklearn.datasets import make_classification\\n sklearn.ensemble import GradientBoostingClassifier\\n sklearn.model_selection import train_test_split\\n = make_classification(random_state=0)\\nain, X_test, y_train, y_test = train_test_split(\\nX, y, random_state=0)\\n= GradientBoostingClassifier(random_state=0)\\nfit(X_train, y_train)\\nBoostingClassifier(random_state=0)\\npredict(X_test[:2])\\n, 0])\\nscore(X_test, y_test)\\n\\n\\n\\n\\nensemble.HistGradientBoostingClassifier,\\ntree.DecisionTreeClassifier, RandomForestClassifier\\nClassifier\\n\\nes\\n--\\nman, Greedy Function Approximation: A Gradient Boosting\\n The Annals of Statistics, Vol. 29, No. 5, 2001.\\n\\nman, Stochastic Gradient Boosting, 1999\\n\\ne, R. Tibshirani and J. Friedman.\\n of Statistical Learning Ed. 2, Springer, 2009.\\n```\\n\"]] [:hr] [:hr]] [:div [:h3 \":sklearn.classification/hist-gradient-boosting-classifier\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [17 2]:\\n\\n|                :name |  :default |\\n|----------------------|-----------|\\n|    :n-iter-no-change |     10.00 |\\n|       :learning-rate |    0.1000 |\\n|      :max-leaf-nodes |     31.00 |\\n|             :scoring |      loss |\\n|                 :tol | 1.000E-07 |\\n|      :early-stopping |      auto |\\n|            :max-iter |       100 |\\n|        :random-state |           |\\n|            :max-bins |       255 |\\n|    :min-samples-leaf |        20 |\\n|       :monotonic-cst |           |\\n|          :warm-start |     false |\\n|           :max-depth |           |\\n| :validation-fraction |    0.1000 |\\n|                :loss |      auto |\\n|             :verbose |         0 |\\n|  :l-2-regularization |     0.000 |\\n\"]]] [:span [:p/markdown \"Histogram-based Gradient Boosting Classification Tree.\\n\\n    This estimator is much faster than\\n    `GradientBoostingClassifier<sklearn.ensemble.GradientBoostingClassifier>`\\n    for big datasets (n_samples >= 10 000).\\n\\n    This estimator has native support for missing values (NaNs). During\\n    training, the tree grower learns at each split point whether samples\\n    with missing values should go to the left or right child, based on the\\n    potential gain. When predicting, samples with missing values are\\n    assigned to the left or right child consequently. If no missing values\\n    were encountered for a given feature during training, then samples with\\n    missing values are mapped to whichever child has the most samples.\\n\\n    This implementation is inspired by\\n    [LightGBM ](https://github.com/Microsoft/LightGBM).\\n\\n\\n---\\n**Note**\\n\\nThis estimator is still **experimental** for now: the predictions\\nand the API might change without any deprecation cycle. To use it,\\nyou need to explicitly import ``enable_hist_gradient_boosting``::\\n\\n  >>> # explicitly require this experimental feature\\n  >>> from sklearn.experimental import enable_hist_gradient_boosting  # noqa\\n  >>> # now you can import normally from ensemble\\n  >>> from sklearn.ensemble import HistGradientBoostingClassifier\\n\\nad more in the :ref:`User Guide <histogram_based_gradient_boosting>`.\\n\\n versionadded:: 0.21\\n\\nrameters\\n--------\\nss : {'auto', 'binary_crossentropy', 'categorical_crossentropy'},             optional (default='auto')\\n  The loss function to use in the boosting process. 'binary_crossentropy'\\n  (also known as logistic loss) is used for binary classification and\\n  generalizes to 'categorical_crossentropy' for multiclass\\n  classification. 'auto' will automatically choose either loss depending\\n  on the nature of the problem.\\narning_rate : float, optional (default=0.1)\\n  The learning rate, also known as *shrinkage*. This is used as a\\n  multiplicative factor for the leaves values. Use ``1`` for no\\n  shrinkage.\\nx_iter : int, optional (default=100)\\n  The maximum number of iterations of the boosting process, i.e. the\\n  maximum number of trees for binary classification. For multiclass\\n  classification, `n_classes` trees per iteration are built.\\nx_leaf_nodes : int or None, optional (default=31)\\n  The maximum number of leaves for each tree. Must be strictly greater\\n  than 1. If None, there is no maximum limit.\\nx_depth : int or None, optional (default=None)\\n  The maximum depth of each tree. The depth of a tree is the number of\\n  edges to go from the root to the deepest leaf.\\n  Depth isn't constrained by default.\\nn_samples_leaf : int, optional (default=20)\\n  The minimum number of samples per leaf. For small datasets with less\\n  than a few hundred samples, it is recommended to lower this value\\n  since only very shallow trees would be built.\\n_regularization : float, optional (default=0)\\n  The L2 regularization parameter. Use 0 for no regularization.\\nx_bins : int, optional (default=255)\\n  The maximum number of bins to use for non-missing values. Before\\n  training, each feature of the input array `X` is binned into\\n  integer-valued bins, which allows for a much faster training stage.\\n  Features with a small number of unique values may use less than\\n  ``max_bins`` bins. In addition to the ``max_bins`` bins, one more bin\\n  is always reserved for missing values. Must be no larger than 255.\\nnotonic_cst : array-like of int of shape (n_features), default=None\\n  Indicates the monotonic constraint to enforce on each feature. -1, 1\\n  and 0 respectively correspond to a positive constraint, negative\\n  constraint and no constraint. Read more in the :ref:`User Guide\\n  <monotonic_cst_gbdt>`.\\nrm_start : bool, optional (default=False)\\n  When set to ``True``, reuse the solution of the previous call to fit\\n  and add more estimators to the ensemble. For results to be valid, the\\n  estimator should be re-trained on the same data only.\\n  See :term:`the Glossary <warm_start>`.\\nrly_stopping : 'auto' or bool (default='auto')\\n  If 'auto', early stopping is enabled if the sample size is larger than\\n  10000. If True, early stopping is enabled, otherwise early stopping is\\n  disabled.\\noring : str or callable or None, optional (default='loss')\\n  Scoring parameter to use for early stopping. It can be a single\\n  string (see :ref:`scoring_parameter`) or a callable (see\\n  :ref:`scoring`). If None, the estimator's default scorer\\n  is used. If ``scoring='loss'``, early stopping is checked\\n  w.r.t the loss value. Only used if early stopping is performed.\\nlidation_fraction : int or float or None, optional (default=0.1)\\n  Proportion (or absolute size) of training data to set aside as\\n  validation data for early stopping. If None, early stopping is done on\\n  the training data. Only used if early stopping is performed.\\niter_no_change : int, optional (default=10)\\n  Used to determine when to \\\"early stop\\\". The fitting process is\\n  stopped when none of the last ``n_iter_no_change`` scores are better\\n  than the ``n_iter_no_change - 1`` -th-to-last one, up to some\\n  tolerance. Only used if early stopping is performed.\\nl : float or None, optional (default=1e-7)\\n  The absolute tolerance to use when comparing scores. The higher the\\n  tolerance, the more likely we are to early stop: higher tolerance\\n  means that it will be harder for subsequent iterations to be\\n  considered an improvement upon the reference score.\\nrbose: int, optional (default=0)\\n  The verbosity level. If not zero, print some information about the\\n  fitting process.\\nndom_state : int, np.random.RandomStateInstance or None,         optional (default=None)\\n  Pseudo-random number generator to control the subsampling in the\\n  binning process, and the train/validation data split if early stopping\\n  is enabled.\\n  Pass an int for reproducible output across multiple function calls.\\n  See :term:`Glossary <random_state>`.\\n\\ntributes\\n--------\\nasses_ : array, shape = (n_classes,)\\n  Class labels.\\niter_ : int\\n  The number of iterations as selected by early stopping, depending on\\n  the `early_stopping` parameter. Otherwise it corresponds to max_iter.\\ntrees_per_iteration_ : int\\n  The number of tree that are built at each iteration. This is equal to 1\\n  for binary classification, and to ``n_classes`` for multiclass\\n  classification.\\nain_score_ : ndarray, shape (n_iter_+1,)\\n  The scores at each iteration on the training data. The first entry\\n  is the score of the ensemble before the first iteration. Scores are\\n  computed according to the ``scoring`` parameter. If ``scoring`` is\\n  not 'loss', scores are computed on a subset of at most 10 000\\n  samples. Empty if no early stopping.\\nlidation_score_ : ndarray, shape (n_iter_+1,)\\n  The scores at each iteration on the held-out validation data. The\\n  first entry is the score of the ensemble before the first iteration.\\n  Scores are computed according to the ``scoring`` parameter. Empty if\\n  no early stopping or if ``validation_fraction`` is None.\\n\\namples\\n------\\n> # To use this experimental feature, we need to explicitly ask for it:\\n> from sklearn.experimental import enable_hist_gradient_boosting  # noqa\\n> from sklearn.ensemble import HistGradientBoostingClassifier\\n> from sklearn.datasets import load_iris\\n> X, y = load_iris(return_X_y=True)\\n> clf = HistGradientBoostingClassifier().fit(X, y)\\n> clf.score(X, y)\\n0\\n\\n---\\n\"]] [:hr] [:hr]] [:div [:h3 \":sklearn.classification/k-neighbors-classifier\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [8 2]:\\n\\n|          :name |  :default |\\n|----------------|-----------|\\n|     :algorithm |      auto |\\n|     :leaf-size |        30 |\\n|        :metric | minkowski |\\n| :metric-params |           |\\n|        :n-jobs |           |\\n|   :n-neighbors |         5 |\\n|             :p |         2 |\\n|       :weights |   uniform |\\n\"]]] [:span [:p/markdown \"Classifier implementing the k-nearest neighbors vote.\\n\\n    Read more in the User Guide: `classification`.\\n\\n    Parameters\\n    ----------\\n- `n_neighbors`: int, default=5\\n        Number of neighbors to use by default for `kneighbors` queries.\\n\\n- `weights`: {'uniform', 'distance'} or callable, default='uniform'\\n        weight function used in prediction.  Possible values:\\n\\n        - 'uniform' : uniform weights.  All points in each neighborhood\\n          are weighted equally.\\n        - 'distance' : weight points by the inverse of their distance.\\n          in this case, closer neighbors of a query point will have a\\n          greater influence than neighbors which are further away.\\n        - [callable] : a user-defined function which accepts an\\n          array of distances, and returns an array of the same shape\\n          containing the weights.\\n\\n- `algorithm`: {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\\n        Algorithm used to compute the nearest neighbors:\\n\\n        - 'ball_tree' will use `BallTree`\\n        - 'kd_tree' will use `KDTree`\\n        - 'brute' will use a brute-force search.\\n        - 'auto' will attempt to decide the most appropriate algorithm\\n          based on the values passed to `fit` method.\\n\\n        Note: fitting on sparse input will override the setting of\\n        this parameter, using brute force.\\n\\n- `leaf_size`: int, default=30\\n        Leaf size passed to BallTree or KDTree.  This can affect the\\n        speed of the construction and query, as well as the memory\\n        required to store the tree.  The optimal value depends on the\\n        nature of the problem.\\n\\n- `p`: int, default=2\\n        Power parameter for the Minkowski metric. When p = 1, this is\\n        equivalent to using manhattan_distance (l1), and euclidean_distance\\n        (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\\n\\n- `metric`: str or callable, default='minkowski'\\n        the distance metric to use for the tree.  The default metric is\\n        minkowski, and with p=2 is equivalent to the standard Euclidean\\n        metric. See the documentation of `DistanceMetric` for a\\n        list of available metrics.\\n        If metric is \\\"precomputed\\\", X is assumed to be a distance matrix and\\n        must be square during fit. X may be a :term:`sparse graph`,\\n        in which case only \\\"nonzero\\\" elements may be considered neighbors.\\n\\n- `metric_params`: dict, default=None\\n        Additional keyword arguments for the metric function.\\n\\n- `n_jobs`: int, default=None\\n        The number of parallel jobs to run for neighbors search.\\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\\n        for more details.\\n        Doesn't affect `fit` method.\\n\\n    Attributes\\n    ----------\\n- `classes_`: array of shape (n_classes,)\\n        Class labels known to the classifier\\n\\n- `effective_metric_`: str or callble\\n        The distance metric used. It will be same as the `metric` parameter\\n        or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\\n        'minkowski' and `p` parameter set to 2.\\n\\n- `effective_metric_params_`: dict\\n        Additional keyword arguments for the metric function. For most metrics\\n        will be same with `metric_params` parameter, but may also contain the\\n        `p` parameter value if the `effective_metric_` attribute is set to\\n        'minkowski'.\\n\\n- `outputs_2d_`: bool\\n        False when `y`'s shape is (n_samples, ) or (n_samples, 1) during fit\\n        otherwise True.\\n\\n    Examples\\n    --------\\n    >>> X = [[0], [1], [2], [3]]\\n    >>> y = [0, 0, 1, 1]\\n    >>> from sklearn.neighbors import KNeighborsClassifier\\n    >>> neigh = KNeighborsClassifier(n_neighbors=3)\\n    >>> neigh.fit(X, y)\\n    KNeighborsClassifier(...)\\n    >>> print(neigh.predict([[1.1]]))\\n    [0]\\n    >>> print(neigh.predict_proba([[0.9]]))\\n    [[0.66666667 0.33333333]]\\n\\n    See also\\n    --------\\n    RadiusNeighborsClassifier\\n    KNeighborsRegressor\\n    RadiusNeighborsRegressor\\n    NearestNeighbors\\n\\n    Notes\\n    -----\\n    See Nearest Neighbors: `neighbors` in the online documentation\\n    for a discussion of the choice of ``algorithm`` and ``leaf_size``.\\n\\n\\n---\\n**Warning**\\n\\nRegarding the Nearest Neighbors algorithms, if it is found that two\\nneighbors, neighbor `k+1` and `k`, have identical distances\\nbut different labels, the results will depend on the ordering of the\\ntraining data.\\n\\nps://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm\\n\\n---\\n\"]] [:hr] [:hr]] [:div [:h3 \":sklearn.classification/label-propagation\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [6 2]:\\n\\n|        :name | :default |\\n|--------------|----------|\\n|       :gamma |       20 |\\n|      :kernel |      rbf |\\n|    :max-iter |     1000 |\\n|      :n-jobs |          |\\n| :n-neighbors |        7 |\\n|         :tol | 0.001000 |\\n\"]]] [:span [:p/markdown \"Label Propagation classifier\\n\\n    Read more in the User Guide: `label_propagation`.\\n\\n    Parameters\\n    ----------\\n- `kernel`: {'knn', 'rbf'} or callable, default='rbf'\\n        String identifier for kernel function to use or the kernel function\\n        itself. Only 'rbf' and 'knn' strings are valid inputs. The function\\n        passed should take two inputs, each of shape (n_samples, n_features),\\n        and return a (n_samples, n_samples) shaped weight matrix.\\n\\n- `gamma`: float, default=20\\n        Parameter for rbf kernel.\\n\\n- `n_neighbors`: int, default=7\\n        Parameter for knn kernel which need to be strictly positive.\\n\\n- `max_iter`: int, default=1000\\n        Change maximum number of iterations allowed.\\n\\n- `tol`: float, 1e-3\\n        Convergence tolerance: threshold to consider the system at steady\\n        state.\\n\\n- `n_jobs`: int, default=None\\n        The number of parallel jobs to run.\\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\\n        for more details.\\n\\n    Attributes\\n    ----------\\n- `X_`: ndarray of shape (n_samples, n_features)\\n        Input array.\\n\\n- `classes_`: ndarray of shape (n_classes,)\\n        The distinct labels used in classifying instances.\\n\\n- `label_distributions_`: ndarray of shape (n_samples, n_classes)\\n        Categorical distribution for each item.\\n\\n- `transduction_`: ndarray of shape (n_samples)\\n        Label assigned to each item via the transduction.\\n\\n- `n_iter_`: int\\n        Number of iterations run.\\n\\n    Examples\\n    --------\\n    >>> import numpy as np\\n    >>> from sklearn import datasets\\n    >>> from sklearn.semi_supervised import LabelPropagation\\n    >>> label_prop_model = LabelPropagation()\\n    >>> iris = datasets.load_iris()\\n    >>> rng = np.random.RandomState(42)\\n    >>> random_unlabeled_points = rng.rand(len(iris.target)) < 0.3\\n    >>> labels = np.copy(iris.target)\\n    >>> labels[random_unlabeled_points] = -1\\n    >>> label_prop_model.fit(iris.data, labels)\\n    LabelPropagation(...)\\n\\n    References\\n    ----------\\n    Xiaojin Zhu and Zoubin Ghahramani. Learning from labeled and unlabeled data\\n    with label propagation. Technical Report CMU-CALD-02-107, Carnegie Mellon\\n    University, 2002 http://pages.cs.wisc.edu/~jerryzhu/pub/CMU-CALD-02-107.pdf\\n\\n    See Also\\n    --------\\n- `LabelSpreading`: Alternate label propagation strategy more robust to noise\\n    \"]] [:hr] [:hr]] [:div [:h3 \":sklearn.classification/label-spreading\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [7 2]:\\n\\n|        :name | :default |\\n|--------------|----------|\\n|       :alpha |   0.2000 |\\n|       :gamma |    20.00 |\\n|      :kernel |      rbf |\\n|    :max-iter |       30 |\\n|      :n-jobs |          |\\n| :n-neighbors |        7 |\\n|         :tol | 0.001000 |\\n\"]]] [:span [:p/markdown \"LabelSpreading model for semi-supervised learning\\n\\n    This model is similar to the basic Label Propagation algorithm,\\n    but uses affinity matrix based on the normalized graph Laplacian\\n    and soft clamping across the labels.\\n\\n    Read more in the User Guide: `label_propagation`.\\n\\n    Parameters\\n    ----------\\n- `kernel`: {'knn', 'rbf'} or callable, default='rbf'\\n        String identifier for kernel function to use or the kernel function\\n        itself. Only 'rbf' and 'knn' strings are valid inputs. The function\\n        passed should take two inputs, each of shape (n_samples, n_features),\\n        and return a (n_samples, n_samples) shaped weight matrix.\\n\\n- `gamma`: float, default=20\\n      Parameter for rbf kernel.\\n\\n- `n_neighbors`: int, default=7\\n      Parameter for knn kernel which is a strictly positive integer.\\n\\n- `alpha`: float, default=0.2\\n      Clamping factor. A value in (0, 1) that specifies the relative amount\\n      that an instance should adopt the information from its neighbors as\\n      opposed to its initial label.\\n      alpha=0 means keeping the initial label information; alpha=1 means\\n      replacing all initial information.\\n\\n- `max_iter`: int, default=30\\n      Maximum number of iterations allowed.\\n\\n- `tol`: float, default=1e-3\\n      Convergence tolerance: threshold to consider the system at steady\\n      state.\\n\\n- `n_jobs`: int, default=None\\n        The number of parallel jobs to run.\\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\\n        for more details.\\n\\n    Attributes\\n    ----------\\n- `X_`: ndarray of shape (n_samples, n_features)\\n        Input array.\\n\\n- `classes_`: ndarray of shape (n_classes,)\\n        The distinct labels used in classifying instances.\\n\\n- `label_distributions_`: ndarray of shape (n_samples, n_classes)\\n        Categorical distribution for each item.\\n\\n- `transduction_`: ndarray of shape (n_samples,)\\n        Label assigned to each item via the transduction.\\n\\n- `n_iter_`: int\\n        Number of iterations run.\\n\\n    Examples\\n    --------\\n    >>> import numpy as np\\n    >>> from sklearn import datasets\\n    >>> from sklearn.semi_supervised import LabelSpreading\\n    >>> label_prop_model = LabelSpreading()\\n    >>> iris = datasets.load_iris()\\n    >>> rng = np.random.RandomState(42)\\n    >>> random_unlabeled_points = rng.rand(len(iris.target)) < 0.3\\n    >>> labels = np.copy(iris.target)\\n    >>> labels[random_unlabeled_points] = -1\\n    >>> label_prop_model.fit(iris.data, labels)\\n    LabelSpreading(...)\\n\\n    References\\n    ----------\\n    Dengyong Zhou, Olivier Bousquet, Thomas Navin Lal, Jason Weston,\\n    Bernhard Schoelkopf. Learning with local and global consistency (2004)\\n    http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.115.3219\\n\\n    See Also\\n    --------\\n- `LabelPropagation`: Unregularized graph based semi-supervised learning\\n    \"]] [:hr] [:hr]] [:div [:h3 \":sklearn.classification/linear-discriminant-analysis\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [6 2]:\\n\\n|             :name |  :default |\\n|-------------------|-----------|\\n|     :n-components |           |\\n|           :priors |           |\\n|        :shrinkage |           |\\n|           :solver |       svd |\\n| :store-covariance |     false |\\n|              :tol | 0.0001000 |\\n\"]]] [:span [:p/markdown \"Linear Discriminant Analysis\\n\\n    A classifier with a linear decision boundary, generated by fitting class\\n    conditional densities to the data and using Bayes' rule.\\n\\n    The model fits a Gaussian density to each class, assuming that all classes\\n    share the same covariance matrix.\\n\\n    The fitted model can also be used to reduce the dimensionality of the input\\n    by projecting it to the most discriminative directions, using the\\n    `transform` method.\\n\\n    *Added in 0.17*\\n       *LinearDiscriminantAnalysis*.\\n\\n    Read more in the User Guide: `lda_qda`.\\n\\n    Parameters\\n    ----------\\n- `solver`: {'svd', 'lsqr', 'eigen'}, default='svd'\\n        Solver to use, possible values:\\n          - 'svd': Singular value decomposition (default).\\n            Does not compute the covariance matrix, therefore this solver is\\n            recommended for data with a large number of features.\\n          - 'lsqr': Least squares solution, can be combined with shrinkage.\\n          - 'eigen': Eigenvalue decomposition, can be combined with shrinkage.\\n\\n- `shrinkage`: 'auto' or float, default=None\\n        Shrinkage parameter, possible values:\\n          - None: no shrinkage (default).\\n          - 'auto': automatic shrinkage using the Ledoit-Wolf lemma.\\n          - float between 0 and 1: fixed shrinkage parameter.\\n\\n        Note that shrinkage works only with 'lsqr' and 'eigen' solvers.\\n\\n- `priors`: array-like of shape (n_classes,), default=None\\n        The class prior probabilities. By default, the class proportions are\\n        inferred from the training data.\\n\\n- `n_components`: int, default=None\\n        Number of components (<= min(n_classes - 1, n_features)) for\\n        dimensionality reduction. If None, will be set to\\n        min(n_classes - 1, n_features). This parameter only affects the\\n        `transform` method.\\n\\n- `store_covariance`: bool, default=False\\n        If True, explicitely compute the weighted within-class covariance\\n        matrix when solver is 'svd'. The matrix is always computed\\n        and stored for the other solvers.\\n\\n        *Added in 0.17*\\n\\n- `tol`: float, default=1.0e-4\\n        Absolute threshold for a singular value of X to be considered\\n        significant, used to estimate the rank of X. Dimensions whose\\n        singular values are non-significant are discarded. Only used if\\n        solver is 'svd'.\\n\\n        *Added in 0.17*\\n\\n    Attributes\\n    ----------\\n- `coef_`: ndarray of shape (n_features,) or (n_classes, n_features)\\n        Weight vector(s).\\n\\n- `intercept_`: ndarray of shape (n_classes,)\\n        Intercept term.\\n\\n- `covariance_`: array-like of shape (n_features, n_features)\\n        Weighted within-class covariance matrix. It corresponds to\\n        `sum_k prior_k * C_k` where `C_k` is the covariance matrix of the\\n        samples in class `k`. The `C_k` are estimated using the (potentially\\n        shrunk) biased estimator of covariance. If solver is 'svd', only\\n        exists when `store_covariance` is True.\\n\\n- `explained_variance_ratio_`: ndarray of shape (n_components,)\\n        Percentage of variance explained by each of the selected components.\\n        If ``n_components`` is not set then all components are stored and the\\n        sum of explained variances is equal to 1.0. Only available when eigen\\n        or svd solver is used.\\n\\n- `means_`: array-like of shape (n_classes, n_features)\\n        Class-wise means.\\n\\n- `priors_`: array-like of shape (n_classes,)\\n        Class priors (sum to 1).\\n\\n- `scalings_`: array-like of shape (rank, n_classes - 1)\\n        Scaling of the features in the space spanned by the class centroids.\\n        Only available for 'svd' and 'eigen' solvers.\\n\\n- `xbar_`: array-like of shape (n_features,)\\n        Overall mean. Only present if solver is 'svd'.\\n\\n- `classes_`: array-like of shape (n_classes,)\\n        Unique class labels.\\n\\n    See also\\n    --------\\n    sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis: Quadratic\\n        Discriminant Analysis\\n\\n    Examples\\n    --------\\n    >>> import numpy as np\\n    >>> from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\\n    >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\\n    >>> y = np.array([1, 1, 1, 2, 2, 2])\\n    >>> clf = LinearDiscriminantAnalysis()\\n    >>> clf.fit(X, y)\\n    LinearDiscriminantAnalysis()\\n    >>> print(clf.predict([[-0.8, -1]]))\\n    [1]\\n    \"]] [:hr] [:hr]] [:div [:h3 \":sklearn.classification/linear-svc\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [12 2]:\\n\\n|              :name |      :default |\\n|--------------------|---------------|\\n|               :tol |     0.0001000 |\\n| :intercept-scaling |         1.000 |\\n|       :multi-class |           ovr |\\n|           :penalty |            l2 |\\n|                 :c |         1.000 |\\n|          :max-iter |          1000 |\\n|      :random-state |               |\\n|              :dual |          true |\\n|     :fit-intercept |          true |\\n|      :class-weight |               |\\n|              :loss | squared_hinge |\\n|           :verbose |             0 |\\n\"]]] [:span [:p/markdown \"Linear Support Vector Classification.\\n\\n    Similar to SVC with parameter kernel='linear', but implemented in terms of\\n    liblinear rather than libsvm, so it has more flexibility in the choice of\\n    penalties and loss functions and should scale better to large numbers of\\n    samples.\\n\\n    This class supports both dense and sparse input and the multiclass support\\n    is handled according to a one-vs-the-rest scheme.\\n\\n    Read more in the User Guide: `svm_classification`.\\n\\n    Parameters\\n    ----------\\n- `penalty`: {'l1', 'l2'}, default='l2'\\n        Specifies the norm used in the penalization. The 'l2'\\n        penalty is the standard used in SVC. The 'l1' leads to ``coef_``\\n        vectors that are sparse.\\n\\n- `loss`: {'hinge', 'squared_hinge'}, default='squared_hinge'\\n        Specifies the loss function. 'hinge' is the standard SVM loss\\n        (used e.g. by the SVC class) while 'squared_hinge' is the\\n        square of the hinge loss.\\n\\n- `dual`: bool, default=True\\n        Select the algorithm to either solve the dual or primal\\n        optimization problem. Prefer dual=False when n_samples > n_features.\\n\\n- `tol`: float, default=1e-4\\n        Tolerance for stopping criteria.\\n\\n- `C`: float, default=1.0\\n        Regularization parameter. The strength of the regularization is\\n        inversely proportional to C. Must be strictly positive.\\n\\n- `multi_class`: {'ovr', 'crammer_singer'}, default='ovr'\\n        Determines the multi-class strategy if `y` contains more than\\n        two classes.\\n        ``\\\"ovr\\\"`` trains n_classes one-vs-rest classifiers, while\\n        ``\\\"crammer_singer\\\"`` optimizes a joint objective over all classes.\\n        While `crammer_singer` is interesting from a theoretical perspective\\n        as it is consistent, it is seldom used in practice as it rarely leads\\n        to better accuracy and is more expensive to compute.\\n        If ``\\\"crammer_singer\\\"`` is chosen, the options loss, penalty and dual\\n        will be ignored.\\n\\n- `fit_intercept`: bool, default=True\\n        Whether to calculate the intercept for this model. If set\\n        to false, no intercept will be used in calculations\\n        (i.e. data is expected to be already centered).\\n\\n- `intercept_scaling`: float, default=1\\n        When self.fit_intercept is True, instance vector x becomes\\n        ``[x, self.intercept_scaling]``,\\n        i.e. a \\\"synthetic\\\" feature with constant value equals to\\n        intercept_scaling is appended to the instance vector.\\n        The intercept becomes intercept_scaling * synthetic feature weight\\n        Note! the synthetic feature weight is subject to l1/l2 regularization\\n        as all other features.\\n        To lessen the effect of regularization on synthetic feature weight\\n        (and therefore on the intercept) intercept_scaling has to be increased.\\n\\n- `class_weight`: dict or 'balanced', default=None\\n        Set the parameter C of class i to ``class_weight[i]*C`` for\\n        SVC. If not given, all classes are supposed to have\\n        weight one.\\n        The \\\"balanced\\\" mode uses the values of y to automatically adjust\\n        weights inversely proportional to class frequencies in the input data\\n        as ``n_samples / (n_classes * np.bincount(y))``.\\n\\n- `verbose`: int, default=0\\n        Enable verbose output. Note that this setting takes advantage of a\\n        per-process runtime setting in liblinear that, if enabled, may not work\\n        properly in a multithreaded context.\\n\\n- `random_state`: int or RandomState instance, default=None\\n        Controls the pseudo random number generation for shuffling the data for\\n        the dual coordinate descent (if ``dual=True``). When ``dual=False`` the\\n        underlying implementation of `LinearSVC` is not random and\\n        ``random_state`` has no effect on the results.\\n        Pass an int for reproducible output across multiple function calls.\\n        See :term:`Glossary <random_state>`.\\n\\n- `max_iter`: int, default=1000\\n        The maximum number of iterations to be run.\\n\\n    Attributes\\n    ----------\\n- `coef_`: ndarray of shape (1, n_features) if n_classes == 2             else (n_classes, n_features)\\n        Weights assigned to the features (coefficients in the primal\\n        problem). This is only available in the case of a linear kernel.\\n\\n        ``coef_`` is a readonly property derived from ``raw_coef_`` that\\n        follows the internal memory layout of liblinear.\\n\\n- `intercept_`: ndarray of shape (1,) if n_classes == 2 else (n_classes,)\\n        Constants in decision function.\\n\\n- `classes_`: ndarray of shape (n_classes,)\\n        The unique classes labels.\\n\\n- `n_iter_`: int\\n        Maximum number of iterations run across all classes.\\n\\n    See Also\\n    --------\\n    SVC\\n        Implementation of Support Vector Machine classifier using libsvm:\\n        the kernel can be non-linear but its SMO algorithm does not\\n        scale to large number of samples as LinearSVC does.\\n\\n        Furthermore SVC multi-class mode is implemented using one\\n        vs one scheme while LinearSVC uses one vs the rest. It is\\n        possible to implement one vs the rest with SVC by using the\\n        `sklearn.multiclass.OneVsRestClassifier` wrapper.\\n\\n        Finally SVC can fit dense data without memory copy if the input\\n        is C-contiguous. Sparse data will still incur memory copy though.\\n\\n    sklearn.linear_model.SGDClassifier\\n        SGDClassifier can optimize the same cost function as LinearSVC\\n        by adjusting the penalty and loss parameters. In addition it requires\\n        less memory, allows incremental (online) learning, and implements\\n        various loss functions and regularization regimes.\\n\\n    Notes\\n    -----\\n    The underlying C implementation uses a random number generator to\\n    select features when fitting the model. It is thus not uncommon\\n    to have slightly different results for the same input data. If\\n    that happens, try with a smaller ``tol`` parameter.\\n\\n    The underlying implementation, liblinear, uses a sparse internal\\n    representation for the data that will incur a memory copy.\\n\\n    Predict output may not match that of standalone liblinear in certain\\n    cases. See differences from liblinear: `liblinear_differences`\\n    in the narrative documentation.\\n\\n    References\\n    ----------\\n    [LIBLINEAR: A Library for Large Linear Classification\\n    ](https://www.csie.ntu.edu.tw/~cjlin/liblinear/)\\n\\n    Examples\\n    --------\\n    >>> from sklearn.svm import LinearSVC\\n    >>> from sklearn.pipeline import make_pipeline\\n    >>> from sklearn.preprocessing import StandardScaler\\n    >>> from sklearn.datasets import make_classification\\n    >>> X, y = make_classification(n_features=4, random_state=0)\\n    >>> clf = make_pipeline(StandardScaler(),\\n    ...                     LinearSVC(random_state=0, tol=1e-5))\\n    >>> clf.fit(X, y)\\n    Pipeline(steps=[('standardscaler', StandardScaler()),\\n                    ('linearsvc', LinearSVC(random_state=0, tol=1e-05))])\\n\\n    >>> print(clf.named_steps['linearsvc'].coef_)\\n    [[0.141...   0.526... 0.679... 0.493...]]\\n\\n    >>> print(clf.named_steps['linearsvc'].intercept_)\\n    [0.1693...]\\n    >>> print(clf.predict([[0, 0, 0, 0]]))\\n    [1]\\n    \"]] [:hr] [:hr]] [:div [:h3 \":sklearn.classification/logistic-regression\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [15 2]:\\n\\n|              :name |  :default |\\n|--------------------|-----------|\\n|               :tol | 0.0001000 |\\n| :intercept-scaling |     1.000 |\\n|       :multi-class |      auto |\\n|            :solver |     lbfgs |\\n|           :penalty |        l2 |\\n|                 :c |     1.000 |\\n|          :max-iter |       100 |\\n|            :n-jobs |           |\\n|      :random-state |           |\\n|              :dual |     false |\\n|     :fit-intercept |      true |\\n|        :warm-start |     false |\\n|         :l-1-ratio |           |\\n|      :class-weight |           |\\n|           :verbose |         0 |\\n\"]]] [:span [:p/markdown \"\\n    Logistic Regression (aka logit, MaxEnt) classifier.\\n\\n    In the multiclass case, the training algorithm uses the one-vs-rest (OvR)\\n    scheme if the 'multi_class' option is set to 'ovr', and uses the\\n    cross-entropy loss if the 'multi_class' option is set to 'multinomial'.\\n    (Currently the 'multinomial' option is supported only by the 'lbfgs',\\n    'sag', 'saga' and 'newton-cg' solvers.)\\n\\n    This class implements regularized logistic regression using the\\n    'liblinear' library, 'newton-cg', 'sag', 'saga' and 'lbfgs' solvers. **Note\\n    that regularization is applied by default**. It can handle both dense\\n    and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit\\n    floats for optimal performance; any other input format will be converted\\n    (and copied).\\n\\n    The 'newton-cg', 'sag', and 'lbfgs' solvers support only L2 regularization\\n    with primal formulation, or no regularization. The 'liblinear' solver\\n    supports both L1 and L2 regularization, with a dual formulation only for\\n    the L2 penalty. The Elastic-Net regularization is only supported by the\\n    'saga' solver.\\n\\n    Read more in the User Guide: `logistic_regression`.\\n\\n    Parameters\\n    ----------\\n- `penalty`: {'l1', 'l2', 'elasticnet', 'none'}, default='l2'\\n        Used to specify the norm used in the penalization. The 'newton-cg',\\n        'sag' and 'lbfgs' solvers support only l2 penalties. 'elasticnet' is\\n        only supported by the 'saga' solver. If 'none' (not supported by the\\n        liblinear solver), no regularization is applied.\\n\\n        *Added in 0.19*\\n           l1 penalty with SAGA solver (allowing 'multinomial' + L1)\\n\\n- `dual`: bool, default=False\\n        Dual or primal formulation. Dual formulation is only implemented for\\n        l2 penalty with liblinear solver. Prefer dual=False when\\n        n_samples > n_features.\\n\\n- `tol`: float, default=1e-4\\n        Tolerance for stopping criteria.\\n\\n- `C`: float, default=1.0\\n        Inverse of regularization strength; must be a positive float.\\n        Like in support vector machines, smaller values specify stronger\\n        regularization.\\n\\n- `fit_intercept`: bool, default=True\\n        Specifies if a constant (a.k.a. bias or intercept) should be\\n        added to the decision function.\\n\\n- `intercept_scaling`: float, default=1\\n        Useful only when the solver 'liblinear' is used\\n        and self.fit_intercept is set to True. In this case, x becomes\\n        [x, self.intercept_scaling],\\n        i.e. a \\\"synthetic\\\" feature with constant value equal to\\n        intercept_scaling is appended to the instance vector.\\n        The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\\n\\n        Note! the synthetic feature weight is subject to l1/l2 regularization\\n        as all other features.\\n        To lessen the effect of regularization on synthetic feature weight\\n        (and therefore on the intercept) intercept_scaling has to be increased.\\n\\n- `class_weight`: dict or 'balanced', default=None\\n        Weights associated with classes in the form ``{class_label: weight}``.\\n        If not given, all classes are supposed to have weight one.\\n\\n        The \\\"balanced\\\" mode uses the values of y to automatically adjust\\n        weights inversely proportional to class frequencies in the input data\\n        as ``n_samples / (n_classes * np.bincount(y))``.\\n\\n        Note that these weights will be multiplied with sample_weight (passed\\n        through the fit method) if sample_weight is specified.\\n\\n        *Added in 0.17*\\n           *class_weight='balanced'*\\n\\n- `random_state`: int, RandomState instance, default=None\\n        Used when ``solver`` == 'sag', 'saga' or 'liblinear' to shuffle the\\n        data. See :term:`Glossary <random_state>` for details.\\n\\n- `solver`: {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'},             default='lbfgs'\\n\\n        Algorithm to use in the optimization problem.\\n\\n        - For small datasets, 'liblinear' is a good choice, whereas 'sag' and\\n          'saga' are faster for large ones.\\n        - For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs'\\n          handle multinomial loss; 'liblinear' is limited to one-versus-rest\\n          schemes.\\n        - 'newton-cg', 'lbfgs', 'sag' and 'saga' handle L2 or no penalty\\n        - 'liblinear' and 'saga' also handle L1 penalty\\n        - 'saga' also supports 'elasticnet' penalty\\n        - 'liblinear' does not support setting ``penalty='none'``\\n\\n        Note that 'sag' and 'saga' fast convergence is only guaranteed on\\n        features with approximately the same scale. You can\\n        preprocess the data with a scaler from sklearn.preprocessing.\\n\\n        *Added in 0.17*\\n           Stochastic Average Gradient descent solver.\\n        *Added in 0.19*\\n           SAGA solver.\\n        *Changed in 0.22*\\n            The default solver changed from 'liblinear' to 'lbfgs' in 0.22.\\n\\n- `max_iter`: int, default=100\\n        Maximum number of iterations taken for the solvers to converge.\\n\\n- `multi_class`: {'auto', 'ovr', 'multinomial'}, default='auto'\\n        If the option chosen is 'ovr', then a binary problem is fit for each\\n        label. For 'multinomial' the loss minimised is the multinomial loss fit\\n        across the entire probability distribution, *even when the data is\\n        binary*. 'multinomial' is unavailable when solver='liblinear'.\\n        'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\\n        and otherwise selects 'multinomial'.\\n\\n        *Added in 0.18*\\n           Stochastic Average Gradient descent solver for 'multinomial' case.\\n        *Changed in 0.22*\\n            Default changed from 'ovr' to 'auto' in 0.22.\\n\\n- `verbose`: int, default=0\\n        For the liblinear and lbfgs solvers set verbose to any positive\\n        number for verbosity.\\n\\n- `warm_start`: bool, default=False\\n        When set to True, reuse the solution of the previous call to fit as\\n        initialization, otherwise, just erase the previous solution.\\n        Useless for liblinear solver. See :term:`the Glossary <warm_start>`.\\n\\n        *Added in 0.17*\\n           *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.\\n\\n- `n_jobs`: int, default=None\\n        Number of CPU cores used when parallelizing over classes if\\n        multi_class='ovr'\\\". This parameter is ignored when the ``solver`` is\\n        set to 'liblinear' regardless of whether 'multi_class' is specified or\\n        not. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\\n        context. ``-1`` means using all processors.\\n        See :term:`Glossary <n_jobs>` for more details.\\n\\n- `l1_ratio`: float, default=None\\n        The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only\\n        used if ``penalty='elasticnet'``. Setting ``l1_ratio=0`` is equivalent\\n        to using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent\\n        to using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a\\n        combination of L1 and L2.\\n\\n    Attributes\\n    ----------\\n\\n- `classes_`: ndarray of shape (n_classes, )\\n        A list of class labels known to the classifier.\\n\\n- `coef_`: ndarray of shape (1, n_features) or (n_classes, n_features)\\n        Coefficient of the features in the decision function.\\n\\n        `coef_` is of shape (1, n_features) when the given problem is binary.\\n        In particular, when `multi_class='multinomial'`, `coef_` corresponds\\n        to outcome 1 (True) and `-coef_` corresponds to outcome 0 (False).\\n\\n- `intercept_`: ndarray of shape (1,) or (n_classes,)\\n        Intercept (a.k.a. bias) added to the decision function.\\n\\n        If `fit_intercept` is set to False, the intercept is set to zero.\\n        `intercept_` is of shape (1,) when the given problem is binary.\\n        In particular, when `multi_class='multinomial'`, `intercept_`\\n        corresponds to outcome 1 (True) and `-intercept_` corresponds to\\n        outcome 0 (False).\\n\\n- `n_iter_`: ndarray of shape (n_classes,) or (1, )\\n        Actual number of iterations for all classes. If binary or multinomial,\\n        it returns only 1 element. For liblinear solver, only the maximum\\n        number of iteration across all classes is given.\\n\\n        *Changed in 0.20*\\n\\n            In SciPy <= 1.0.0 the number of lbfgs iterations may exceed\\n            ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.\\n\\n    See Also\\n    --------\\n- `SGDClassifier`: Incrementally trained logistic regression (when given\\n        the parameter ``loss=\\\"log\\\"``).\\n- `LogisticRegressionCV`: Logistic regression with built-in cross validation.\\n\\n    Notes\\n    -----\\n    The underlying C implementation uses a random number generator to\\n    select features when fitting the model. It is thus not uncommon,\\n    to have slightly different results for the same input data. If\\n    that happens, try with a smaller tol parameter.\\n\\n    Predict output may not match that of standalone liblinear in certain\\n    cases. See differences from liblinear: `liblinear_differences`\\n    in the narrative documentation.\\n\\n    References\\n    ----------\\n\\n    L-BFGS-B -- Software for Large-scale Bound-constrained Optimization\\n        Ciyou Zhu, Richard Byrd, Jorge Nocedal and Jose Luis Morales.\\n        http://users.iems.northwestern.edu/~nocedal/lbfgsb.html\\n\\n    LIBLINEAR -- A Library for Large Linear Classification\\n        https://www.csie.ntu.edu.tw/~cjlin/liblinear/\\n\\n    SAG -- Mark Schmidt, Nicolas Le Roux, and Francis Bach\\n        Minimizing Finite Sums with the Stochastic Average Gradient\\n        https://hal.inria.fr/hal-00860051/document\\n\\n    SAGA -- Defazio, A., Bach F. & Lacoste-Julien S. (2014).\\n        SAGA: A Fast Incremental Gradient Method With Support\\n        for Non-Strongly Convex Composite Objectives\\n        https://arxiv.org/abs/1407.0202\\n\\n    Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent\\n        methods for logistic regression and maximum entropy models.\\n        Machine Learning 85(1-2):41-75.\\n        https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf\\n\\n    Examples\\n    --------\\n    >>> from sklearn.datasets import load_iris\\n    >>> from sklearn.linear_model import LogisticRegression\\n    >>> X, y = load_iris(return_X_y=True)\\n    >>> clf = LogisticRegression(random_state=0).fit(X, y)\\n    >>> clf.predict(X[:2, :])\\n    array([0, 0])\\n    >>> clf.predict_proba(X[:2, :])\\n    array([[9.8...e-01, 1.8...e-02, 1.4...e-08],\\n           [9.7...e-01, 2.8...e-02, ...e-08]])\\n    >>> clf.score(X, y)\\n    0.97...\\n    \"]] [:hr] [:hr]] [:div [:h3 \":sklearn.classification/logistic-regression-cv\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [17 2]:\\n\\n|              :name |  :default |\\n|--------------------|-----------|\\n|             :refit |      true |\\n|           :scoring |           |\\n|               :tol | 0.0001000 |\\n| :intercept-scaling |     1.000 |\\n|       :multi-class |      auto |\\n|            :solver |     lbfgs |\\n|           :penalty |        l2 |\\n|          :max-iter |       100 |\\n|            :n-jobs |           |\\n|      :random-state |           |\\n|              :dual |     false |\\n|     :fit-intercept |      true |\\n|                :cv |           |\\n|                :cs |        10 |\\n|      :class-weight |           |\\n|           :verbose |         0 |\\n|        :l-1-ratios |           |\\n\"]]] [:span [:p/markdown \"Logistic Regression CV (aka logit, MaxEnt) classifier.\\n\\n    See glossary entry for :term:`cross-validation estimator`.\\n\\n    This class implements logistic regression using liblinear, newton-cg, sag\\n    of lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2\\n    regularization with primal formulation. The liblinear solver supports both\\n    L1 and L2 regularization, with a dual formulation only for the L2 penalty.\\n    Elastic-Net penalty is only supported by the saga solver.\\n\\n    For the grid of `Cs` values and `l1_ratios` values, the best hyperparameter\\n    is selected by the cross-validator\\n    `~sklearn.model_selection.StratifiedKFold`, but it can be changed\\n    using the :term:`cv` parameter. The 'newton-cg', 'sag', 'saga' and 'lbfgs'\\n    solvers can warm-start the coefficients (see :term:`Glossary<warm_start>`).\\n\\n    Read more in the User Guide: `logistic_regression`.\\n\\n    Parameters\\n    ----------\\n- `Cs`: int or list of floats, default=10\\n        Each of the values in Cs describes the inverse of regularization\\n        strength. If Cs is as an int, then a grid of Cs values are chosen\\n        in a logarithmic scale between 1e-4 and 1e4.\\n        Like in support vector machines, smaller values specify stronger\\n        regularization.\\n\\n- `fit_intercept`: bool, default=True\\n        Specifies if a constant (a.k.a. bias or intercept) should be\\n        added to the decision function.\\n\\n- `cv`: int or cross-validation generator, default=None\\n        The default cross-validation generator used is Stratified K-Folds.\\n        If an integer is provided, then it is the number of folds used.\\n        See the module `sklearn.model_selection` module for the\\n        list of possible cross-validation objects.\\n\\n        *Changed in 0.22*\\n            ``cv`` default value if None changed from 3-fold to 5-fold.\\n\\n- `dual`: bool, default=False\\n        Dual or primal formulation. Dual formulation is only implemented for\\n        l2 penalty with liblinear solver. Prefer dual=False when\\n        n_samples > n_features.\\n\\n- `penalty`: {'l1', 'l2', 'elasticnet'}, default='l2'\\n        Used to specify the norm used in the penalization. The 'newton-cg',\\n        'sag' and 'lbfgs' solvers support only l2 penalties. 'elasticnet' is\\n        only supported by the 'saga' solver.\\n\\n- `scoring`: str or callable, default=None\\n        A string (see model evaluation documentation) or\\n        a scorer callable object / function with signature\\n        ``scorer(estimator, X, y)``. For a list of scoring functions\\n        that can be used, look at `sklearn.metrics`. The\\n        default scoring option used is 'accuracy'.\\n\\n- `solver`: {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'},             default='lbfgs'\\n\\n        Algorithm to use in the optimization problem.\\n\\n        - For small datasets, 'liblinear' is a good choice, whereas 'sag' and\\n          'saga' are faster for large ones.\\n        - For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs'\\n          handle multinomial loss; 'liblinear' is limited to one-versus-rest\\n          schemes.\\n        - 'newton-cg', 'lbfgs' and 'sag' only handle L2 penalty, whereas\\n          'liblinear' and 'saga' handle L1 penalty.\\n        - 'liblinear' might be slower in LogisticRegressionCV because it does\\n          not handle warm-starting.\\n\\n        Note that 'sag' and 'saga' fast convergence is only guaranteed on\\n        features with approximately the same scale. You can preprocess the data\\n        with a scaler from sklearn.preprocessing.\\n\\n        *Added in 0.17*\\n           Stochastic Average Gradient descent solver.\\n        *Added in 0.19*\\n           SAGA solver.\\n\\n- `tol`: float, default=1e-4\\n        Tolerance for stopping criteria.\\n\\n- `max_iter`: int, default=100\\n        Maximum number of iterations of the optimization algorithm.\\n\\n- `class_weight`: dict or 'balanced', default=None\\n        Weights associated with classes in the form ``{class_label: weight}``.\\n        If not given, all classes are supposed to have weight one.\\n\\n        The \\\"balanced\\\" mode uses the values of y to automatically adjust\\n        weights inversely proportional to class frequencies in the input data\\n        as ``n_samples / (n_classes * np.bincount(y))``.\\n\\n        Note that these weights will be multiplied with sample_weight (passed\\n        through the fit method) if sample_weight is specified.\\n\\n        *Added in 0.17*\\n           class_weight == 'balanced'\\n\\n- `n_jobs`: int, default=None\\n        Number of CPU cores used during the cross-validation loop.\\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\\n        for more details.\\n\\n- `verbose`: int, default=0\\n        For the 'liblinear', 'sag' and 'lbfgs' solvers set verbose to any\\n        positive number for verbosity.\\n\\n- `refit`: bool, default=True\\n        If set to True, the scores are averaged across all folds, and the\\n        coefs and the C that corresponds to the best score is taken, and a\\n        final refit is done using these parameters.\\n        Otherwise the coefs, intercepts and C that correspond to the\\n        best scores across folds are averaged.\\n\\n- `intercept_scaling`: float, default=1\\n        Useful only when the solver 'liblinear' is used\\n        and self.fit_intercept is set to True. In this case, x becomes\\n        [x, self.intercept_scaling],\\n        i.e. a \\\"synthetic\\\" feature with constant value equal to\\n        intercept_scaling is appended to the instance vector.\\n        The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\\n\\n        Note! the synthetic feature weight is subject to l1/l2 regularization\\n        as all other features.\\n        To lessen the effect of regularization on synthetic feature weight\\n        (and therefore on the intercept) intercept_scaling has to be increased.\\n\\n- `multi_class`: {'auto, 'ovr', 'multinomial'}, default='auto'\\n        If the option chosen is 'ovr', then a binary problem is fit for each\\n        label. For 'multinomial' the loss minimised is the multinomial loss fit\\n        across the entire probability distribution, *even when the data is\\n        binary*. 'multinomial' is unavailable when solver='liblinear'.\\n        'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\\n        and otherwise selects 'multinomial'.\\n\\n        *Added in 0.18*\\n           Stochastic Average Gradient descent solver for 'multinomial' case.\\n        *Changed in 0.22*\\n            Default changed from 'ovr' to 'auto' in 0.22.\\n\\n- `random_state`: int, RandomState instance, default=None\\n        Used when `solver='sag'`, 'saga' or 'liblinear' to shuffle the data.\\n        Note that this only applies to the solver and not the cross-validation\\n        generator. See :term:`Glossary <random_state>` for details.\\n\\n- `l1_ratios`: list of float, default=None\\n        The list of Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``.\\n        Only used if ``penalty='elasticnet'``. A value of 0 is equivalent to\\n        using ``penalty='l2'``, while 1 is equivalent to using\\n        ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a combination\\n        of L1 and L2.\\n\\n    Attributes\\n    ----------\\n- `classes_`: ndarray of shape (n_classes, )\\n        A list of class labels known to the classifier.\\n\\n- `coef_`: ndarray of shape (1, n_features) or (n_classes, n_features)\\n        Coefficient of the features in the decision function.\\n\\n        `coef_` is of shape (1, n_features) when the given problem\\n        is binary.\\n\\n- `intercept_`: ndarray of shape (1,) or (n_classes,)\\n        Intercept (a.k.a. bias) added to the decision function.\\n\\n        If `fit_intercept` is set to False, the intercept is set to zero.\\n        `intercept_` is of shape(1,) when the problem is binary.\\n\\n- `Cs_`: ndarray of shape (n_cs)\\n        Array of C i.e. inverse of regularization parameter values used\\n        for cross-validation.\\n\\n- `l1_ratios_`: ndarray of shape (n_l1_ratios)\\n        Array of l1_ratios used for cross-validation. If no l1_ratio is used\\n        (i.e. penalty is not 'elasticnet'), this is set to ``[None]``\\n\\n- `coefs_paths_`: ndarray of shape (n_folds, n_cs, n_features) or                    (n_folds, n_cs, n_features + 1)\\n        dict with classes as the keys, and the path of coefficients obtained\\n        during cross-validating across each fold and then across each Cs\\n        after doing an OvR for the corresponding class as values.\\n        If the 'multi_class' option is set to 'multinomial', then\\n        the coefs_paths are the coefficients corresponding to each class.\\n        Each dict value has shape ``(n_folds, n_cs, n_features)`` or\\n        ``(n_folds, n_cs, n_features + 1)`` depending on whether the\\n        intercept is fit or not. If ``penalty='elasticnet'``, the shape is\\n        ``(n_folds, n_cs, n_l1_ratios_, n_features)`` or\\n        ``(n_folds, n_cs, n_l1_ratios_, n_features + 1)``.\\n\\n- `scores_`: dict\\n        dict with classes as the keys, and the values as the\\n        grid of scores obtained during cross-validating each fold, after doing\\n        an OvR for the corresponding class. If the 'multi_class' option\\n        given is 'multinomial' then the same scores are repeated across\\n        all classes, since this is the multinomial class. Each dict value\\n        has shape ``(n_folds, n_cs`` or ``(n_folds, n_cs, n_l1_ratios)`` if\\n        ``penalty='elasticnet'``.\\n\\n- `C_`: ndarray of shape (n_classes,) or (n_classes - 1,)\\n        Array of C that maps to the best scores across every class. If refit is\\n        set to False, then for each class, the best C is the average of the\\n        C's that correspond to the best scores for each fold.\\n        `C_` is of shape(n_classes,) when the problem is binary.\\n\\n- `l1_ratio_`: ndarray of shape (n_classes,) or (n_classes - 1,)\\n        Array of l1_ratio that maps to the best scores across every class. If\\n        refit is set to False, then for each class, the best l1_ratio is the\\n        average of the l1_ratio's that correspond to the best scores for each\\n        fold.  `l1_ratio_` is of shape(n_classes,) when the problem is binary.\\n\\n- `n_iter_`: ndarray of shape (n_classes, n_folds, n_cs) or (1, n_folds, n_cs)\\n        Actual number of iterations for all classes, folds and Cs.\\n        In the binary or multinomial cases, the first dimension is equal to 1.\\n        If ``penalty='elasticnet'``, the shape is ``(n_classes, n_folds,\\n        n_cs, n_l1_ratios)`` or ``(1, n_folds, n_cs, n_l1_ratios)``.\\n\\n\\n    Examples\\n    --------\\n    >>> from sklearn.datasets import load_iris\\n    >>> from sklearn.linear_model import LogisticRegressionCV\\n    >>> X, y = load_iris(return_X_y=True)\\n    >>> clf = LogisticRegressionCV(cv=5, random_state=0).fit(X, y)\\n    >>> clf.predict(X[:2, :])\\n    array([0, 0])\\n    >>> clf.predict_proba(X[:2, :]).shape\\n    (2, 3)\\n    >>> clf.score(X, y)\\n    0.98...\\n\\n    See also\\n    --------\\n    LogisticRegression\\n\\n    \"]] [:hr] [:hr]] [:div [:h3 \":sklearn.classification/mlp-classifier\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [23 2]:\\n\\n|                :name |  :default |\\n|----------------------|-----------|\\n|    :n-iter-no-change |        10 |\\n|       :learning-rate |  constant |\\n|          :activation |      relu |\\n|  :hidden-layer-sizes |     [100] |\\n|                 :tol | 0.0001000 |\\n|              :beta-2 |    0.9990 |\\n|      :early-stopping |     false |\\n|  :nesterovs-momentum |      true |\\n|          :batch-size |      auto |\\n|              :solver |      adam |\\n|             :shuffle |      true |\\n|             :power-t |    0.5000 |\\n|             :max-fun |     15000 |\\n|              :beta-1 |    0.9000 |\\n|            :max-iter |       200 |\\n|        :random-state |           |\\n|            :momentum |    0.9000 |\\n|  :learning-rate-init |  0.001000 |\\n|               :alpha | 0.0001000 |\\n|          :warm-start |     false |\\n| :validation-fraction |    0.1000 |\\n|             :verbose |     false |\\n|             :epsilon | 1.000E-08 |\\n\"]]] [:span [:p/markdown \"Multi-layer Perceptron classifier.\\n\\n    This model optimizes the log-loss function using LBFGS or stochastic\\n    gradient descent.\\n\\n    *Added in 0.18*\\n\\n    Parameters\\n    ----------\\n- `hidden_layer_sizes`: tuple, length = n_layers - 2, default=(100,)\\n        The ith element represents the number of neurons in the ith\\n        hidden layer.\\n\\n- `activation`: {'identity', 'logistic', 'tanh', 'relu'}, default='relu'\\n        Activation function for the hidden layer.\\n\\n        - 'identity', no-op activation, useful to implement linear bottleneck,\\n          returns f(x) = x\\n\\n        - 'logistic', the logistic sigmoid function,\\n          returns f(x) = 1 / (1 + exp(-x)).\\n\\n        - 'tanh', the hyperbolic tan function,\\n          returns f(x) = tanh(x).\\n\\n        - 'relu', the rectified linear unit function,\\n          returns f(x) = max(0, x)\\n\\n- `solver`: {'lbfgs', 'sgd', 'adam'}, default='adam'\\n        The solver for weight optimization.\\n\\n        - 'lbfgs' is an optimizer in the family of quasi-Newton methods.\\n\\n        - 'sgd' refers to stochastic gradient descent.\\n\\n        - 'adam' refers to a stochastic gradient-based optimizer proposed\\n          by Kingma, Diederik, and Jimmy Ba\\n\\n        Note: The default solver 'adam' works pretty well on relatively\\n        large datasets (with thousands of training samples or more) in terms of\\n        both training time and validation score.\\n        For small datasets, however, 'lbfgs' can converge faster and perform\\n        better.\\n\\n- `alpha`: float, default=0.0001\\n        L2 penalty (regularization term) parameter.\\n\\n- `batch_size`: int, default='auto'\\n        Size of minibatches for stochastic optimizers.\\n        If the solver is 'lbfgs', the classifier will not use minibatch.\\n        When set to \\\"auto\\\", `batch_size=min(200, n_samples)`\\n\\n- `learning_rate`: {'constant', 'invscaling', 'adaptive'}, default='constant'\\n        Learning rate schedule for weight updates.\\n\\n        - 'constant' is a constant learning rate given by\\n          'learning_rate_init'.\\n\\n        - 'invscaling' gradually decreases the learning rate at each\\n          time step 't' using an inverse scaling exponent of 'power_t'.\\n          effective_learning_rate = learning_rate_init / pow(t, power_t)\\n\\n        - 'adaptive' keeps the learning rate constant to\\n          'learning_rate_init' as long as training loss keeps decreasing.\\n          Each time two consecutive epochs fail to decrease training loss by at\\n          least tol, or fail to increase validation score by at least tol if\\n          'early_stopping' is on, the current learning rate is divided by 5.\\n\\n        Only used when ``solver='sgd'``.\\n\\n- `learning_rate_init`: double, default=0.001\\n        The initial learning rate used. It controls the step-size\\n        in updating the weights. Only used when solver='sgd' or 'adam'.\\n\\n- `power_t`: double, default=0.5\\n        The exponent for inverse scaling learning rate.\\n        It is used in updating effective learning rate when the learning_rate\\n        is set to 'invscaling'. Only used when solver='sgd'.\\n\\n- `max_iter`: int, default=200\\n        Maximum number of iterations. The solver iterates until convergence\\n        (determined by 'tol') or this number of iterations. For stochastic\\n        solvers ('sgd', 'adam'), note that this determines the number of epochs\\n        (how many times each data point will be used), not the number of\\n        gradient steps.\\n\\n- `shuffle`: bool, default=True\\n        Whether to shuffle samples in each iteration. Only used when\\n        solver='sgd' or 'adam'.\\n\\n- `random_state`: int, RandomState instance, default=None\\n        Determines random number generation for weights and bias\\n        initialization, train-test split if early stopping is used, and batch\\n        sampling when solver='sgd' or 'adam'.\\n        Pass an int for reproducible results across multiple function calls.\\n        See :term:`Glossary <random_state>`.\\n\\n- `tol`: float, default=1e-4\\n        Tolerance for the optimization. When the loss or score is not improving\\n        by at least ``tol`` for ``n_iter_no_change`` consecutive iterations,\\n        unless ``learning_rate`` is set to 'adaptive', convergence is\\n        considered to be reached and training stops.\\n\\n- `verbose`: bool, default=False\\n        Whether to print progress messages to stdout.\\n\\n- `warm_start`: bool, default=False\\n        When set to True, reuse the solution of the previous\\n        call to fit as initialization, otherwise, just erase the\\n        previous solution. See :term:`the Glossary <warm_start>`.\\n\\n- `momentum`: float, default=0.9\\n        Momentum for gradient descent update. Should be between 0 and 1. Only\\n        used when solver='sgd'.\\n\\n- `nesterovs_momentum`: boolean, default=True\\n        Whether to use Nesterov's momentum. Only used when solver='sgd' and\\n        momentum > 0.\\n\\n- `early_stopping`: bool, default=False\\n        Whether to use early stopping to terminate training when validation\\n        score is not improving. If set to true, it will automatically set\\n        aside 10% of training data as validation and terminate training when\\n        validation score is not improving by at least tol for\\n        ``n_iter_no_change`` consecutive epochs. The split is stratified,\\n        except in a multilabel setting.\\n        Only effective when solver='sgd' or 'adam'\\n\\n- `validation_fraction`: float, default=0.1\\n        The proportion of training data to set aside as validation set for\\n        early stopping. Must be between 0 and 1.\\n        Only used if early_stopping is True\\n\\n- `beta_1`: float, default=0.9\\n        Exponential decay rate for estimates of first moment vector in adam,\\n        should be in [0, 1). Only used when solver='adam'\\n\\n- `beta_2`: float, default=0.999\\n        Exponential decay rate for estimates of second moment vector in adam,\\n        should be in [0, 1). Only used when solver='adam'\\n\\n- `epsilon`: float, default=1e-8\\n        Value for numerical stability in adam. Only used when solver='adam'\\n\\n- `n_iter_no_change`: int, default=10\\n        Maximum number of epochs to not meet ``tol`` improvement.\\n        Only effective when solver='sgd' or 'adam'\\n\\n        *Added in 0.20*\\n\\n- `max_fun`: int, default=15000\\n        Only used when solver='lbfgs'. Maximum number of loss function calls.\\n        The solver iterates until convergence (determined by 'tol'), number\\n        of iterations reaches max_iter, or this number of loss function calls.\\n        Note that number of loss function calls will be greater than or equal\\n        to the number of iterations for the `MLPClassifier`.\\n\\n        *Added in 0.22*\\n\\n    Attributes\\n    ----------\\n- `classes_`: ndarray or list of ndarray of shape (n_classes,)\\n        Class labels for each output.\\n\\n- `loss_`: float\\n        The current loss computed with the loss function.\\n\\n- `coefs_`: list, length n_layers - 1\\n        The ith element in the list represents the weight matrix corresponding\\n        to layer i.\\n\\n- `intercepts_`: list, length n_layers - 1\\n        The ith element in the list represents the bias vector corresponding to\\n        layer i + 1.\\n\\n- `n_iter_`: int,\\n        The number of iterations the solver has ran.\\n\\n- `n_layers_`: int\\n        Number of layers.\\n\\n- `n_outputs_`: int\\n        Number of outputs.\\n\\n- `out_activation_`: string\\n        Name of the output activation function.\\n\\n\\n    Examples\\n    --------\\n    >>> from sklearn.neural_network import MLPClassifier\\n    >>> from sklearn.datasets import make_classification\\n    >>> from sklearn.model_selection import train_test_split\\n    >>> X, y = make_classification(n_samples=100, random_state=1)\\n    >>> X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,\\n    ...                                                     random_state=1)\\n    >>> clf = MLPClassifier(random_state=1, max_iter=300).fit(X_train, y_train)\\n    >>> clf.predict_proba(X_test[:1])\\n    array([[0.038..., 0.961...]])\\n    >>> clf.predict(X_test[:5, :])\\n    array([1, 0, 1, 0, 1])\\n    >>> clf.score(X_test, y_test)\\n    0.8...\\n\\n    Notes\\n    -----\\n    MLPClassifier trains iteratively since at each time step\\n    the partial derivatives of the loss function with respect to the model\\n    parameters are computed to update the parameters.\\n\\n    It can also have a regularization term added to the loss function\\n    that shrinks model parameters to prevent overfitting.\\n\\n    This implementation works with data represented as dense numpy arrays or\\n    sparse scipy arrays of floating point values.\\n\\n    References\\n    ----------\\n    Hinton, Geoffrey E.\\n        \\\"Connectionist learning procedures.\\\" Artificial intelligence 40.1\\n        (1989): 185-234.\\n\\n    Glorot, Xavier, and Yoshua Bengio. \\\"Understanding the difficulty of\\n        training deep feedforward neural networks.\\\" International Conference\\n        on Artificial Intelligence and Statistics. 2010.\\n\\n    He, Kaiming, et al. \\\"Delving deep into rectifiers: Surpassing human-level\\n        performance on imagenet classification.\\\" arXiv preprint\\n        arXiv:1502.01852 (2015).\\n\\n    Kingma, Diederik, and Jimmy Ba. \\\"Adam: A method for stochastic\\n        optimization.\\\" arXiv preprint arXiv:1412.6980 (2014).\\n    \"]] [:hr] [:hr]] [:div [:h3 \":sklearn.classification/multinomial-nb\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [3 2]:\\n\\n|        :name | :default |\\n|--------------|----------|\\n|       :alpha |    1.000 |\\n| :class-prior |          |\\n|   :fit-prior |     true |\\n\"]]] [:span [:p/markdown \"\\n    Naive Bayes classifier for multinomial models\\n\\n    The multinomial Naive Bayes classifier is suitable for classification with\\n    discrete features (e.g., word counts for text classification). The\\n    multinomial distribution normally requires integer feature counts. However,\\n    in practice, fractional counts such as tf-idf may also work.\\n\\n    Read more in the User Guide: `multinomial_naive_bayes`.\\n\\n    Parameters\\n    ----------\\n- `alpha`: float, default=1.0\\n        Additive (Laplace/Lidstone) smoothing parameter\\n        (0 for no smoothing).\\n\\n- `fit_prior`: bool, default=True\\n        Whether to learn class prior probabilities or not.\\n        If false, a uniform prior will be used.\\n\\n- `class_prior`: array-like of shape (n_classes,), default=None\\n        Prior probabilities of the classes. If specified the priors are not\\n        adjusted according to the data.\\n\\n    Attributes\\n    ----------\\n- `class_count_`: ndarray of shape (n_classes,)\\n        Number of samples encountered for each class during fitting. This\\n        value is weighted by the sample weight when provided.\\n\\n- `class_log_prior_`: ndarray of shape (n_classes, )\\n        Smoothed empirical log probability for each class.\\n\\n- `classes_`: ndarray of shape (n_classes,)\\n        Class labels known to the classifier\\n\\n- `coef_`: ndarray of shape (n_classes, n_features)\\n        Mirrors ``feature_log_prob_`` for interpreting MultinomialNB\\n        as a linear model.\\n\\n- `feature_count_`: ndarray of shape (n_classes, n_features)\\n        Number of samples encountered for each (class, feature)\\n        during fitting. This value is weighted by the sample weight when\\n        provided.\\n\\n- `feature_log_prob_`: ndarray of shape (n_classes, n_features)\\n        Empirical log probability of features\\n        given a class, ``P(x_i|y)``.\\n\\n- `intercept_`: ndarray of shape (n_classes, )\\n        Mirrors ``class_log_prior_`` for interpreting MultinomialNB\\n        as a linear model.\\n\\n- `n_features_`: int\\n        Number of features of each sample.\\n\\n    Examples\\n    --------\\n    >>> import numpy as np\\n    >>> rng = np.random.RandomState(1)\\n    >>> X = rng.randint(5, size=(6, 100))\\n    >>> y = np.array([1, 2, 3, 4, 5, 6])\\n    >>> from sklearn.naive_bayes import MultinomialNB\\n    >>> clf = MultinomialNB()\\n    >>> clf.fit(X, y)\\n    MultinomialNB()\\n    >>> print(clf.predict(X[2:3]))\\n    [3]\\n\\n    Notes\\n    -----\\n    For the rationale behind the names `coef_` and `intercept_`, i.e.\\n    naive Bayes as a linear classifier, see J. Rennie et al. (2003),\\n    Tackling the poor assumptions of naive Bayes text classifiers, ICML.\\n\\n    References\\n    ----------\\n    C.D. Manning, P. Raghavan and H. Schuetze (2008). Introduction to\\n    Information Retrieval. Cambridge University Press, pp. 234-265.\\n    https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html\\n    \"]] [:hr] [:hr]] [:div [:h3 \":sklearn.classification/nearest-centroid\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [2 2]:\\n\\n|             :name |  :default |\\n|-------------------|-----------|\\n|           :metric | euclidean |\\n| :shrink-threshold |           |\\n\"]]] [:span [:p/markdown \"Nearest centroid classifier.\\n\\n    Each class is represented by its centroid, with test samples classified to\\n    the class with the nearest centroid.\\n\\n    Read more in the User Guide: `nearest_centroid_classifier`.\\n\\n    Parameters\\n    ----------\\n- `metric`: str or callable\\n        The metric to use when calculating distance between instances in a\\n        feature array. If metric is a string or callable, it must be one of\\n        the options allowed by metrics.pairwise.pairwise_distances for its\\n        metric parameter.\\n        The centroids for the samples corresponding to each class is the point\\n        from which the sum of the distances (according to the metric) of all\\n        samples that belong to that particular class are minimized.\\n        If the \\\"manhattan\\\" metric is provided, this centroid is the median and\\n        for all other metrics, the centroid is now set to be the mean.\\n\\n        *Changed in 0.19*\\n            ``metric='precomputed'`` was deprecated and now raises an error\\n\\n- `shrink_threshold`: float, default=None\\n        Threshold for shrinking centroids to remove features.\\n\\n    Attributes\\n    ----------\\n- `centroids_`: array-like of shape (n_classes, n_features)\\n        Centroid of each class.\\n\\n- `classes_`: array of shape (n_classes,)\\n        The unique classes labels.\\n\\n    Examples\\n    --------\\n    >>> from sklearn.neighbors import NearestCentroid\\n    >>> import numpy as np\\n    >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\\n    >>> y = np.array([1, 1, 1, 2, 2, 2])\\n    >>> clf = NearestCentroid()\\n    >>> clf.fit(X, y)\\n    NearestCentroid()\\n    >>> print(clf.predict([[-0.8, -1]]))\\n    [1]\\n\\n    See also\\n    --------\\n    sklearn.neighbors.KNeighborsClassifier: nearest neighbors classifier\\n\\n    Notes\\n    -----\\n    When used for text classification with tf-idf vectors, this classifier is\\n    also known as the Rocchio classifier.\\n\\n    References\\n    ----------\\n    Tibshirani, R., Hastie, T., Narasimhan, B., & Chu, G. (2002). Diagnosis of\\n    multiple cancer types by shrunken centroids of gene expression. Proceedings\\n    of the National Academy of Sciences of the United States of America,\\n    99(10), 6567-6572. The National Academy of Sciences.\\n\\n    \"]] [:hr] [:hr]] [:div [:h3 \":sklearn.classification/nu-svc\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [15 2]:\\n\\n|                    :name | :default |\\n|--------------------------|----------|\\n|              :break-ties |    false |\\n|                  :kernel |      rbf |\\n|                   :gamma |    scale |\\n|                  :degree |        3 |\\n| :decision-function-shape |      ovr |\\n|             :probability |    false |\\n|                     :tol | 0.001000 |\\n|                      :nu |   0.5000 |\\n|               :shrinking |     true |\\n|                :max-iter |       -1 |\\n|            :random-state |          |\\n|                  :coef-0 |    0.000 |\\n|            :class-weight |          |\\n|              :cache-size |      200 |\\n|                 :verbose |    false |\\n\"]]] [:span [:p/markdown \"Nu-Support Vector Classification.\\n\\n    Similar to SVC but uses a parameter to control the number of support\\n    vectors.\\n\\n    The implementation is based on libsvm.\\n\\n    Read more in the User Guide: `svm_classification`.\\n\\n    Parameters\\n    ----------\\n- `nu`: float, default=0.5\\n        An upper bound on the fraction of margin errors (see User Guide: `nu_svc`) and a lower bound of the fraction of support vectors.\\n        Should be in the interval (0, 1].\\n\\n- `kernel`: {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'}, default='rbf'\\n         Specifies the kernel type to be used in the algorithm.\\n         It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or\\n         a callable.\\n         If none is given, 'rbf' will be used. If a callable is given it is\\n         used to precompute the kernel matrix.\\n\\n- `degree`: int, default=3\\n        Degree of the polynomial kernel function ('poly').\\n        Ignored by all other kernels.\\n\\n- `gamma`: {'scale', 'auto'} or float, default='scale'\\n        Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.\\n\\n        - if ``gamma='scale'`` (default) is passed then it uses\\n          1 / (n_features * X.var()) as value of gamma,\\n        - if 'auto', uses 1 / n_features.\\n\\n        *Changed in 0.22*\\n           The default value of ``gamma`` changed from 'auto' to 'scale'.\\n\\n- `coef0`: float, default=0.0\\n        Independent term in kernel function.\\n        It is only significant in 'poly' and 'sigmoid'.\\n\\n- `shrinking`: bool, default=True\\n        Whether to use the shrinking heuristic.\\n        See the User Guide: `shrinking_svm`.\\n\\n- `probability`: bool, default=False\\n        Whether to enable probability estimates. This must be enabled prior\\n        to calling `fit`, will slow down that method as it internally uses\\n        5-fold cross-validation, and `predict_proba` may be inconsistent with\\n        `predict`. Read more in the User Guide: `scores_probabilities`.\\n\\n- `tol`: float, default=1e-3\\n        Tolerance for stopping criterion.\\n\\n- `cache_size`: float, default=200\\n        Specify the size of the kernel cache (in MB).\\n\\n- `class_weight`: {dict, 'balanced'}, default=None\\n        Set the parameter C of class i to class_weight[i]*C for\\n        SVC. If not given, all classes are supposed to have\\n        weight one. The \\\"balanced\\\" mode uses the values of y to automatically\\n        adjust weights inversely proportional to class frequencies as\\n        ``n_samples / (n_classes * np.bincount(y))``\\n\\n- `verbose`: bool, default=False\\n        Enable verbose output. Note that this setting takes advantage of a\\n        per-process runtime setting in libsvm that, if enabled, may not work\\n        properly in a multithreaded context.\\n\\n- `max_iter`: int, default=-1\\n        Hard limit on iterations within solver, or -1 for no limit.\\n\\n- `decision_function_shape`: {'ovo', 'ovr'}, default='ovr'\\n        Whether to return a one-vs-rest ('ovr') decision function of shape\\n        (n_samples, n_classes) as all other classifiers, or the original\\n        one-vs-one ('ovo') decision function of libsvm which has shape\\n        (n_samples, n_classes * (n_classes - 1) / 2). However, one-vs-one\\n        ('ovo') is always used as multi-class strategy. The parameter is\\n        ignored for binary classification.\\n\\n        *Changed in 0.19*\\n            decision_function_shape is 'ovr' by default.\\n\\n        *Added in 0.17*\\n           *decision_function_shape='ovr'* is recommended.\\n\\n        *Changed in 0.17*\\n           Deprecated *decision_function_shape='ovo' and None*.\\n\\n- `break_ties`: bool, default=False\\n        If true, ``decision_function_shape='ovr'``, and number of classes > 2,\\n        :term:`predict` will break ties according to the confidence values of\\n        :term:`decision_function`; otherwise the first class among the tied\\n        classes is returned. Please note that breaking ties comes at a\\n        relatively high computational cost compared to a simple predict.\\n\\n        *Added in 0.22*\\n\\n- `random_state`: int or RandomState instance, default=None\\n        Controls the pseudo random number generation for shuffling the data for\\n        probability estimates. Ignored when `probability` is False.\\n        Pass an int for reproducible output across multiple function calls.\\n        See :term:`Glossary <random_state>`.\\n\\n    Attributes\\n    ----------\\n- `support_`: ndarray of shape (n_SV,)\\n        Indices of support vectors.\\n\\n- `support_vectors_`: ndarray of shape (n_SV, n_features)\\n        Support vectors.\\n\\n- `n_support_`: ndarray of shape (n_class), dtype=int32\\n        Number of support vectors for each class.\\n\\n- `dual_coef_`: ndarray of shape (n_class-1, n_SV)\\n        Dual coefficients of the support vector in the decision\\n        function (see :ref:`sgd_mathematical_formulation`), multiplied by\\n        their targets.\\n        For multiclass, coefficient for all 1-vs-1 classifiers.\\n        The layout of the coefficients in the multiclass case is somewhat\\n        non-trivial. See the multi-class section of the User Guide: `svm_multi_class` for details.\\n\\n- `coef_`: ndarray of shape (n_class * (n_class-1) / 2, n_features)\\n        Weights assigned to the features (coefficients in the primal\\n        problem). This is only available in the case of a linear kernel.\\n\\n        `coef_` is readonly property derived from `dual_coef_` and\\n        `support_vectors_`.\\n\\n- `intercept_`: ndarray of shape (n_class * (n_class-1) / 2,)\\n        Constants in decision function.\\n\\n- `classes_`: ndarray of shape (n_classes,)\\n        The unique classes labels.\\n\\n- `fit_status_`: int\\n        0 if correctly fitted, 1 if the algorithm did not converge.\\n\\n- `probA_`: ndarray of shape (n_class * (n_class-1) / 2,)\\n- `probB_`: ndarray of shape (n_class * (n_class-1) / 2,)\\n        If `probability=True`, it corresponds to the parameters learned in\\n        Platt scaling to produce probability estimates from decision values.\\n        If `probability=False`, it's an empty array. Platt scaling uses the\\n        logistic function\\n        ``1 / (1 + exp(decision_value * probA_ + probB_))``\\n        where ``probA_`` and ``probB_`` are learned from the dataset [2]_. For\\n        more information on the multiclass case and training procedure see\\n        section 8 of [1]_.\\n\\n- `class_weight_`: ndarray of shape (n_class,)\\n        Multipliers of parameter C of each class.\\n        Computed based on the ``class_weight`` parameter.\\n\\n- `shape_fit_`: tuple of int of shape (n_dimensions_of_X,)\\n        Array dimensions of training vector ``X``.\\n\\n    Examples\\n    --------\\n    >>> import numpy as np\\n    >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\\n    >>> y = np.array([1, 1, 2, 2])\\n    >>> from sklearn.pipeline import make_pipeline\\n    >>> from sklearn.preprocessing import StandardScaler\\n    >>> from sklearn.svm import NuSVC\\n    >>> clf = make_pipeline(StandardScaler(), NuSVC())\\n    >>> clf.fit(X, y)\\n    Pipeline(steps=[('standardscaler', StandardScaler()), ('nusvc', NuSVC())])\\n    >>> print(clf.predict([[-0.8, -1]]))\\n    [1]\\n\\n    See also\\n    --------\\n    SVC\\n        Support Vector Machine for classification using libsvm.\\n\\n    LinearSVC\\n        Scalable linear Support Vector Machine for classification using\\n        liblinear.\\n\\n    References\\n    ----------\\n - [1] [LIBSVM: A Library for Support Vector Machines\\n        ](http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf)\\n\\n - [2] [Platt, John (1999). \\\"Probabilistic outputs for support vector\\n        machines and comparison to regularizedlikelihood methods.\\\"\\n        ](http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.41.1639)\\n    \"]] [:hr] [:hr]] [:div [:h3 \":sklearn.classification/passive-aggressive-classifier\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [15 2]:\\n\\n|                :name | :default |\\n|----------------------|----------|\\n|    :n-iter-no-change |        5 |\\n|             :average |    false |\\n|                 :tol | 0.001000 |\\n|      :early-stopping |    false |\\n|             :shuffle |     true |\\n|                   :c |    1.000 |\\n|            :max-iter |     1000 |\\n|              :n-jobs |          |\\n|        :random-state |          |\\n|       :fit-intercept |     true |\\n|          :warm-start |    false |\\n| :validation-fraction |   0.1000 |\\n|        :class-weight |          |\\n|                :loss |    hinge |\\n|             :verbose |        0 |\\n\"]]] [:span [:p/markdown \"Passive Aggressive Classifier\\n\\n    Read more in the User Guide: `passive_aggressive`.\\n\\n    Parameters\\n    ----------\\n\\n- `C`: float\\n        Maximum step size (regularization). Defaults to 1.0.\\n\\n- `fit_intercept`: bool, default=False\\n        Whether the intercept should be estimated or not. If False, the\\n        data is assumed to be already centered.\\n\\n- `max_iter`: int, optional (default=1000)\\n        The maximum number of passes over the training data (aka epochs).\\n        It only impacts the behavior in the ``fit`` method, and not the\\n        `partial_fit` method.\\n\\n        *Added in 0.19*\\n\\n- `tol`: float or None, optional (default=1e-3)\\n        The stopping criterion. If it is not None, the iterations will stop\\n        when (loss > previous_loss - tol).\\n\\n        *Added in 0.19*\\n\\n- `early_stopping`: bool, default=False\\n        Whether to use early stopping to terminate training when validation.\\n        score is not improving. If set to True, it will automatically set aside\\n        a stratified fraction of training data as validation and terminate\\n        training when validation score is not improving by at least tol for\\n        n_iter_no_change consecutive epochs.\\n\\n        *Added in 0.20*\\n\\n- `validation_fraction`: float, default=0.1\\n        The proportion of training data to set aside as validation set for\\n        early stopping. Must be between 0 and 1.\\n        Only used if early_stopping is True.\\n\\n        *Added in 0.20*\\n\\n- `n_iter_no_change`: int, default=5\\n        Number of iterations with no improvement to wait before early stopping.\\n\\n        *Added in 0.20*\\n\\n- `shuffle`: bool, default=True\\n        Whether or not the training data should be shuffled after each epoch.\\n\\n- `verbose`: integer, optional\\n        The verbosity level\\n\\n- `loss`: string, optional\\n        The loss function to be used:\\n        hinge: equivalent to PA-I in the reference paper.\\n        squared_hinge: equivalent to PA-II in the reference paper.\\n\\n- `n_jobs`: int or None, optional (default=None)\\n        The number of CPUs to use to do the OVA (One Versus All, for\\n        multi-class problems) computation.\\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\\n        for more details.\\n\\n- `random_state`: int, RandomState instance, default=None\\n        Used to shuffle the training data, when ``shuffle`` is set to\\n        ``True``. Pass an int for reproducible output across multiple\\n        function calls.\\n        See :term:`Glossary <random_state>`.\\n\\n- `warm_start`: bool, optional\\n        When set to True, reuse the solution of the previous call to fit as\\n        initialization, otherwise, just erase the previous solution.\\n        See :term:`the Glossary <warm_start>`.\\n\\n        Repeatedly calling fit or partial_fit when warm_start is True can\\n        result in a different solution than when calling fit a single time\\n        because of the way the data is shuffled.\\n\\n- `class_weight`: dict, {class_label: weight} or \\\"balanced\\\" or None, optional\\n        Preset for the class_weight fit parameter.\\n\\n        Weights associated with classes. If not given, all classes\\n        are supposed to have weight one.\\n\\n        The \\\"balanced\\\" mode uses the values of y to automatically adjust\\n        weights inversely proportional to class frequencies in the input data\\n        as ``n_samples / (n_classes * np.bincount(y))``\\n\\n        *Added in 0.17*\\n           parameter *class_weight* to automatically weight samples.\\n\\n- `average`: bool or int, optional\\n        When set to True, computes the averaged SGD weights and stores the\\n        result in the ``coef_`` attribute. If set to an int greater than 1,\\n        averaging will begin once the total number of samples seen reaches\\n        average. So average=10 will begin averaging after seeing 10 samples.\\n\\n        *Added in 0.19*\\n           parameter *average* to use weights averaging in SGD\\n\\n    Attributes\\n    ----------\\n- `coef_`: array, shape = [1, n_features] if n_classes == 2 else [n_classes,            n_features]\\n        Weights assigned to the features.\\n\\n- `intercept_`: array, shape = [1] if n_classes == 2 else [n_classes]\\n        Constants in decision function.\\n\\n- `n_iter_`: int\\n        The actual number of iterations to reach the stopping criterion.\\n        For multiclass fits, it is the maximum over every binary fit.\\n\\n- `classes_`: array of shape (n_classes,)\\n        The unique classes labels.\\n\\n- `t_`: int\\n        Number of weight updates performed during training.\\n        Same as ``(n_iter_ * n_samples)``.\\n\\n- `loss_function_`: callable\\n        Loss function used by the algorithm.\\n\\n    Examples\\n    --------\\n    >>> from sklearn.linear_model import PassiveAggressiveClassifier\\n    >>> from sklearn.datasets import make_classification\\n\\n    >>> X, y = make_classification(n_features=4, random_state=0)\\n    >>> clf = PassiveAggressiveClassifier(max_iter=1000, random_state=0,\\n    ... tol=1e-3)\\n    >>> clf.fit(X, y)\\n    PassiveAggressiveClassifier(random_state=0)\\n    >>> print(clf.coef_)\\n    [[0.26642044 0.45070924 0.67251877 0.64185414]]\\n    >>> print(clf.intercept_)\\n    [1.84127814]\\n    >>> print(clf.predict([[0, 0, 0, 0]]))\\n    [1]\\n\\n    See also\\n    --------\\n\\n    SGDClassifier\\n    Perceptron\\n\\n    References\\n    ----------\\n    Online Passive-Aggressive Algorithms\\n    <http://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf>\\n    K. Crammer, O. Dekel, J. Keshat, S. Shalev-Shwartz, Y. Singer - JMLR (2006)\\n\\n    \"]] [:hr] [:hr]] [:div [:h3 \":sklearn.classification/perceptron\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [15 2]:\\n\\n|                :name |  :default |\\n|----------------------|-----------|\\n|    :n-iter-no-change |     5.000 |\\n|                 :tol |  0.001000 |\\n|      :early-stopping |     false |\\n|               :eta-0 |     1.000 |\\n|             :shuffle |      true |\\n|             :penalty |           |\\n|            :max-iter |      1000 |\\n|              :n-jobs |           |\\n|        :random-state |         0 |\\n|       :fit-intercept |      true |\\n|               :alpha | 0.0001000 |\\n|          :warm-start |     false |\\n| :validation-fraction |    0.1000 |\\n|        :class-weight |           |\\n|             :verbose |         0 |\\n\"]]] [:span [:p/markdown \"Perceptron\\n\\n    Read more in the User Guide: `perceptron`.\\n\\n    Parameters\\n    ----------\\n\\n- `penalty`: {'l2','l1','elasticnet'}, default=None\\n        The penalty (aka regularization term) to be used.\\n\\n- `alpha`: float, default=0.0001\\n        Constant that multiplies the regularization term if regularization is\\n        used.\\n\\n- `fit_intercept`: bool, default=True\\n        Whether the intercept should be estimated or not. If False, the\\n        data is assumed to be already centered.\\n\\n- `max_iter`: int, default=1000\\n        The maximum number of passes over the training data (aka epochs).\\n        It only impacts the behavior in the ``fit`` method, and not the\\n        `partial_fit` method.\\n\\n        *Added in 0.19*\\n\\n- `tol`: float, default=1e-3\\n        The stopping criterion. If it is not None, the iterations will stop\\n        when (loss > previous_loss - tol).\\n\\n        *Added in 0.19*\\n\\n- `shuffle`: bool, default=True\\n        Whether or not the training data should be shuffled after each epoch.\\n\\n- `verbose`: int, default=0\\n        The verbosity level\\n\\n- `eta0`: double, default=1\\n        Constant by which the updates are multiplied.\\n\\n- `n_jobs`: int, default=None\\n        The number of CPUs to use to do the OVA (One Versus All, for\\n        multi-class problems) computation.\\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\\n        for more details.\\n\\n- `random_state`: int, RandomState instance, default=None\\n        Used to shuffle the training data, when ``shuffle`` is set to\\n        ``True``. Pass an int for reproducible output across multiple\\n        function calls.\\n        See :term:`Glossary <random_state>`.\\n\\n- `early_stopping`: bool, default=False\\n        Whether to use early stopping to terminate training when validation.\\n        score is not improving. If set to True, it will automatically set aside\\n        a stratified fraction of training data as validation and terminate\\n        training when validation score is not improving by at least tol for\\n        n_iter_no_change consecutive epochs.\\n\\n        *Added in 0.20*\\n\\n- `validation_fraction`: float, default=0.1\\n        The proportion of training data to set aside as validation set for\\n        early stopping. Must be between 0 and 1.\\n        Only used if early_stopping is True.\\n\\n        *Added in 0.20*\\n\\n- `n_iter_no_change`: int, default=5\\n        Number of iterations with no improvement to wait before early stopping.\\n\\n        *Added in 0.20*\\n\\n- `class_weight`: dict, {class_label: weight} or \\\"balanced\\\", default=None\\n        Preset for the class_weight fit parameter.\\n\\n        Weights associated with classes. If not given, all classes\\n        are supposed to have weight one.\\n\\n        The \\\"balanced\\\" mode uses the values of y to automatically adjust\\n        weights inversely proportional to class frequencies in the input data\\n        as ``n_samples / (n_classes * np.bincount(y))``\\n\\n- `warm_start`: bool, default=False\\n        When set to True, reuse the solution of the previous call to fit as\\n        initialization, otherwise, just erase the previous solution. See\\n        :term:`the Glossary <warm_start>`.\\n\\n    Attributes\\n    ----------\\n- `coef_`: ndarray of shape = [1, n_features] if n_classes == 2 else         [n_classes, n_features]\\n        Weights assigned to the features.\\n\\n- `intercept_`: ndarray of shape = [1] if n_classes == 2 else [n_classes]\\n        Constants in decision function.\\n\\n- `n_iter_`: int\\n        The actual number of iterations to reach the stopping criterion.\\n        For multiclass fits, it is the maximum over every binary fit.\\n\\n- `classes_`: ndarray of shape (n_classes,)\\n        The unique classes labels.\\n\\n- `t_`: int\\n        Number of weight updates performed during training.\\n        Same as ``(n_iter_ * n_samples)``.\\n\\n    Notes\\n    -----\\n\\n    ``Perceptron`` is a classification algorithm which shares the same\\n    underlying implementation with ``SGDClassifier``. In fact,\\n    ``Perceptron()`` is equivalent to `SGDClassifier(loss=\\\"perceptron\\\",\\n    eta0=1, learning_rate=\\\"constant\\\", penalty=None)`.\\n\\n    Examples\\n    --------\\n    >>> from sklearn.datasets import load_digits\\n    >>> from sklearn.linear_model import Perceptron\\n    >>> X, y = load_digits(return_X_y=True)\\n    >>> clf = Perceptron(tol=1e-3, random_state=0)\\n    >>> clf.fit(X, y)\\n    Perceptron()\\n    >>> clf.score(X, y)\\n    0.939...\\n\\n    See also\\n    --------\\n\\n    SGDClassifier\\n\\n    References\\n    ----------\\n\\n    https://en.wikipedia.org/wiki/Perceptron and references therein.\\n    \"]] [:hr] [:hr]] [:div [:h3 \":sklearn.classification/quadratic-discriminant-analysis\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [4 2]:\\n\\n|             :name |  :default |\\n|-------------------|-----------|\\n|           :priors |           |\\n|        :reg-param |     0.000 |\\n| :store-covariance |     false |\\n|              :tol | 0.0001000 |\\n\"]]] [:span [:p/markdown \"Quadratic Discriminant Analysis\\n\\n    A classifier with a quadratic decision boundary, generated\\n    by fitting class conditional densities to the data\\n    and using Bayes' rule.\\n\\n    The model fits a Gaussian density to each class.\\n\\n    *Added in 0.17*\\n       *QuadraticDiscriminantAnalysis*\\n\\n    Read more in the User Guide: `lda_qda`.\\n\\n    Parameters\\n    ----------\\n- `priors`: ndarray of shape (n_classes,), default=None\\n        Class priors. By default, the class proportions are inferred from the\\n        training data.\\n\\n- `reg_param`: float, default=0.0\\n        Regularizes the per-class covariance estimates by transforming S2 as\\n        ``S2 = (1 - reg_param) * S2 + reg_param * np.eye(n_features)``,\\n        where S2 corresponds to the `scaling_` attribute of a given class.\\n\\n- `store_covariance`: bool, default=False\\n        If True, the class covariance matrices are explicitely computed and\\n        stored in the `self.covariance_` attribute.\\n\\n        *Added in 0.17*\\n\\n- `tol`: float, default=1.0e-4\\n        Absolute threshold for a singular value to be considered significant,\\n        used to estimate the rank of `Xk` where `Xk` is the centered matrix\\n        of samples in class k. This parameter does not affect the\\n        predictions. It only controls a warning that is raised when features\\n        are considered to be colinear.\\n\\n        *Added in 0.17*\\n\\n    Attributes\\n    ----------\\n- `covariance_`: list of len n_classes of ndarray             of shape (n_features, n_features)\\n        For each class, gives the covariance matrix estimated using the\\n        samples of that class. The estimations are unbiased. Only present if\\n        `store_covariance` is True.\\n\\n- `means_`: array-like of shape (n_classes, n_features)\\n        Class-wise means.\\n\\n- `priors_`: array-like of shape (n_classes,)\\n        Class priors (sum to 1).\\n\\n- `rotations_`: list of len n_classes of ndarray of shape (n_features, n_k)\\n        For each class k an array of shape (n_features, n_k), where\\n        ``n_k = min(n_features, number of elements in class k)``\\n        It is the rotation of the Gaussian distribution, i.e. its\\n        principal axis. It corresponds to `V`, the matrix of eigenvectors\\n        coming from the SVD of `Xk = U S Vt` where `Xk` is the centered\\n        matrix of samples from class k.\\n\\n- `scalings_`: list of len n_classes of ndarray of shape (n_k,)\\n        For each class, contains the scaling of\\n        the Gaussian distributions along its principal axes, i.e. the\\n        variance in the rotated coordinate system. It corresponds to `S^2 /\\n        (n_samples - 1)`, where `S` is the diagonal matrix of singular values\\n        from the SVD of `Xk`, where `Xk` is the centered matrix of samples\\n        from class k.\\n\\n- `classes_`: ndarray of shape (n_classes,)\\n        Unique class labels.\\n\\n    Examples\\n    --------\\n    >>> from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\\n    >>> import numpy as np\\n    >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\\n    >>> y = np.array([1, 1, 1, 2, 2, 2])\\n    >>> clf = QuadraticDiscriminantAnalysis()\\n    >>> clf.fit(X, y)\\n    QuadraticDiscriminantAnalysis()\\n    >>> print(clf.predict([[-0.8, -1]]))\\n    [1]\\n\\n    See also\\n    --------\\n    sklearn.discriminant_analysis.LinearDiscriminantAnalysis: Linear\\n        Discriminant Analysis\\n    \"]] [:hr] [:hr]] [:div [:h3 \":sklearn.classification/radius-neighbors-classifier\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [9 2]:\\n\\n|          :name |  :default |\\n|----------------|-----------|\\n|       :weights |   uniform |\\n|             :p |         2 |\\n|     :leaf-size |        30 |\\n| :metric-params |           |\\n|        :radius |     1.000 |\\n| :outlier-label |           |\\n|     :algorithm |      auto |\\n|        :n-jobs |           |\\n|        :metric | minkowski |\\n\"]]] [:span [:p/markdown \"Classifier implementing a vote among neighbors within a given radius\\n\\n    Read more in the User Guide: `classification`.\\n\\n    Parameters\\n    ----------\\n- `radius`: float, default=1.0\\n        Range of parameter space to use by default for `radius_neighbors`\\n        queries.\\n\\n- `weights`: {'uniform', 'distance'} or callable, default='uniform'\\n        weight function used in prediction.  Possible values:\\n\\n        - 'uniform' : uniform weights.  All points in each neighborhood\\n          are weighted equally.\\n        - 'distance' : weight points by the inverse of their distance.\\n          in this case, closer neighbors of a query point will have a\\n          greater influence than neighbors which are further away.\\n        - [callable] : a user-defined function which accepts an\\n          array of distances, and returns an array of the same shape\\n          containing the weights.\\n\\n        Uniform weights are used by default.\\n\\n- `algorithm`: {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\\n        Algorithm used to compute the nearest neighbors:\\n\\n        - 'ball_tree' will use `BallTree`\\n        - 'kd_tree' will use `KDTree`\\n        - 'brute' will use a brute-force search.\\n        - 'auto' will attempt to decide the most appropriate algorithm\\n          based on the values passed to `fit` method.\\n\\n        Note: fitting on sparse input will override the setting of\\n        this parameter, using brute force.\\n\\n- `leaf_size`: int, default=30\\n        Leaf size passed to BallTree or KDTree.  This can affect the\\n        speed of the construction and query, as well as the memory\\n        required to store the tree.  The optimal value depends on the\\n        nature of the problem.\\n\\n- `p`: int, default=2\\n        Power parameter for the Minkowski metric. When p = 1, this is\\n        equivalent to using manhattan_distance (l1), and euclidean_distance\\n        (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\\n\\n- `metric`: str or callable, default='minkowski'\\n        the distance metric to use for the tree.  The default metric is\\n        minkowski, and with p=2 is equivalent to the standard Euclidean\\n        metric. See the documentation of `DistanceMetric` for a\\n        list of available metrics.\\n        If metric is \\\"precomputed\\\", X is assumed to be a distance matrix and\\n        must be square during fit. X may be a :term:`sparse graph`,\\n        in which case only \\\"nonzero\\\" elements may be considered neighbors.\\n\\n- `outlier_label`: {manual label, 'most_frequent'}, default=None\\n        label for outlier samples (samples with no neighbors in given radius).\\n\\n        - manual label: str or int label (should be the same type as y)\\n          or list of manual labels if multi-output is used.\\n        - 'most_frequent' : assign the most frequent label of y to outliers.\\n        - None : when any outlier is detected, ValueError will be raised.\\n\\n- `metric_params`: dict, default=None\\n        Additional keyword arguments for the metric function.\\n\\n- `n_jobs`: int, default=None\\n        The number of parallel jobs to run for neighbors search.\\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\\n        for more details.\\n\\n    Attributes\\n    ----------\\n- `classes_`: ndarray of shape (n_classes,)\\n        Class labels known to the classifier.\\n\\n- `effective_metric_`: str or callble\\n        The distance metric used. It will be same as the `metric` parameter\\n        or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\\n        'minkowski' and `p` parameter set to 2.\\n\\n- `effective_metric_params_`: dict\\n        Additional keyword arguments for the metric function. For most metrics\\n        will be same with `metric_params` parameter, but may also contain the\\n        `p` parameter value if the `effective_metric_` attribute is set to\\n        'minkowski'.\\n\\n- `outputs_2d_`: bool\\n        False when `y`'s shape is (n_samples, ) or (n_samples, 1) during fit\\n        otherwise True.\\n\\n    Examples\\n    --------\\n    >>> X = [[0], [1], [2], [3]]\\n    >>> y = [0, 0, 1, 1]\\n    >>> from sklearn.neighbors import RadiusNeighborsClassifier\\n    >>> neigh = RadiusNeighborsClassifier(radius=1.0)\\n    >>> neigh.fit(X, y)\\n    RadiusNeighborsClassifier(...)\\n    >>> print(neigh.predict([[1.5]]))\\n    [0]\\n    >>> print(neigh.predict_proba([[1.0]]))\\n    [[0.66666667 0.33333333]]\\n\\n    See also\\n    --------\\n    KNeighborsClassifier\\n    RadiusNeighborsRegressor\\n    KNeighborsRegressor\\n    NearestNeighbors\\n\\n    Notes\\n    -----\\n    See Nearest Neighbors: `neighbors` in the online documentation\\n    for a discussion of the choice of ``algorithm`` and ``leaf_size``.\\n\\n    https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm\\n    \"]] [:hr] [:hr]] [:div [:h3 \":sklearn.classification/random-forest-classifier\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [19 2]:\\n\\n|                     :name | :default |\\n|---------------------------|----------|\\n| :min-weight-fraction-leaf |    0.000 |\\n|           :max-leaf-nodes |          |\\n|    :min-impurity-decrease |    0.000 |\\n|        :min-samples-split |    2.000 |\\n|                :bootstrap |     true |\\n|                :ccp-alpha |    0.000 |\\n|                   :n-jobs |          |\\n|             :random-state |          |\\n|                :oob-score |    false |\\n|         :min-samples-leaf |        1 |\\n|             :max-features |     auto |\\n|       :min-impurity-split |          |\\n|               :warm-start |    false |\\n|                :max-depth |          |\\n|             :class-weight |          |\\n|             :n-estimators |      100 |\\n|              :max-samples |          |\\n|                :criterion |     gini |\\n|                  :verbose |        0 |\\n\"]]] [:span [:p/markdown \"\\n    A random forest classifier.\\n\\n    A random forest is a meta estimator that fits a number of decision tree\\n    classifiers on various sub-samples of the dataset and uses averaging to\\n    improve the predictive accuracy and control over-fitting.\\n    The sub-sample size is controlled with the `max_samples` parameter if\\n    `bootstrap=True` (default), otherwise the whole dataset is used to build\\n    each tree.\\n\\n    Read more in the User Guide: `forest`.\\n\\n    Parameters\\n    ----------\\n- `n_estimators`: int, default=100\\n        The number of trees in the forest.\\n\\n        *Changed in 0.22*\\n           The default value of ``n_estimators`` changed from 10 to 100\\n           in 0.22.\\n\\n- `criterion`: {\\\"gini\\\", \\\"entropy\\\"}, default=\\\"gini\\\"\\n        The function to measure the quality of a split. Supported criteria are\\n        \\\"gini\\\" for the Gini impurity and \\\"entropy\\\" for the information gain.\\n        Note: this parameter is tree-specific.\\n\\n- `max_depth`: int, default=None\\n        The maximum depth of the tree. If None, then nodes are expanded until\\n        all leaves are pure or until all leaves contain less than\\n        min_samples_split samples.\\n\\n- `min_samples_split`: int or float, default=2\\n        The minimum number of samples required to split an internal node:\\n\\n        - If int, then consider `min_samples_split` as the minimum number.\\n        - If float, then `min_samples_split` is a fraction and\\n          `ceil(min_samples_split * n_samples)` are the minimum\\n          number of samples for each split.\\n\\n        *Changed in 0.18*\\n           Added float values for fractions.\\n\\n- `min_samples_leaf`: int or float, default=1\\n        The minimum number of samples required to be at a leaf node.\\n        A split point at any depth will only be considered if it leaves at\\n        least ``min_samples_leaf`` training samples in each of the left and\\n        right branches.  This may have the effect of smoothing the model,\\n        especially in regression.\\n\\n        - If int, then consider `min_samples_leaf` as the minimum number.\\n        - If float, then `min_samples_leaf` is a fraction and\\n          `ceil(min_samples_leaf * n_samples)` are the minimum\\n          number of samples for each node.\\n\\n        *Changed in 0.18*\\n           Added float values for fractions.\\n\\n- `min_weight_fraction_leaf`: float, default=0.0\\n        The minimum weighted fraction of the sum total of weights (of all\\n        the input samples) required to be at a leaf node. Samples have\\n        equal weight when sample_weight is not provided.\\n\\n- `max_features`: {\\\"auto\\\", \\\"sqrt\\\", \\\"log2\\\"}, int or float, default=\\\"auto\\\"\\n        The number of features to consider when looking for the best split:\\n\\n        - If int, then consider `max_features` features at each split.\\n        - If float, then `max_features` is a fraction and\\n          `int(max_features * n_features)` features are considered at each\\n          split.\\n        - If \\\"auto\\\", then `max_features=sqrt(n_features)`.\\n        - If \\\"sqrt\\\", then `max_features=sqrt(n_features)` (same as \\\"auto\\\").\\n        - If \\\"log2\\\", then `max_features=log2(n_features)`.\\n        - If None, then `max_features=n_features`.\\n\\n        Note: the search for a split does not stop until at least one\\n        valid partition of the node samples is found, even if it requires to\\n        effectively inspect more than ``max_features`` features.\\n\\n- `max_leaf_nodes`: int, default=None\\n        Grow trees with ``max_leaf_nodes`` in best-first fashion.\\n        Best nodes are defined as relative reduction in impurity.\\n        If None then unlimited number of leaf nodes.\\n\\n- `min_impurity_decrease`: float, default=0.0\\n        A node will be split if this split induces a decrease of the impurity\\n        greater than or equal to this value.\\n\\n        The weighted impurity decrease equation is the following\\n\\n```python\\nN_t / N * (impurity - N_t_R / N_t * right_impurity\\n                    - N_t_L / N_t * left_impurity)\\n\\ne ``N`` is the total number of samples, ``N_t`` is the number of\\nles at the current node, ``N_t_L`` is the number of samples in the\\n child, and ``N_t_R`` is the number of samples in the right child.\\n\\n`, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\\n`sample_weight`` is passed.\\n\\nersionadded:: 0.19\\n\\nrity_split : float, default=None\\nshold for early stopping in tree growth. A node will split\\nts impurity is above the threshold, otherwise it is a leaf.\\n\\neprecated:: 0.19\\n`min_impurity_split`` has been deprecated in favor of\\n`min_impurity_decrease`` in 0.19. The default value of\\n`min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\\nill be removed in 0.25. Use ``min_impurity_decrease`` instead.\\n\\n\\np : bool, default=True\\nher bootstrap samples are used when building trees. If False, the\\ne dataset is used to build each tree.\\n\\ne : bool, default=False\\nher to use out-of-bag samples to estimate\\ngeneralization accuracy.\\n\\n int, default=None\\nnumber of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\\nh:`decision_path` and :meth:`apply` are all parallelized over the\\ns. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\\next. ``-1`` means using all processors. See :term:`Glossary\\nobs>` for more details.\\n\\ntate : int or RandomState, default=None\\nrols both the randomness of the bootstrapping of the samples used\\n building trees (if ``bootstrap=True``) and the sampling of the\\nures to consider when looking for the best split at each node\\n``max_features < n_features``).\\n:term:`Glossary <random_state>` for details.\\n\\n: int, default=0\\nrols the verbosity when fitting and predicting.\\n\\nrt : bool, default=False\\n set to ``True``, reuse the solution of the previous call to fit\\nadd more estimators to the ensemble, otherwise, just fit a whole\\nforest. See :term:`the Glossary <warm_start>`.\\n\\night : {\\\"balanced\\\", \\\"balanced_subsample\\\"}, dict or list of dicts,             default=None\\nhts associated with classes in the form ``{class_label: weight}``.\\not given, all classes are supposed to have weight one. For\\ni-output problems, a list of dicts can be provided in the same\\nr as the columns of y.\\n\\n that for multioutput (including multilabel) weights should be\\nned for each class of every column in its own dict. For example,\\nfour-class multilabel classification weights should be\\n 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\\n1}, {2:5}, {3:1}, {4:1}].\\n\\n\\\"balanced\\\" mode uses the values of y to automatically adjust\\nhts inversely proportional to class frequencies in the input data\\n`n_samples / (n_classes * np.bincount(y))``\\n\\n\\\"balanced_subsample\\\" mode is the same as \\\"balanced\\\" except that\\nhts are computed based on the bootstrap sample for every tree\\nn.\\n\\nmulti-output, the weights of each column of y will be multiplied.\\n\\n that these weights will be multiplied with sample_weight (passed\\nugh the fit method) if sample_weight is specified.\\n\\na : non-negative float, default=0.0\\nlexity parameter used for Minimal Cost-Complexity Pruning. The\\nree with the largest cost complexity that is smaller than\\np_alpha`` will be chosen. By default, no pruning is performed. See\\n:`minimal_cost_complexity_pruning` for details.\\n\\nersionadded:: 0.22\\n\\nles : int or float, default=None\\nootstrap is True, the number of samples to draw from X\\nrain each base estimator.\\n\\n None (default), then draw `X.shape[0]` samples.\\n int, then draw `max_samples` samples.\\n float, then draw `max_samples * X.shape[0]` samples. Thus,\\nax_samples` should be in the interval `(0, 1)`.\\n\\nersionadded:: 0.22\\n\\nes\\n--\\nimator_ : DecisionTreeClassifier\\nchild estimator template used to create the collection of fitted\\nestimators.\\n\\nrs_ : list of DecisionTreeClassifier\\ncollection of fitted sub-estimators.\\n\\n : ndarray of shape (n_classes,) or a list of such arrays\\nclasses labels (single output problem), or a list of arrays of\\ns labels (multi-output problem).\\n\\ns_ : int or list\\nnumber of classes (single output problem), or a list containing the\\ner of classes for each output (multi-output problem).\\n\\nes_ : int\\nnumber of features when ``fit`` is performed.\\n\\ns_ : int\\nnumber of outputs when ``fit`` is performed.\\n\\nimportances_ : ndarray of shape (n_features,)\\nimpurity-based feature importances.\\nhigher, the more important the feature.\\nimportance of a feature is computed as the (normalized)\\nl reduction of the criterion brought by that feature.  It is also\\nn as the Gini importance.\\n\\ning: impurity-based feature importances can be misleading for\\n cardinality features (many unique values). See\\nc:`sklearn.inspection.permutation_importance` as an alternative.\\n\\ne_ : float\\ne of the training dataset obtained using an out-of-bag estimate.\\n attribute exists only when ``oob_score`` is True.\\n\\nsion_function_ : ndarray of shape (n_samples, n_classes)\\nsion function computed with out-of-bag estimate on the training\\n If n_estimators is small it might be possible that a data point\\nnever left out during the bootstrap. In this case,\\n_decision_function_` might contain NaN. This attribute exists\\n when ``oob_score`` is True.\\n\\n\\n\\nTreeClassifier, ExtraTreesClassifier\\n\\n\\n\\nult values for the parameters controlling the size of the trees\\nmax_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\\n trees which can potentially be very large on some data sets. To\\nemory consumption, the complexity and size of the trees should be\\ned by setting those parameter values.\\n\\nures are always randomly permuted at each split. Therefore,\\n found split may vary, even with the same training data,\\natures=n_features`` and ``bootstrap=False``, if the improvement\\nriterion is identical for several splits enumerated during the\\nf the best split. To obtain a deterministic behaviour during\\n ``random_state`` has to be fixed.\\n\\nes\\n--\\n. Breiman, \\\"Random Forests\\\", Machine Learning, 45(1), 5-32, 2001.\\n\\n\\n\\n sklearn.ensemble import RandomForestClassifier\\n sklearn.datasets import make_classification\\n = make_classification(n_samples=1000, n_features=4,\\n                       n_informative=2, n_redundant=0,\\n                       random_state=0, shuffle=False)\\n= RandomForestClassifier(max_depth=2, random_state=0)\\nfit(X, y)\\nrestClassifier(...)\\nt(clf.predict([[0, 0, 0, 0]]))\\n\\n```\\n\"]] [:hr] [:hr]] [:div [:h3 \":sklearn.classification/ridge-classifier\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [9 2]:\\n\\n|          :name | :default |\\n|----------------|----------|\\n|     :normalize |    false |\\n|           :tol | 0.001000 |\\n|        :solver |     auto |\\n|      :max-iter |          |\\n|  :random-state |          |\\n|        :copy-x |     true |\\n| :fit-intercept |     true |\\n|         :alpha |    1.000 |\\n|  :class-weight |          |\\n\"]]] [:span [:p/markdown \"Classifier using Ridge regression.\\n\\n    This classifier first converts the target values into ``{-1, 1}`` and\\n    then treats the problem as a regression task (multi-output regression in\\n    the multiclass case).\\n\\n    Read more in the User Guide: `ridge_regression`.\\n\\n    Parameters\\n    ----------\\n- `alpha`: float, default=1.0\\n        Regularization strength; must be a positive float. Regularization\\n        improves the conditioning of the problem and reduces the variance of\\n        the estimates. Larger values specify stronger regularization.\\n        Alpha corresponds to ``1 / (2C)`` in other linear models such as\\n        `~sklearn.linear_model.LogisticRegression` or\\n        `sklearn.svm.LinearSVC`.\\n\\n- `fit_intercept`: bool, default=True\\n        Whether to calculate the intercept for this model. If set to false, no\\n        intercept will be used in calculations (e.g. data is expected to be\\n        already centered).\\n\\n- `normalize`: bool, default=False\\n        This parameter is ignored when ``fit_intercept`` is set to False.\\n        If True, the regressors X will be normalized before regression by\\n        subtracting the mean and dividing by the l2-norm.\\n        If you wish to standardize, please use\\n        `sklearn.preprocessing.StandardScaler` before calling ``fit``\\n        on an estimator with ``normalize=False``.\\n\\n- `copy_X`: bool, default=True\\n        If True, X will be copied; else, it may be overwritten.\\n\\n- `max_iter`: int, default=None\\n        Maximum number of iterations for conjugate gradient solver.\\n        The default value is determined by scipy.sparse.linalg.\\n\\n- `tol`: float, default=1e-3\\n        Precision of the solution.\\n\\n- `class_weight`: dict or 'balanced', default=None\\n        Weights associated with classes in the form ``{class_label: weight}``.\\n        If not given, all classes are supposed to have weight one.\\n\\n        The \\\"balanced\\\" mode uses the values of y to automatically adjust\\n        weights inversely proportional to class frequencies in the input data\\n        as ``n_samples / (n_classes * np.bincount(y))``.\\n\\n- `solver`: {'auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'},         default='auto'\\n        Solver to use in the computational routines:\\n\\n        - 'auto' chooses the solver automatically based on the type of data.\\n\\n        - 'svd' uses a Singular Value Decomposition of X to compute the Ridge\\n          coefficients. More stable for singular matrices than 'cholesky'.\\n\\n        - 'cholesky' uses the standard scipy.linalg.solve function to\\n          obtain a closed-form solution.\\n\\n        - 'sparse_cg' uses the conjugate gradient solver as found in\\n          scipy.sparse.linalg.cg. As an iterative algorithm, this solver is\\n          more appropriate than 'cholesky' for large-scale data\\n          (possibility to set `tol` and `max_iter`).\\n\\n        - 'lsqr' uses the dedicated regularized least-squares routine\\n          scipy.sparse.linalg.lsqr. It is the fastest and uses an iterative\\n          procedure.\\n\\n        - 'sag' uses a Stochastic Average Gradient descent, and 'saga' uses\\n          its unbiased and more flexible version named SAGA. Both methods\\n          use an iterative procedure, and are often faster than other solvers\\n          when both n_samples and n_features are large. Note that 'sag' and\\n          'saga' fast convergence is only guaranteed on features with\\n          approximately the same scale. You can preprocess the data with a\\n          scaler from sklearn.preprocessing.\\n\\n          *Added in 0.17*\\n             Stochastic Average Gradient descent solver.\\n          *Added in 0.19*\\n           SAGA solver.\\n\\n- `random_state`: int, RandomState instance, default=None\\n        Used when ``solver`` == 'sag' or 'saga' to shuffle the data.\\n        See :term:`Glossary <random_state>` for details.\\n\\n    Attributes\\n    ----------\\n- `coef_`: ndarray of shape (1, n_features) or (n_classes, n_features)\\n        Coefficient of the features in the decision function.\\n\\n        ``coef_`` is of shape (1, n_features) when the given problem is binary.\\n\\n- `intercept_`: float or ndarray of shape (n_targets,)\\n        Independent term in decision function. Set to 0.0 if\\n        ``fit_intercept = False``.\\n\\n- `n_iter_`: None or ndarray of shape (n_targets,)\\n        Actual number of iterations for each target. Available only for\\n        sag and lsqr solvers. Other solvers will return None.\\n\\n- `classes_`: ndarray of shape (n_classes,)\\n        The classes labels.\\n\\n    See Also\\n    --------\\n- `Ridge`: Ridge regression.\\n- `RidgeClassifierCV`:  Ridge classifier with built-in cross validation.\\n\\n    Notes\\n    -----\\n    For multi-class classification, n_class classifiers are trained in\\n    a one-versus-all approach. Concretely, this is implemented by taking\\n    advantage of the multi-variate response support in Ridge.\\n\\n    Examples\\n    --------\\n    >>> from sklearn.datasets import load_breast_cancer\\n    >>> from sklearn.linear_model import RidgeClassifier\\n    >>> X, y = load_breast_cancer(return_X_y=True)\\n    >>> clf = RidgeClassifier().fit(X, y)\\n    >>> clf.score(X, y)\\n    0.9595...\\n    \"]] [:hr] [:hr]] [:div [:h3 \":sklearn.classification/ridge-classifier-cv\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [7 2]:\\n\\n|            :name |                    :default |\\n|------------------|-----------------------------|\\n|          :alphas | #tech.v3.tensor<float64>[3] |\\n|                  | [0.1000 1.000 10.00]        |\\n|    :class-weight |                             |\\n|              :cv |                             |\\n|   :fit-intercept |                        true |\\n|       :normalize |                       false |\\n|         :scoring |                             |\\n| :store-cv-values |                       false |\\n\"]]] [:span [:p/markdown \"Ridge classifier with built-in cross-validation.\\n\\n    See glossary entry for :term:`cross-validation estimator`.\\n\\n    By default, it performs Generalized Cross-Validation, which is a form of\\n    efficient Leave-One-Out cross-validation. Currently, only the n_features >\\n    n_samples case is handled efficiently.\\n\\n    Read more in the User Guide: `ridge_regression`.\\n\\n    Parameters\\n    ----------\\n- `alphas`: ndarray of shape (n_alphas,), default=(0.1, 1.0, 10.0)\\n        Array of alpha values to try.\\n        Regularization strength; must be a positive float. Regularization\\n        improves the conditioning of the problem and reduces the variance of\\n        the estimates. Larger values specify stronger regularization.\\n        Alpha corresponds to ``1 / (2C)`` in other linear models such as\\n        `~sklearn.linear_model.LogisticRegression` or\\n        `sklearn.svm.LinearSVC`.\\n\\n- `fit_intercept`: bool, default=True\\n        Whether to calculate the intercept for this model. If set\\n        to false, no intercept will be used in calculations\\n        (i.e. data is expected to be centered).\\n\\n- `normalize`: bool, default=False\\n        This parameter is ignored when ``fit_intercept`` is set to False.\\n        If True, the regressors X will be normalized before regression by\\n        subtracting the mean and dividing by the l2-norm.\\n        If you wish to standardize, please use\\n        `sklearn.preprocessing.StandardScaler` before calling ``fit``\\n        on an estimator with ``normalize=False``.\\n\\n- `scoring`: string, callable, default=None\\n        A string (see model evaluation documentation) or\\n        a scorer callable object / function with signature\\n        ``scorer(estimator, X, y)``.\\n\\n- `cv`: int, cross-validation generator or an iterable, default=None\\n        Determines the cross-validation splitting strategy.\\n        Possible inputs for cv are:\\n\\n        - None, to use the efficient Leave-One-Out cross-validation\\n        - integer, to specify the number of folds.\\n        - :term:`CV splitter`,\\n        - An iterable yielding (train, test) splits as arrays of indices.\\n\\n        Refer User Guide: `cross_validation` for the various\\n        cross-validation strategies that can be used here.\\n\\n- `class_weight`: dict or 'balanced', default=None\\n        Weights associated with classes in the form ``{class_label: weight}``.\\n        If not given, all classes are supposed to have weight one.\\n\\n        The \\\"balanced\\\" mode uses the values of y to automatically adjust\\n        weights inversely proportional to class frequencies in the input data\\n        as ``n_samples / (n_classes * np.bincount(y))``\\n\\n- `store_cv_values`: bool, default=False\\n        Flag indicating if the cross-validation values corresponding to\\n        each alpha should be stored in the ``cv_values_`` attribute (see\\n        below). This flag is only compatible with ``cv=None`` (i.e. using\\n        Generalized Cross-Validation).\\n\\n    Attributes\\n    ----------\\n- `cv_values_`: ndarray of shape (n_samples, n_targets, n_alphas), optional\\n        Cross-validation values for each alpha (if ``store_cv_values=True`` and\\n        ``cv=None``). After ``fit()`` has been called, this attribute will\\n        contain the mean squared errors (by default) or the values of the\\n        ``{loss,score}_func`` function (if provided in the constructor). This\\n        attribute exists only when ``store_cv_values`` is True.\\n\\n- `coef_`: ndarray of shape (1, n_features) or (n_targets, n_features)\\n        Coefficient of the features in the decision function.\\n\\n        ``coef_`` is of shape (1, n_features) when the given problem is binary.\\n\\n- `intercept_`: float or ndarray of shape (n_targets,)\\n        Independent term in decision function. Set to 0.0 if\\n        ``fit_intercept = False``.\\n\\n- `alpha_`: float\\n        Estimated regularization parameter.\\n\\n- `best_score_`: float\\n        Score of base estimator with best alpha.\\n\\n- `classes_`: ndarray of shape (n_classes,)\\n        The classes labels.\\n\\n    Examples\\n    --------\\n    >>> from sklearn.datasets import load_breast_cancer\\n    >>> from sklearn.linear_model import RidgeClassifierCV\\n    >>> X, y = load_breast_cancer(return_X_y=True)\\n    >>> clf = RidgeClassifierCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X, y)\\n    >>> clf.score(X, y)\\n    0.9630...\\n\\n    See also\\n    --------\\n- `Ridge`: Ridge regression\\n- `RidgeClassifier`: Ridge classifier\\n- `RidgeCV`: Ridge regression with built-in cross validation\\n\\n    Notes\\n    -----\\n    For multi-class classification, n_class classifiers are trained in\\n    a one-versus-all approach. Concretely, this is implemented by taking\\n    advantage of the multi-variate response support in Ridge.\\n    \"]] [:hr] [:hr]] [:div [:h3 \":sklearn.classification/sgd-classifier\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [21 2]:\\n\\n|                :name |  :default |\\n|----------------------|-----------|\\n|    :n-iter-no-change |         5 |\\n|       :learning-rate |   optimal |\\n|             :average |     false |\\n|                 :tol |  0.001000 |\\n|      :early-stopping |     false |\\n|               :eta-0 |     0.000 |\\n|             :shuffle |      true |\\n|             :penalty |        l2 |\\n|             :power-t |    0.5000 |\\n|            :max-iter |      1000 |\\n|              :n-jobs |           |\\n|        :random-state |           |\\n|       :fit-intercept |      true |\\n|               :alpha | 0.0001000 |\\n|          :warm-start |     false |\\n|           :l-1-ratio |    0.1500 |\\n| :validation-fraction |    0.1000 |\\n|        :class-weight |           |\\n|                :loss |     hinge |\\n|             :verbose |         0 |\\n|             :epsilon |    0.1000 |\\n\"]]] [:span [:p/markdown \"Linear classifiers (SVM, logistic regression, etc.) with SGD training.\\n\\n    This estimator implements regularized linear models with stochastic\\n    gradient descent (SGD) learning: the gradient of the loss is estimated\\n    each sample at a time and the model is updated along the way with a\\n    decreasing strength schedule (aka learning rate). SGD allows minibatch\\n    (online/out-of-core) learning via the `partial_fit` method.\\n    For best results using the default learning rate schedule, the data should\\n    have zero mean and unit variance.\\n\\n    This implementation works with data represented as dense or sparse arrays\\n    of floating point values for the features. The model it fits can be\\n    controlled with the loss parameter; by default, it fits a linear support\\n    vector machine (SVM).\\n\\n    The regularizer is a penalty added to the loss function that shrinks model\\n    parameters towards the zero vector using either the squared euclidean norm\\n    L2 or the absolute norm L1 or a combination of both (Elastic Net). If the\\n    parameter update crosses the 0.0 value because of the regularizer, the\\n    update is truncated to 0.0 to allow for learning sparse models and achieve\\n    online feature selection.\\n\\n    Read more in the User Guide: `sgd`.\\n\\n    Parameters\\n    ----------\\n- `loss`: str, default='hinge'\\n        The loss function to be used. Defaults to 'hinge', which gives a\\n        linear SVM.\\n\\n        The possible options are 'hinge', 'log', 'modified_huber',\\n        'squared_hinge', 'perceptron', or a regression loss: 'squared_loss',\\n        'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'.\\n\\n        The 'log' loss gives logistic regression, a probabilistic classifier.\\n        'modified_huber' is another smooth loss that brings tolerance to\\n        outliers as well as probability estimates.\\n        'squared_hinge' is like hinge but is quadratically penalized.\\n        'perceptron' is the linear loss used by the perceptron algorithm.\\n        The other losses are designed for regression but can be useful in\\n        classification as well; see\\n        `~sklearn.linear_model.SGDRegressor` for a description.\\n\\n        More details about the losses formulas can be found in the\\n        User Guide: `sgd_mathematical_formulation`.\\n\\n- `penalty`: {'l2', 'l1', 'elasticnet'}, default='l2'\\n        The penalty (aka regularization term) to be used. Defaults to 'l2'\\n        which is the standard regularizer for linear SVM models. 'l1' and\\n        'elasticnet' might bring sparsity to the model (feature selection)\\n        not achievable with 'l2'.\\n\\n- `alpha`: float, default=0.0001\\n        Constant that multiplies the regularization term. The higher the\\n        value, the stronger the regularization.\\n        Also used to compute the learning rate when set to `learning_rate` is\\n        set to 'optimal'.\\n\\n- `l1_ratio`: float, default=0.15\\n        The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1.\\n        l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1.\\n        Only used if `penalty` is 'elasticnet'.\\n\\n- `fit_intercept`: bool, default=True\\n        Whether the intercept should be estimated or not. If False, the\\n        data is assumed to be already centered.\\n\\n- `max_iter`: int, default=1000\\n        The maximum number of passes over the training data (aka epochs).\\n        It only impacts the behavior in the ``fit`` method, and not the\\n        `partial_fit` method.\\n\\n        *Added in 0.19*\\n\\n- `tol`: float, default=1e-3\\n        The stopping criterion. If it is not None, training will stop\\n        when (loss > best_loss - tol) for ``n_iter_no_change`` consecutive\\n        epochs.\\n\\n        *Added in 0.19*\\n\\n- `shuffle`: bool, default=True\\n        Whether or not the training data should be shuffled after each epoch.\\n\\n- `verbose`: int, default=0\\n        The verbosity level.\\n\\n- `epsilon`: float, default=0.1\\n        Epsilon in the epsilon-insensitive loss functions; only if `loss` is\\n        'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'.\\n        For 'huber', determines the threshold at which it becomes less\\n        important to get the prediction exactly right.\\n        For epsilon-insensitive, any differences between the current prediction\\n        and the correct label are ignored if they are less than this threshold.\\n\\n- `n_jobs`: int, default=None\\n        The number of CPUs to use to do the OVA (One Versus All, for\\n        multi-class problems) computation.\\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\\n        for more details.\\n\\n- `random_state`: int, RandomState instance, default=None\\n        Used for shuffling the data, when ``shuffle`` is set to ``True``.\\n        Pass an int for reproducible output across multiple function calls.\\n        See :term:`Glossary <random_state>`.\\n\\n- `learning_rate`: str, default='optimal'\\n        The learning rate schedule:\\n\\n        - 'constant': `eta = eta0`\\n        - 'optimal': `eta = 1.0 / (alpha * (t + t0))`\\n          where t0 is chosen by a heuristic proposed by Leon Bottou.\\n        - 'invscaling': `eta = eta0 / pow(t, power_t)`\\n        - 'adaptive': eta = eta0, as long as the training keeps decreasing.\\n          Each time n_iter_no_change consecutive epochs fail to decrease the\\n          training loss by tol or fail to increase validation score by tol if\\n          early_stopping is True, the current learning rate is divided by 5.\\n\\n            *Added in 0.20*\\n                Added 'adaptive' option\\n\\n- `eta0`: double, default=0.0\\n        The initial learning rate for the 'constant', 'invscaling' or\\n        'adaptive' schedules. The default value is 0.0 as eta0 is not used by\\n        the default schedule 'optimal'.\\n\\n- `power_t`: double, default=0.5\\n        The exponent for inverse scaling learning rate [default 0.5].\\n\\n- `early_stopping`: bool, default=False\\n        Whether to use early stopping to terminate training when validation\\n        score is not improving. If set to True, it will automatically set aside\\n        a stratified fraction of training data as validation and terminate\\n        training when validation score returned by the `score` method is not\\n        improving by at least tol for n_iter_no_change consecutive epochs.\\n\\n        *Added in 0.20*\\n            Added 'early_stopping' option\\n\\n- `validation_fraction`: float, default=0.1\\n        The proportion of training data to set aside as validation set for\\n        early stopping. Must be between 0 and 1.\\n        Only used if `early_stopping` is True.\\n\\n        *Added in 0.20*\\n            Added 'validation_fraction' option\\n\\n- `n_iter_no_change`: int, default=5\\n        Number of iterations with no improvement to wait before early stopping.\\n\\n        *Added in 0.20*\\n            Added 'n_iter_no_change' option\\n\\n- `class_weight`: dict, {class_label: weight} or \\\"balanced\\\", default=None\\n        Preset for the class_weight fit parameter.\\n\\n        Weights associated with classes. If not given, all classes\\n        are supposed to have weight one.\\n\\n        The \\\"balanced\\\" mode uses the values of y to automatically adjust\\n        weights inversely proportional to class frequencies in the input data\\n        as ``n_samples / (n_classes * np.bincount(y))``.\\n\\n- `warm_start`: bool, default=False\\n        When set to True, reuse the solution of the previous call to fit as\\n        initialization, otherwise, just erase the previous solution.\\n        See :term:`the Glossary <warm_start>`.\\n\\n        Repeatedly calling fit or partial_fit when warm_start is True can\\n        result in a different solution than when calling fit a single time\\n        because of the way the data is shuffled.\\n        If a dynamic learning rate is used, the learning rate is adapted\\n        depending on the number of samples already seen. Calling ``fit`` resets\\n        this counter, while ``partial_fit`` will result in increasing the\\n        existing counter.\\n\\n- `average`: bool or int, default=False\\n        When set to True, computes the averaged SGD weights accross all\\n        updates and stores the result in the ``coef_`` attribute. If set to\\n        an int greater than 1, averaging will begin once the total number of\\n        samples seen reaches `average`. So ``average=10`` will begin\\n        averaging after seeing 10 samples.\\n\\n    Attributes\\n    ----------\\n- `coef_`: ndarray of shape (1, n_features) if n_classes == 2 else             (n_classes, n_features)\\n        Weights assigned to the features.\\n\\n- `intercept_`: ndarray of shape (1,) if n_classes == 2 else (n_classes,)\\n        Constants in decision function.\\n\\n- `n_iter_`: int\\n        The actual number of iterations before reaching the stopping criterion.\\n        For multiclass fits, it is the maximum over every binary fit.\\n\\n- `loss_function_`: concrete ``LossFunction``\\n\\n- `classes_`: array of shape (n_classes,)\\n\\n- `t_`: int\\n        Number of weight updates performed during training.\\n        Same as ``(n_iter_ * n_samples)``.\\n\\n    See Also\\n    --------\\n    sklearn.svm.LinearSVC: Linear support vector classification.\\n    LogisticRegression: Logistic regression.\\n    Perceptron: Inherits from SGDClassifier. ``Perceptron()`` is equivalent to\\n        ``SGDClassifier(loss=\\\"perceptron\\\", eta0=1, learning_rate=\\\"constant\\\",\\n        penalty=None)``.\\n\\n    Examples\\n    --------\\n    >>> import numpy as np\\n    >>> from sklearn.linear_model import SGDClassifier\\n    >>> from sklearn.preprocessing import StandardScaler\\n    >>> from sklearn.pipeline import make_pipeline\\n    >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\\n    >>> Y = np.array([1, 1, 2, 2])\\n    >>> # Always scale the input. The most convenient way is to use a pipeline.\\n    >>> clf = make_pipeline(StandardScaler(),\\n    ...                     SGDClassifier(max_iter=1000, tol=1e-3))\\n    >>> clf.fit(X, Y)\\n    Pipeline(steps=[('standardscaler', StandardScaler()),\\n                    ('sgdclassifier', SGDClassifier())])\\n    >>> print(clf.predict([[-0.8, -1]]))\\n    [1]\\n    \"]] [:hr] [:hr]] [:div [:h3 \":sklearn.classification/svc\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [15 2]:\\n\\n|                    :name | :default |\\n|--------------------------|----------|\\n|              :break-ties |    false |\\n|                  :kernel |      rbf |\\n|                   :gamma |    scale |\\n|                  :degree |        3 |\\n| :decision-function-shape |      ovr |\\n|             :probability |    false |\\n|                     :tol | 0.001000 |\\n|               :shrinking |     true |\\n|                       :c |    1.000 |\\n|                :max-iter |       -1 |\\n|            :random-state |          |\\n|                  :coef-0 |    0.000 |\\n|            :class-weight |          |\\n|              :cache-size |      200 |\\n|                 :verbose |    false |\\n\"]]] [:span [:p/markdown \"C-Support Vector Classification.\\n\\n    The implementation is based on libsvm. The fit time scales at least\\n    quadratically with the number of samples and may be impractical\\n    beyond tens of thousands of samples. For large datasets\\n    consider using `sklearn.svm.LinearSVC` or\\n    `sklearn.linear_model.SGDClassifier` instead, possibly after a\\n    `sklearn.kernel_approximation.Nystroem` transformer.\\n\\n    The multiclass support is handled according to a one-vs-one scheme.\\n\\n    For details on the precise mathematical formulation of the provided\\n    kernel functions and how `gamma`, `coef0` and `degree` affect each\\n    other, see the corresponding section in the narrative documentation:\\n    :ref:`svm_kernels`.\\n\\n    Read more in the User Guide: `svm_classification`.\\n\\n    Parameters\\n    ----------\\n- `C`: float, default=1.0\\n        Regularization parameter. The strength of the regularization is\\n        inversely proportional to C. Must be strictly positive. The penalty\\n        is a squared l2 penalty.\\n\\n- `kernel`: {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'}, default='rbf'\\n        Specifies the kernel type to be used in the algorithm.\\n        It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or\\n        a callable.\\n        If none is given, 'rbf' will be used. If a callable is given it is\\n        used to pre-compute the kernel matrix from data matrices; that matrix\\n        should be an array of shape ``(n_samples, n_samples)``.\\n\\n- `degree`: int, default=3\\n        Degree of the polynomial kernel function ('poly').\\n        Ignored by all other kernels.\\n\\n- `gamma`: {'scale', 'auto'} or float, default='scale'\\n        Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.\\n\\n        - if ``gamma='scale'`` (default) is passed then it uses\\n          1 / (n_features * X.var()) as value of gamma,\\n        - if 'auto', uses 1 / n_features.\\n\\n        *Changed in 0.22*\\n           The default value of ``gamma`` changed from 'auto' to 'scale'.\\n\\n- `coef0`: float, default=0.0\\n        Independent term in kernel function.\\n        It is only significant in 'poly' and 'sigmoid'.\\n\\n- `shrinking`: bool, default=True\\n        Whether to use the shrinking heuristic.\\n        See the User Guide: `shrinking_svm`.\\n\\n- `probability`: bool, default=False\\n        Whether to enable probability estimates. This must be enabled prior\\n        to calling `fit`, will slow down that method as it internally uses\\n        5-fold cross-validation, and `predict_proba` may be inconsistent with\\n        `predict`. Read more in the User Guide: `scores_probabilities`.\\n\\n- `tol`: float, default=1e-3\\n        Tolerance for stopping criterion.\\n\\n- `cache_size`: float, default=200\\n        Specify the size of the kernel cache (in MB).\\n\\n- `class_weight`: dict or 'balanced', default=None\\n        Set the parameter C of class i to class_weight[i]*C for\\n        SVC. If not given, all classes are supposed to have\\n        weight one.\\n        The \\\"balanced\\\" mode uses the values of y to automatically adjust\\n        weights inversely proportional to class frequencies in the input data\\n        as ``n_samples / (n_classes * np.bincount(y))``\\n\\n- `verbose`: bool, default=False\\n        Enable verbose output. Note that this setting takes advantage of a\\n        per-process runtime setting in libsvm that, if enabled, may not work\\n        properly in a multithreaded context.\\n\\n- `max_iter`: int, default=-1\\n        Hard limit on iterations within solver, or -1 for no limit.\\n\\n- `decision_function_shape`: {'ovo', 'ovr'}, default='ovr'\\n        Whether to return a one-vs-rest ('ovr') decision function of shape\\n        (n_samples, n_classes) as all other classifiers, or the original\\n        one-vs-one ('ovo') decision function of libsvm which has shape\\n        (n_samples, n_classes * (n_classes - 1) / 2). However, one-vs-one\\n        ('ovo') is always used as multi-class strategy. The parameter is\\n        ignored for binary classification.\\n\\n        *Changed in 0.19*\\n            decision_function_shape is 'ovr' by default.\\n\\n        *Added in 0.17*\\n           *decision_function_shape='ovr'* is recommended.\\n\\n        *Changed in 0.17*\\n           Deprecated *decision_function_shape='ovo' and None*.\\n\\n- `break_ties`: bool, default=False\\n        If true, ``decision_function_shape='ovr'``, and number of classes > 2,\\n        :term:`predict` will break ties according to the confidence values of\\n        :term:`decision_function`; otherwise the first class among the tied\\n        classes is returned. Please note that breaking ties comes at a\\n        relatively high computational cost compared to a simple predict.\\n\\n        *Added in 0.22*\\n\\n- `random_state`: int or RandomState instance, default=None\\n        Controls the pseudo random number generation for shuffling the data for\\n        probability estimates. Ignored when `probability` is False.\\n        Pass an int for reproducible output across multiple function calls.\\n        See :term:`Glossary <random_state>`.\\n\\n    Attributes\\n    ----------\\n- `support_`: ndarray of shape (n_SV,)\\n        Indices of support vectors.\\n\\n- `support_vectors_`: ndarray of shape (n_SV, n_features)\\n        Support vectors.\\n\\n- `n_support_`: ndarray of shape (n_class,), dtype=int32\\n        Number of support vectors for each class.\\n\\n- `dual_coef_`: ndarray of shape (n_class-1, n_SV)\\n        Dual coefficients of the support vector in the decision\\n        function (see :ref:`sgd_mathematical_formulation`), multiplied by\\n        their targets.\\n        For multiclass, coefficient for all 1-vs-1 classifiers.\\n        The layout of the coefficients in the multiclass case is somewhat\\n        non-trivial. See the multi-class section of the User Guide: `svm_multi_class` for details.\\n\\n- `coef_`: ndarray of shape (n_class * (n_class-1) / 2, n_features)\\n        Weights assigned to the features (coefficients in the primal\\n        problem). This is only available in the case of a linear kernel.\\n\\n        `coef_` is a readonly property derived from `dual_coef_` and\\n        `support_vectors_`.\\n\\n- `intercept_`: ndarray of shape (n_class * (n_class-1) / 2,)\\n        Constants in decision function.\\n\\n- `fit_status_`: int\\n        0 if correctly fitted, 1 otherwise (will raise warning)\\n\\n- `classes_`: ndarray of shape (n_classes,)\\n        The classes labels.\\n\\n- `probA_`: ndarray of shape (n_class * (n_class-1) / 2)\\n- `probB_`: ndarray of shape (n_class * (n_class-1) / 2)\\n        If `probability=True`, it corresponds to the parameters learned in\\n        Platt scaling to produce probability estimates from decision values.\\n        If `probability=False`, it's an empty array. Platt scaling uses the\\n        logistic function\\n        ``1 / (1 + exp(decision_value * probA_ + probB_))``\\n        where ``probA_`` and ``probB_`` are learned from the dataset [2]_. For\\n        more information on the multiclass case and training procedure see\\n        section 8 of [1]_.\\n\\n- `class_weight_`: ndarray of shape (n_class,)\\n        Multipliers of parameter C for each class.\\n        Computed based on the ``class_weight`` parameter.\\n\\n- `shape_fit_`: tuple of int of shape (n_dimensions_of_X,)\\n        Array dimensions of training vector ``X``.\\n\\n    Examples\\n    --------\\n    >>> import numpy as np\\n    >>> from sklearn.pipeline import make_pipeline\\n    >>> from sklearn.preprocessing import StandardScaler\\n    >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\\n    >>> y = np.array([1, 1, 2, 2])\\n    >>> from sklearn.svm import SVC\\n    >>> clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\\n    >>> clf.fit(X, y)\\n    Pipeline(steps=[('standardscaler', StandardScaler()),\\n                    ('svc', SVC(gamma='auto'))])\\n\\n    >>> print(clf.predict([[-0.8, -1]]))\\n    [1]\\n\\n    See also\\n    --------\\n    SVR\\n        Support Vector Machine for Regression implemented using libsvm.\\n\\n    LinearSVC\\n        Scalable Linear Support Vector Machine for classification\\n        implemented using liblinear. Check the See also section of\\n        LinearSVC for more comparison element.\\n\\n    References\\n    ----------\\n - [1] [LIBSVM: A Library for Support Vector Machines\\n        ](http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf)\\n\\n - [2] [Platt, John (1999). \\\"Probabilistic outputs for support vector\\n        machines and comparison to regularizedlikelihood methods.\\\"\\n        ](http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.41.1639)\\n    \"]] [:hr] [:hr]])], \"1182\" [:div [:p] nil nil [:p/markdown \"The scicloj.ml plugin [sklearn-clj](https://github.com/scicloj/sklearn-clj)\\n gives easy access to all models from [scikit-learn](https://scikit-learn.org/stable/)\"]], \"1206\" [:div [:p] [:div [:p/code {:code \"^kind/vega\\n;; (surface-plot iris [:sepal_width :sepal_length]\\n;;               {:model-type\\n;;                :sklearn.classification/svc})\\n\\n\\n\\n\\n\\n\\n\\n\\n[\\\"# Models\\\"]\", :bg-class \"bg-light\"}]] nil [:p/vega [\"# Models\"]]], \"1196\" [:div [:p] [:div [:p/code {:code \"(def iris-split\\n  (->\\n   (toydata/iris-ds)\\n   (ds/split->seq  :holdout))\\n  )\", :bg-class \"bg-light\"}]] nil nil], \"1184\" [:div [:p] nil nil [:p/markdown \"After [libpython.clj](https://github.com/clj-python/libpython-clj)\\n has been setup with the python package sklearn installed,\\nthe following lines show how to use any sklearn model in a usual scicloj.ml pipeline:\"]], \"1194\" [:div [:p] nil nil [:p/markdown \"SVM\"]], \"1190\" [:div [:p] [:div [:p/code {:code \"(def pipe\\n  (ml/pipeline\\n   (mm/set-inference-target 2)\\n   (mm/model {:model-type :sklearn.classification/logistic-regression\\n              :max-iter 100\\n              })))\", :bg-class \"bg-light\"}]] nil nil], \"1198\" [:div [:p] [:div [:p/code {:code \"(def pipe-fn\\n  (ml/pipeline\\n   ;; {:metamorph/id :model}\\n   (mm/model {:model-type\\n              :sklearn.classification/svc})\\n   )\\n\\n  )\", :bg-class \"bg-light\"}]] nil nil], \"1208\" [:div [:p] nil nil [:p/markdown \"Below all models are listed with their parameters and the original documentation.\\n\\nThe parameters are given as Clojure keys in kebap-case. As the document texts are imported from python\\nthey refer to the python spelling of the parameter. But the translation between the two should be obvious.\"]], \"1180\" [:div [:p] nil nil [:p/markdown \"# sklearn-clj\"]], \"1200\" [:div [:p] [:div [:p/code {:code \"(def fitted-ctx\\n  (ml/fit-pipe\\n   (:train (first  iris-split))\\n   pipe-fn))\", :bg-class \"bg-light\"}]] nil nil], \"1210\" [:div [:p] nil nil [:p/markdown \"## Sklearn classification\"]], \"1192\" [:div [:p] [:div [:p/code {:code \"(pipe {:metamorph/data ds\\n       :metamorph/mode :fit})\", :bg-class \"bg-light\"}]] nil [:p/code {:code \"{:metamorph/data :_unnamed [3 3]:\\n\\n| 0 | 1 | 2 |\\n|--:|--:|--:|\\n| 0 | 0 | 0 |\\n| 1 | 1 | 1 |\\n| 2 | 2 | 2 |\\n,\\n :metamorph/mode :fit,\\n #uuid \\\"439c6ce0-17ec-427d-8818-c8a545b2b252\\\"\\n {:model-data LogisticRegression(),\\n  :options\\n  {:model-type :sklearn.classification/logistic-regression,\\n   :max-iter 100},\\n  :id #uuid \\\"2076fc59-8735-47b4-89af-40cb56c09323\\\",\\n  :feature-columns [0 1],\\n  :target-columns [2]}}\\n\"}]], \"1216\" [:div [:p] nil nil ([:div [:h3 \":sklearn.regression/ada-boost-regressor\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [5 2]:\\n\\n|           :name | :default |\\n|-----------------|----------|\\n| :base-estimator |          |\\n|  :learning-rate |    1.000 |\\n|           :loss |   linear |\\n|   :n-estimators |       50 |\\n|   :random-state |          |\\n\"]]] [:span [:p/markdown \"An AdaBoost regressor.\\n\\n    An AdaBoost [1] regressor is a meta-estimator that begins by fitting a\\n    regressor on the original dataset and then fits additional copies of the\\n    regressor on the same dataset but where the weights of instances are\\n    adjusted according to the error of the current prediction. As such,\\n    subsequent regressors focus more on difficult cases.\\n\\n    This class implements the algorithm known as AdaBoost.R2 [2].\\n\\n    Read more in the User Guide: `adaboost`.\\n\\n    *Added in 0.14*\\n\\n    Parameters\\n    ----------\\n- `base_estimator`: object, default=None\\n        The base estimator from which the boosted ensemble is built.\\n        If ``None``, then the base estimator is\\n        ``DecisionTreeRegressor(max_depth=3)``.\\n\\n- `n_estimators`: int, default=50\\n        The maximum number of estimators at which boosting is terminated.\\n        In case of perfect fit, the learning procedure is stopped early.\\n\\n- `learning_rate`: float, default=1.\\n        Learning rate shrinks the contribution of each regressor by\\n        ``learning_rate``. There is a trade-off between ``learning_rate`` and\\n        ``n_estimators``.\\n\\n- `loss`: {'linear', 'square', 'exponential'}, default='linear'\\n        The loss function to use when updating the weights after each\\n        boosting iteration.\\n\\n- `random_state`: int or RandomState, default=None\\n        Controls the random seed given at each `base_estimator` at each\\n        boosting iteration.\\n        Thus, it is only used when `base_estimator` exposes a `random_state`.\\n        In addition, it controls the bootstrap of the weights used to train the\\n        `base_estimator` at each boosting iteration.\\n        Pass an int for reproducible output across multiple function calls.\\n        See :term:`Glossary <random_state>`.\\n\\n    Attributes\\n    ----------\\n- `base_estimator_`: estimator\\n        The base estimator from which the ensemble is grown.\\n\\n- `estimators_`: list of classifiers\\n        The collection of fitted sub-estimators.\\n\\n- `estimator_weights_`: ndarray of floats\\n        Weights for each estimator in the boosted ensemble.\\n\\n- `estimator_errors_`: ndarray of floats\\n        Regression error for each estimator in the boosted ensemble.\\n\\n- `feature_importances_`: ndarray of shape (n_features,)\\n        The impurity-based feature importances if supported by the\\n        ``base_estimator`` (when based on decision trees).\\n\\n        Warning: impurity-based feature importances can be misleading for\\n        high cardinality features (many unique values). See\\n        `sklearn.inspection.permutation_importance` as an alternative.\\n\\n    Examples\\n    --------\\n    >>> from sklearn.ensemble import AdaBoostRegressor\\n    >>> from sklearn.datasets import make_regression\\n    >>> X, y = make_regression(n_features=4, n_informative=2,\\n    ...                        random_state=0, shuffle=False)\\n    >>> regr = AdaBoostRegressor(random_state=0, n_estimators=100)\\n    >>> regr.fit(X, y)\\n    AdaBoostRegressor(n_estimators=100, random_state=0)\\n    >>> regr.predict([[0, 0, 0, 0]])\\n    array([4.7972...])\\n    >>> regr.score(X, y)\\n    0.9771...\\n\\n    See also\\n    --------\\n    AdaBoostClassifier, GradientBoostingRegressor,\\n    sklearn.tree.DecisionTreeRegressor\\n\\n    References\\n    ----------\\n - [1] Y. Freund, R. Schapire, \\\"A Decision-Theoretic Generalization of\\n           on-Line Learning and an Application to Boosting\\\", 1995.\\n\\n - [2] H. Drucker, \\\"Improving Regressors using Boosting Techniques\\\", 1997.\\n\\n    \"]] [:hr] [:hr]] [:div [:h3 \":sklearn.regression/ard-regression\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [12 2]:\\n\\n|             :name |  :default |\\n|-------------------|-----------|\\n|        :normalize |     false |\\n|              :tol |  0.001000 |\\n|          :alpha-2 | 1.000E-06 |\\n| :threshold-lambda | 1.000E+04 |\\n|         :lambda-1 | 1.000E-06 |\\n|           :copy-x |      true |\\n|         :lambda-2 | 1.000E-06 |\\n|    :fit-intercept |      true |\\n|          :alpha-1 | 1.000E-06 |\\n|           :n-iter |       300 |\\n|          :verbose |     false |\\n|    :compute-score |     false |\\n\"]]] [:span [:p/markdown \"Bayesian ARD regression.\\n\\n    Fit the weights of a regression model, using an ARD prior. The weights of\\n    the regression model are assumed to be in Gaussian distributions.\\n    Also estimate the parameters lambda (precisions of the distributions of the\\n    weights) and alpha (precision of the distribution of the noise).\\n    The estimation is done by an iterative procedures (Evidence Maximization)\\n\\n    Read more in the User Guide: `bayesian_regression`.\\n\\n    Parameters\\n    ----------\\n- `n_iter`: int, default=300\\n        Maximum number of iterations.\\n\\n- `tol`: float, default=1e-3\\n        Stop the algorithm if w has converged.\\n\\n- `alpha_1`: float, default=1e-6\\n- `Hyper-parameter`: shape parameter for the Gamma distribution prior\\n        over the alpha parameter.\\n\\n- `alpha_2`: float, default=1e-6\\n- `Hyper-parameter`: inverse scale parameter (rate parameter) for the\\n        Gamma distribution prior over the alpha parameter.\\n\\n- `lambda_1`: float, default=1e-6\\n- `Hyper-parameter`: shape parameter for the Gamma distribution prior\\n        over the lambda parameter.\\n\\n- `lambda_2`: float, default=1e-6\\n- `Hyper-parameter`: inverse scale parameter (rate parameter) for the\\n        Gamma distribution prior over the lambda parameter.\\n\\n- `compute_score`: bool, default=False\\n        If True, compute the objective function at each step of the model.\\n\\n- `threshold_lambda`: float, default=10 000\\n        threshold for removing (pruning) weights with high precision from\\n        the computation.\\n\\n- `fit_intercept`: bool, default=True\\n        whether to calculate the intercept for this model. If set\\n        to false, no intercept will be used in calculations\\n        (i.e. data is expected to be centered).\\n\\n- `normalize`: bool, default=False\\n        This parameter is ignored when ``fit_intercept`` is set to False.\\n        If True, the regressors X will be normalized before regression by\\n        subtracting the mean and dividing by the l2-norm.\\n        If you wish to standardize, please use\\n        `sklearn.preprocessing.StandardScaler` before calling ``fit``\\n        on an estimator with ``normalize=False``.\\n\\n- `copy_X`: bool, default=True\\n        If True, X will be copied; else, it may be overwritten.\\n\\n- `verbose`: bool, default=False\\n        Verbose mode when fitting the model.\\n\\n    Attributes\\n    ----------\\n- `coef_`: array-like of shape (n_features,)\\n        Coefficients of the regression model (mean of distribution)\\n\\n- `alpha_`: float\\n       estimated precision of the noise.\\n\\n- `lambda_`: array-like of shape (n_features,)\\n       estimated precisions of the weights.\\n\\n- `sigma_`: array-like of shape (n_features, n_features)\\n        estimated variance-covariance matrix of the weights\\n\\n- `scores_`: float\\n        if computed, value of the objective function (to be maximized)\\n\\n- `intercept_`: float\\n        Independent term in decision function. Set to 0.0 if\\n        ``fit_intercept = False``.\\n\\n    Examples\\n    --------\\n    >>> from sklearn import linear_model\\n    >>> clf = linear_model.ARDRegression()\\n    >>> clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])\\n    ARDRegression()\\n    >>> clf.predict([[1, 1]])\\n    array([1.])\\n\\n    Notes\\n    -----\\n    For an example, see examples/linear_model/plot_ard.py: `sphx_glr_auto_examples_linear_model_plot_ard.py`.\\n\\n    References\\n    ----------\\n    D. J. C. MacKay, Bayesian nonlinear modeling for the prediction\\n    competition, ASHRAE Transactions, 1994.\\n\\n    R. Salakhutdinov, Lecture notes on Statistical Machine Learning,\\n    http://www.utstat.toronto.edu/~rsalakhu/sta4273/notes/Lecture2.pdf#page=15\\n    Their beta is our ``self.alpha_``\\n    Their alpha is our ``self.lambda_``\\n    ARD is a little different than the slide: only dimensions/features for\\n    which ``self.lambda_ < self.threshold_lambda`` are kept and the rest are\\n    discarded.\\n    \"]] [:hr] [:hr]] [:div [:h3 \":sklearn.regression/bagging-regressor\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [11 2]:\\n\\n|               :name | :default |\\n|---------------------|----------|\\n|          :bootstrap |     true |\\n| :bootstrap-features |    false |\\n|             :n-jobs |          |\\n|       :random-state |          |\\n|          :oob-score |    false |\\n|     :base-estimator |          |\\n|       :max-features |    1.000 |\\n|         :warm-start |    false |\\n|       :n-estimators |       10 |\\n|        :max-samples |    1.000 |\\n|            :verbose |        0 |\\n\"]]] [:span [:p/markdown \"A Bagging regressor.\\n\\n    A Bagging regressor is an ensemble meta-estimator that fits base\\n    regressors each on random subsets of the original dataset and then\\n    aggregate their individual predictions (either by voting or by averaging)\\n    to form a final prediction. Such a meta-estimator can typically be used as\\n    a way to reduce the variance of a black-box estimator (e.g., a decision\\n    tree), by introducing randomization into its construction procedure and\\n    then making an ensemble out of it.\\n\\n    This algorithm encompasses several works from the literature. When random\\n    subsets of the dataset are drawn as random subsets of the samples, then\\n    this algorithm is known as Pasting [1]_. If samples are drawn with\\n    replacement, then the method is known as Bagging [2]_. When random subsets\\n    of the dataset are drawn as random subsets of the features, then the method\\n    is known as Random Subspaces [3]_. Finally, when base estimators are built\\n    on subsets of both samples and features, then the method is known as\\n    Random Patches [4]_.\\n\\n    Read more in the User Guide: `bagging`.\\n\\n    *Added in 0.15*\\n\\n    Parameters\\n    ----------\\n- `base_estimator`: object, default=None\\n        The base estimator to fit on random subsets of the dataset.\\n        If None, then the base estimator is a decision tree.\\n\\n- `n_estimators`: int, default=10\\n        The number of base estimators in the ensemble.\\n\\n- `max_samples`: int or float, default=1.0\\n        The number of samples to draw from X to train each base estimator (with\\n        replacement by default, see `bootstrap` for more details).\\n\\n        - If int, then draw `max_samples` samples.\\n        - If float, then draw `max_samples * X.shape[0]` samples.\\n\\n- `max_features`: int or float, default=1.0\\n        The number of features to draw from X to train each base estimator (\\n        without replacement by default, see `bootstrap_features` for more\\n        details).\\n\\n        - If int, then draw `max_features` features.\\n        - If float, then draw `max_features * X.shape[1]` features.\\n\\n- `bootstrap`: bool, default=True\\n        Whether samples are drawn with replacement. If False, sampling\\n        without replacement is performed.\\n\\n- `bootstrap_features`: bool, default=False\\n        Whether features are drawn with replacement.\\n\\n- `oob_score`: bool, default=False\\n        Whether to use out-of-bag samples to estimate\\n        the generalization error.\\n\\n- `warm_start`: bool, default=False\\n        When set to True, reuse the solution of the previous call to fit\\n        and add more estimators to the ensemble, otherwise, just fit\\n        a whole new ensemble. See :term:`the Glossary <warm_start>`.\\n\\n- `n_jobs`: int, default=None\\n        The number of jobs to run in parallel for both `fit` and\\n        `predict`. ``None`` means 1 unless in a\\n        :obj:`joblib.parallel_backend` context. ``-1`` means using all\\n        processors. See :term:`Glossary <n_jobs>` for more details.\\n\\n- `random_state`: int or RandomState, default=None\\n        Controls the random resampling of the original dataset\\n        (sample wise and feature wise).\\n        If the base estimator accepts a `random_state` attribute, a different\\n        seed is generated for each instance in the ensemble.\\n        Pass an int for reproducible output across multiple function calls.\\n        See :term:`Glossary <random_state>`.\\n\\n- `verbose`: int, default=0\\n        Controls the verbosity when fitting and predicting.\\n\\n    Attributes\\n    ----------\\n- `base_estimator_`: estimator\\n        The base estimator from which the ensemble is grown.\\n\\n- `n_features_`: int\\n        The number of features when `fit` is performed.\\n\\n- `estimators_`: list of estimators\\n        The collection of fitted sub-estimators.\\n\\n- `estimators_samples_`: list of arrays\\n        The subset of drawn samples (i.e., the in-bag samples) for each base\\n        estimator. Each subset is defined by an array of the indices selected.\\n\\n- `estimators_features_`: list of arrays\\n        The subset of drawn features for each base estimator.\\n\\n- `oob_score_`: float\\n        Score of the training dataset obtained using an out-of-bag estimate.\\n        This attribute exists only when ``oob_score`` is True.\\n\\n- `oob_prediction_`: ndarray of shape (n_samples,)\\n        Prediction computed with out-of-bag estimate on the training\\n        set. If n_estimators is small it might be possible that a data point\\n        was never left out during the bootstrap. In this case,\\n        `oob_prediction_` might contain NaN. This attribute exists only\\n        when ``oob_score`` is True.\\n\\n    Examples\\n    --------\\n    >>> from sklearn.svm import SVR\\n    >>> from sklearn.ensemble import BaggingRegressor\\n    >>> from sklearn.datasets import make_regression\\n    >>> X, y = make_regression(n_samples=100, n_features=4,\\n    ...                        n_informative=2, n_targets=1,\\n    ...                        random_state=0, shuffle=False)\\n    >>> regr = BaggingRegressor(base_estimator=SVR(),\\n    ...                         n_estimators=10, random_state=0).fit(X, y)\\n    >>> regr.predict([[0, 0, 0, 0]])\\n    array([-2.8720...])\\n\\n    References\\n    ----------\\n\\n - [1] L. Breiman, \\\"Pasting small votes for classification in large\\n           databases and on-line\\\", Machine Learning, 36(1), 85-103, 1999.\\n\\n - [2] L. Breiman, \\\"Bagging predictors\\\", Machine Learning, 24(2), 123-140,\\n           1996.\\n\\n - [3] T. Ho, \\\"The random subspace method for constructing decision\\n           forests\\\", Pattern Analysis and Machine Intelligence, 20(8), 832-844,\\n           1998.\\n\\n - [4] G. Louppe and P. Geurts, \\\"Ensembles on Random Patches\\\", Machine\\n           Learning and Knowledge Discovery in Databases, 346-361, 2012.\\n    \"]] [:hr] [:hr]] [:div [:h3 \":sklearn.regression/bayesian-ridge\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [13 2]:\\n\\n|          :name |  :default |\\n|----------------|-----------|\\n|     :normalize |     false |\\n|           :tol |  0.001000 |\\n|       :alpha-2 | 1.000E-06 |\\n|      :lambda-1 | 1.000E-06 |\\n|        :copy-x |      true |\\n|      :lambda-2 | 1.000E-06 |\\n|    :alpha-init |           |\\n| :fit-intercept |      true |\\n|       :alpha-1 | 1.000E-06 |\\n|   :lambda-init |           |\\n|        :n-iter |       300 |\\n|       :verbose |     false |\\n| :compute-score |     false |\\n\"]]] [:span [:p/markdown \"Bayesian ridge regression.\\n\\n    Fit a Bayesian ridge model. See the Notes section for details on this\\n    implementation and the optimization of the regularization parameters\\n    lambda (precision of the weights) and alpha (precision of the noise).\\n\\n    Read more in the User Guide: `bayesian_regression`.\\n\\n    Parameters\\n    ----------\\n- `n_iter`: int, default=300\\n        Maximum number of iterations. Should be greater than or equal to 1.\\n\\n- `tol`: float, default=1e-3\\n        Stop the algorithm if w has converged.\\n\\n- `alpha_1`: float, default=1e-6\\n- `Hyper-parameter`: shape parameter for the Gamma distribution prior\\n        over the alpha parameter.\\n\\n- `alpha_2`: float, default=1e-6\\n- `Hyper-parameter`: inverse scale parameter (rate parameter) for the\\n        Gamma distribution prior over the alpha parameter.\\n\\n- `lambda_1`: float, default=1e-6\\n- `Hyper-parameter`: shape parameter for the Gamma distribution prior\\n        over the lambda parameter.\\n\\n- `lambda_2`: float, default=1e-6\\n- `Hyper-parameter`: inverse scale parameter (rate parameter) for the\\n        Gamma distribution prior over the lambda parameter.\\n\\n- `alpha_init`: float, default=None\\n        Initial value for alpha (precision of the noise).\\n        If not set, alpha_init is 1/Var(y).\\n\\n            *Added in 0.22*\\n\\n- `lambda_init`: float, default=None\\n        Initial value for lambda (precision of the weights).\\n        If not set, lambda_init is 1.\\n\\n            *Added in 0.22*\\n\\n- `compute_score`: bool, default=False\\n        If True, compute the log marginal likelihood at each iteration of the\\n        optimization.\\n\\n- `fit_intercept`: bool, default=True\\n        Whether to calculate the intercept for this model.\\n        The intercept is not treated as a probabilistic parameter\\n        and thus has no associated variance. If set\\n        to False, no intercept will be used in calculations\\n        (i.e. data is expected to be centered).\\n\\n- `normalize`: bool, default=False\\n        This parameter is ignored when ``fit_intercept`` is set to False.\\n        If True, the regressors X will be normalized before regression by\\n        subtracting the mean and dividing by the l2-norm.\\n        If you wish to standardize, please use\\n        `sklearn.preprocessing.StandardScaler` before calling ``fit``\\n        on an estimator with ``normalize=False``.\\n\\n- `copy_X`: bool, default=True\\n        If True, X will be copied; else, it may be overwritten.\\n\\n- `verbose`: bool, default=False\\n        Verbose mode when fitting the model.\\n\\n\\n    Attributes\\n    ----------\\n- `coef_`: array-like of shape (n_features,)\\n        Coefficients of the regression model (mean of distribution)\\n\\n- `intercept_`: float\\n        Independent term in decision function. Set to 0.0 if\\n        ``fit_intercept = False``.\\n\\n- `alpha_`: float\\n       Estimated precision of the noise.\\n\\n- `lambda_`: float\\n       Estimated precision of the weights.\\n\\n- `sigma_`: array-like of shape (n_features, n_features)\\n        Estimated variance-covariance matrix of the weights\\n\\n- `scores_`: array-like of shape (n_iter_+1,)\\n        If computed_score is True, value of the log marginal likelihood (to be\\n        maximized) at each iteration of the optimization. The array starts\\n        with the value of the log marginal likelihood obtained for the initial\\n        values of alpha and lambda and ends with the value obtained for the\\n        estimated alpha and lambda.\\n\\n- `n_iter_`: int\\n        The actual number of iterations to reach the stopping criterion.\\n\\n    Examples\\n    --------\\n    >>> from sklearn import linear_model\\n    >>> clf = linear_model.BayesianRidge()\\n    >>> clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])\\n    BayesianRidge()\\n    >>> clf.predict([[1, 1]])\\n    array([1.])\\n\\n    Notes\\n    -----\\n    There exist several strategies to perform Bayesian ridge regression. This\\n    implementation is based on the algorithm described in Appendix A of\\n    (Tipping, 2001) where updates of the regularization parameters are done as\\n    suggested in (MacKay, 1992). Note that according to A New\\n    View of Automatic Relevance Determination (Wipf and Nagarajan, 2008) these\\n    update rules do not guarantee that the marginal likelihood is increasing\\n    between two consecutive iterations of the optimization.\\n\\n    References\\n    ----------\\n    D. J. C. MacKay, Bayesian Interpolation, Computation and Neural Systems,\\n    Vol. 4, No. 3, 1992.\\n\\n    M. E. Tipping, Sparse Bayesian Learning and the Relevance Vector Machine,\\n    Journal of Machine Learning Research, Vol. 1, 2001.\\n    \"]] [:hr] [:hr]] [:div [:h3 \":sklearn.regression/cca\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [5 2]:\\n\\n|         :name |  :default |\\n|---------------|-----------|\\n|         :copy |      true |\\n|     :max-iter |       500 |\\n| :n-components |         2 |\\n|        :scale |      true |\\n|          :tol | 1.000E-06 |\\n\"]]] [:span [:p/markdown \"CCA Canonical Correlation Analysis.\\n\\n    CCA inherits from PLS with mode=\\\"B\\\" and deflation_mode=\\\"canonical\\\".\\n\\n    Read more in the User Guide: `cross_decomposition`.\\n\\n    Parameters\\n    ----------\\n- `n_components`: int, (default 2).\\n        number of components to keep.\\n\\n- `scale`: boolean, (default True)\\n        whether to scale the data?\\n\\n- `max_iter`: an integer, (default 500)\\n        the maximum number of iterations of the NIPALS inner loop\\n\\n- `tol`: non-negative real, default 1e-06.\\n        the tolerance used in the iterative algorithm\\n\\n- `copy`: boolean\\n        Whether the deflation be done on a copy. Let the default value\\n        to True unless you don't care about side effects\\n\\n    Attributes\\n    ----------\\n- `x_weights_`: array, [p, n_components]\\n        X block weights vectors.\\n\\n- `y_weights_`: array, [q, n_components]\\n        Y block weights vectors.\\n\\n- `x_loadings_`: array, [p, n_components]\\n        X block loadings vectors.\\n\\n- `y_loadings_`: array, [q, n_components]\\n        Y block loadings vectors.\\n\\n- `x_scores_`: array, [n_samples, n_components]\\n        X scores.\\n\\n- `y_scores_`: array, [n_samples, n_components]\\n        Y scores.\\n\\n- `x_rotations_`: array, [p, n_components]\\n        X block to latents rotations.\\n\\n- `y_rotations_`: array, [q, n_components]\\n        Y block to latents rotations.\\n\\n- `coef_`: array of shape (p, q)\\n        The coefficients of the linear model: ``Y = X coef_ + Err``\\n\\n- `n_iter_`: array-like\\n        Number of iterations of the NIPALS inner loop for each\\n        component.\\n\\n    Notes\\n    -----\\n    For each component k, find the weights u, v that maximizes\\n    max corr(Xk u, Yk v), such that ``|u| = |v| = 1``\\n\\n    Note that it maximizes only the correlations between the scores.\\n\\n    The residual matrix of X (Xk+1) block is obtained by the deflation on the\\n    current X score: x_score.\\n\\n    The residual matrix of Y (Yk+1) block is obtained by deflation on the\\n    current Y score.\\n\\n    Examples\\n    --------\\n    >>> from sklearn.cross_decomposition import CCA\\n    >>> X = [[0., 0., 1.], [1.,0.,0.], [2.,2.,2.], [3.,5.,4.]]\\n    >>> Y = [[0.1, -0.2], [0.9, 1.1], [6.2, 5.9], [11.9, 12.3]]\\n    >>> cca = CCA(n_components=1)\\n    >>> cca.fit(X, Y)\\n    CCA(n_components=1)\\n    >>> X_c, Y_c = cca.transform(X, Y)\\n\\n    References\\n    ----------\\n\\n    Jacob A. Wegelin. A survey of Partial Least Squares (PLS) methods, with\\n    emphasis on the two-block case. Technical Report 371, Department of\\n    Statistics, University of Washington, Seattle, 2000.\\n\\n    In french but still a reference:\\n    Tenenhaus, M. (1998). La regression PLS: theorie et pratique. Paris:\\n    Editions Technic.\\n\\n    See also\\n    --------\\n    PLSCanonical\\n    PLSSVD\\n    \"]] [:hr] [:hr]] [:div [:h3 \":sklearn.regression/decision-tree-regressor\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [13 2]:\\n\\n|                     :name |   :default |\\n|---------------------------|------------|\\n| :min-weight-fraction-leaf |      0.000 |\\n|           :max-leaf-nodes |            |\\n|    :min-impurity-decrease |      0.000 |\\n|        :min-samples-split |      2.000 |\\n|                  :presort | deprecated |\\n|                :ccp-alpha |      0.000 |\\n|                 :splitter |       best |\\n|             :random-state |            |\\n|         :min-samples-leaf |          1 |\\n|             :max-features |            |\\n|       :min-impurity-split |            |\\n|                :max-depth |            |\\n|                :criterion |        mse |\\n\"]]] [:span [:p/markdown \"A decision tree regressor.\\n\\n    Read more in the User Guide: `tree`.\\n\\n    Parameters\\n    ----------\\n- `criterion`: {\\\"mse\\\", \\\"friedman_mse\\\", \\\"mae\\\"}, default=\\\"mse\\\"\\n        The function to measure the quality of a split. Supported criteria\\n        are \\\"mse\\\" for the mean squared error, which is equal to variance\\n        reduction as feature selection criterion and minimizes the L2 loss\\n        using the mean of each terminal node, \\\"friedman_mse\\\", which uses mean\\n        squared error with Friedman's improvement score for potential splits,\\n        and \\\"mae\\\" for the mean absolute error, which minimizes the L1 loss\\n        using the median of each terminal node.\\n\\n        *Added in 0.18*\\n           Mean Absolute Error (MAE) criterion.\\n\\n- `splitter`: {\\\"best\\\", \\\"random\\\"}, default=\\\"best\\\"\\n        The strategy used to choose the split at each node. Supported\\n        strategies are \\\"best\\\" to choose the best split and \\\"random\\\" to choose\\n        the best random split.\\n\\n- `max_depth`: int, default=None\\n        The maximum depth of the tree. If None, then nodes are expanded until\\n        all leaves are pure or until all leaves contain less than\\n        min_samples_split samples.\\n\\n- `min_samples_split`: int or float, default=2\\n        The minimum number of samples required to split an internal node:\\n\\n        - If int, then consider `min_samples_split` as the minimum number.\\n        - If float, then `min_samples_split` is a fraction and\\n          `ceil(min_samples_split * n_samples)` are the minimum\\n          number of samples for each split.\\n\\n        *Changed in 0.18*\\n           Added float values for fractions.\\n\\n- `min_samples_leaf`: int or float, default=1\\n        The minimum number of samples required to be at a leaf node.\\n        A split point at any depth will only be considered if it leaves at\\n        least ``min_samples_leaf`` training samples in each of the left and\\n        right branches.  This may have the effect of smoothing the model,\\n        especially in regression.\\n\\n        - If int, then consider `min_samples_leaf` as the minimum number.\\n        - If float, then `min_samples_leaf` is a fraction and\\n          `ceil(min_samples_leaf * n_samples)` are the minimum\\n          number of samples for each node.\\n\\n        *Changed in 0.18*\\n           Added float values for fractions.\\n\\n- `min_weight_fraction_leaf`: float, default=0.0\\n        The minimum weighted fraction of the sum total of weights (of all\\n        the input samples) required to be at a leaf node. Samples have\\n        equal weight when sample_weight is not provided.\\n\\n- `max_features`: int, float or {\\\"auto\\\", \\\"sqrt\\\", \\\"log2\\\"}, default=None\\n        The number of features to consider when looking for the best split:\\n\\n        - If int, then consider `max_features` features at each split.\\n        - If float, then `max_features` is a fraction and\\n          `int(max_features * n_features)` features are considered at each\\n          split.\\n        - If \\\"auto\\\", then `max_features=n_features`.\\n        - If \\\"sqrt\\\", then `max_features=sqrt(n_features)`.\\n        - If \\\"log2\\\", then `max_features=log2(n_features)`.\\n        - If None, then `max_features=n_features`.\\n\\n        Note: the search for a split does not stop until at least one\\n        valid partition of the node samples is found, even if it requires to\\n        effectively inspect more than ``max_features`` features.\\n\\n- `random_state`: int, RandomState instance, default=None\\n        Controls the randomness of the estimator. The features are always\\n        randomly permuted at each split, even if ``splitter`` is set to\\n        ``\\\"best\\\"``. When ``max_features < n_features``, the algorithm will\\n        select ``max_features`` at random at each split before finding the best\\n        split among them. But the best found split may vary across different\\n        runs, even if ``max_features=n_features``. That is the case, if the\\n        improvement of the criterion is identical for several splits and one\\n        split has to be selected at random. To obtain a deterministic behaviour\\n        during fitting, ``random_state`` has to be fixed to an integer.\\n        See :term:`Glossary <random_state>` for details.\\n\\n- `max_leaf_nodes`: int, default=None\\n        Grow a tree with ``max_leaf_nodes`` in best-first fashion.\\n        Best nodes are defined as relative reduction in impurity.\\n        If None then unlimited number of leaf nodes.\\n\\n- `min_impurity_decrease`: float, default=0.0\\n        A node will be split if this split induces a decrease of the impurity\\n        greater than or equal to this value.\\n\\n        The weighted impurity decrease equation is the following\\n\\n```python\\nN_t / N * (impurity - N_t_R / N_t * right_impurity\\n                    - N_t_L / N_t * left_impurity)\\n\\ne ``N`` is the total number of samples, ``N_t`` is the number of\\nles at the current node, ``N_t_L`` is the number of samples in the\\n child, and ``N_t_R`` is the number of samples in the right child.\\n\\n`, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\\n`sample_weight`` is passed.\\n\\nersionadded:: 0.19\\n\\nrity_split : float, (default=0)\\nshold for early stopping in tree growth. A node will split\\nts impurity is above the threshold, otherwise it is a leaf.\\n\\neprecated:: 0.19\\n`min_impurity_split`` has been deprecated in favor of\\n`min_impurity_decrease`` in 0.19. The default value of\\n`min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\\nill be removed in 0.25. Use ``min_impurity_decrease`` instead.\\n\\n: deprecated, default='deprecated'\\n parameter is deprecated and will be removed in v0.24.\\n\\neprecated:: 0.22\\n\\na : non-negative float, default=0.0\\nlexity parameter used for Minimal Cost-Complexity Pruning. The\\nree with the largest cost complexity that is smaller than\\np_alpha`` will be chosen. By default, no pruning is performed. See\\n:`minimal_cost_complexity_pruning` for details.\\n\\nersionadded:: 0.22\\n\\nes\\n--\\nimportances_ : ndarray of shape (n_features,)\\nfeature importances.\\nhigher, the more important the feature.\\nimportance of a feature is computed as the\\nmalized) total reduction of the criterion brought\\nhat feature. It is also known as the Gini importance [4]_.\\n\\ning: impurity-based feature importances can be misleading for\\n cardinality features (many unique values). See\\nc:`sklearn.inspection.permutation_importance` as an alternative.\\n\\nures_ : int\\ninferred value of max_features.\\n\\nes_ : int\\nnumber of features when ``fit`` is performed.\\n\\ns_ : int\\nnumber of outputs when ``fit`` is performed.\\n\\nTree\\nunderlying Tree object. Please refer to\\nlp(sklearn.tree._tree.Tree)`` for attributes of Tree object and\\n:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py`\\nbasic usage of these attributes.\\n\\n\\n\\nTreeClassifier : A decision tree classifier.\\n\\n\\n\\nult values for the parameters controlling the size of the trees\\nmax_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\\n trees which can potentially be very large on some data sets. To\\nemory consumption, the complexity and size of the trees should be\\ned by setting those parameter values.\\n\\nes\\n--\\n\\nttps://en.wikipedia.org/wiki/Decision_tree_learning\\n\\n. Breiman, J. Friedman, R. Olshen, and C. Stone, \\\"Classification\\nnd Regression Trees\\\", Wadsworth, Belmont, CA, 1984.\\n\\n. Hastie, R. Tibshirani and J. Friedman. \\\"Elements of Statistical\\nearning\\\", Springer, 2009.\\n\\n. Breiman, and A. Cutler, \\\"Random Forests\\\",\\nttps://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm\\n\\n\\n\\n sklearn.datasets import load_diabetes\\n sklearn.model_selection import cross_val_score\\n sklearn.tree import DecisionTreeRegressor\\n = load_diabetes(return_X_y=True)\\nessor = DecisionTreeRegressor(random_state=0)\\ns_val_score(regressor, X, y, cv=10)\\n               # doctest: +SKIP\\n\\n0.39..., -0.46...,  0.02...,  0.06..., -0.50...,\\n.16...,  0.11..., -0.73..., -0.30..., -0.00...])\\n```\\n\"]] [:hr] [:hr]] [:div [:h3 \":sklearn.regression/dummy-regressor\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [3 2]:\\n\\n|     :name | :default |\\n|-----------|----------|\\n| :constant |          |\\n| :quantile |          |\\n| :strategy |     mean |\\n\"]]] [:span [:p/markdown \"\\n    DummyRegressor is a regressor that makes predictions using\\n    simple rules.\\n\\n    This regressor is useful as a simple baseline to compare with other\\n    (real) regressors. Do not use it for real problems.\\n\\n    Read more in the User Guide: `dummy_estimators`.\\n\\n    *Added in 0.13*\\n\\n    Parameters\\n    ----------\\n- `strategy`: str\\n        Strategy to use to generate predictions.\\n\\n        * \\\"mean\\\": always predicts the mean of the training set\\n        * \\\"median\\\": always predicts the median of the training set\\n        * \\\"quantile\\\": always predicts a specified quantile of the training set,\\n          provided with the quantile parameter.\\n        * \\\"constant\\\": always predicts a constant value that is provided by\\n          the user.\\n\\n- `constant`: int or float or array-like of shape (n_outputs,)\\n        The explicit constant as predicted by the \\\"constant\\\" strategy. This\\n        parameter is useful only for the \\\"constant\\\" strategy.\\n\\n- `quantile`: float in [0.0, 1.0]\\n        The quantile to predict using the \\\"quantile\\\" strategy. A quantile of\\n        0.5 corresponds to the median, while 0.0 to the minimum and 1.0 to the\\n        maximum.\\n\\n    Attributes\\n    ----------\\n- `constant_`: array, shape (1, n_outputs)\\n        Mean or median or quantile of the training targets or constant value\\n        given by the user.\\n\\n- `n_outputs_`: int,\\n        Number of outputs.\\n\\n    Examples\\n    --------\\n    >>> import numpy as np\\n    >>> from sklearn.dummy import DummyRegressor\\n    >>> X = np.array([1.0, 2.0, 3.0, 4.0])\\n    >>> y = np.array([2.0, 3.0, 5.0, 10.0])\\n    >>> dummy_regr = DummyRegressor(strategy=\\\"mean\\\")\\n    >>> dummy_regr.fit(X, y)\\n    DummyRegressor()\\n    >>> dummy_regr.predict(X)\\n    array([5., 5., 5., 5.])\\n    >>> dummy_regr.score(X, y)\\n    0.0\\n    \"]] [:hr] [:hr]] [:div [:h3 \":sklearn.regression/elastic-net\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [12 2]:\\n\\n|          :name |  :default |\\n|----------------|-----------|\\n|     :normalize |     false |\\n|      :positive |     false |\\n|           :tol | 0.0001000 |\\n|      :max-iter |      1000 |\\n|  :random-state |           |\\n|        :copy-x |      true |\\n|    :precompute |     false |\\n| :fit-intercept |      true |\\n|         :alpha |     1.000 |\\n|    :warm-start |     false |\\n|     :selection |    cyclic |\\n|     :l-1-ratio |    0.5000 |\\n\"]]] [:span [:p/markdown \"Linear regression with combined L1 and L2 priors as regularizer.\\n\\n    Minimizes the objective function\\n\\n```python\\n1 / (2 * n_samples) * ||y - Xw||^2_2\\n+ alpha * l1_ratio * ||w||_1\\n+ 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\\n\\nre interested in controlling the L1 and L2 penalty\\nly, keep in mind that this is equivalent to::\\n\\na * L1 + b * L2\\n\\n\\n\\nalpha = a + b and l1_ratio = a / (a + b)\\n\\nmeter l1_ratio corresponds to alpha in the glmnet R package while\\nrresponds to the lambda parameter in glmnet. Specifically, l1_ratio\\nhe lasso penalty. Currently, l1_ratio <= 0.01 is not reliable,\\nou supply your own sequence of alpha.\\n\\ne in the :ref:`User Guide <elastic_net>`.\\n\\nrs\\n--\\nfloat, default=1.0\\ntant that multiplies the penalty terms. Defaults to 1.0.\\nthe notes for the exact mathematical meaning of this\\nmeter. ``alpha = 0`` is equivalent to an ordinary least square,\\ned by the :class:`LinearRegression` object. For numerical\\nons, using ``alpha = 0`` with the ``Lasso`` object is not advised.\\nn this, you should use the :class:`LinearRegression` object.\\n\\n : float, default=0.5\\nElasticNet mixing parameter, with ``0 <= l1_ratio <= 1``. For\\n_ratio = 0`` the penalty is an L2 penalty. ``For l1_ratio = 1`` it\\nn L1 penalty.  For ``0 < l1_ratio < 1``, the penalty is a\\nination of L1 and L2.\\n\\nrcept : bool, default=True\\nher the intercept should be estimated or not. If ``False``, the\\n is assumed to be already centered.\\n\\ne : bool, default=False\\n parameter is ignored when ``fit_intercept`` is set to False.\\nrue, the regressors X will be normalized before regression by\\nracting the mean and dividing by the l2-norm.\\nou wish to standardize, please use\\nss:`sklearn.preprocessing.StandardScaler` before calling ``fit``\\nn estimator with ``normalize=False``.\\n\\nte : bool or array-like of shape (n_features, n_features),                 default=False\\nher to use a precomputed Gram matrix to speed up\\nulations. The Gram matrix can also be passed as argument.\\nsparse input this option is always ``True`` to preserve sparsity.\\n\\n : int, default=1000\\nmaximum number of iterations\\n\\n bool, default=True\\n`True``, X will be copied; else, it may be overwritten.\\n\\noat, default=1e-4\\ntolerance for the optimization: if the updates are\\nler than ``tol``, the optimization code checks the\\n gap for optimality and continues until it is smaller\\n ``tol``.\\n\\nrt : bool, default=False\\n set to ``True``, reuse the solution of the previous call to fit as\\nialization, otherwise, just erase the previous solution.\\n:term:`the Glossary <warm_start>`.\\n\\n : bool, default=False\\n set to ``True``, forces the coefficients to be positive.\\n\\ntate : int, RandomState instance, default=None\\nseed of the pseudo random number generator that selects a random\\nure to update. Used when ``selection`` == 'random'.\\n an int for reproducible output across multiple function calls.\\n:term:`Glossary <random_state>`.\\n\\nn : {'cyclic', 'random'}, default='cyclic'\\net to 'random', a random coefficient is updated every iteration\\ner than looping over features sequentially by default. This\\nting to 'random') often leads to significantly faster convergence\\ncially when tol is higher than 1e-4.\\n\\nes\\n--\\nndarray of shape (n_features,) or (n_targets, n_features)\\nmeter vector (w in the cost function formula)\\n\\noef_ : sparse matrix of shape (n_features, 1) or             (n_targets, n_features)\\narse_coef_`` is a readonly property derived from ``coef_``\\n\\nt_ : float or ndarray of shape (n_targets,)\\npendent term in decision function.\\n\\n: list of int\\ner of iterations run by the coordinate descent solver to reach\\nspecified tolerance.\\n\\n\\n\\n sklearn.linear_model import ElasticNet\\n sklearn.datasets import make_regression\\n\\n = make_regression(n_features=2, random_state=0)\\n = ElasticNet(random_state=0)\\n.fit(X, y)\\net(random_state=0)\\nt(regr.coef_)\\n6048 64.55968825]\\nt(regr.intercept_)\\n\\nt(regr.predict([[0, 0]]))\\n.]\\n\\n\\n\\n\\n unnecessary memory duplication the X argument of the fit method\\ne directly passed as a Fortran-contiguous numpy array.\\n\\n\\n\\netCV : Elastic net model with best model selection by\\ns-validation.\\nssor: implements elastic net regression with incremental training.\\nifier: implements logistic regression with elastic net penalty\\nGDClassifier(loss=\\\"log\\\", penalty=\\\"elasticnet\\\")``).\\n```\\n\"]] [:hr] [:hr]] [:div [:h3 \":sklearn.regression/elastic-net-cv\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [16 2]:\\n\\n|          :name |  :default |\\n|----------------|-----------|\\n|     :normalize |     false |\\n|      :positive |     false |\\n|           :tol | 0.0001000 |\\n|      :n-alphas |       100 |\\n|           :eps |  0.001000 |\\n|        :alphas |           |\\n|      :max-iter |      1000 |\\n|        :n-jobs |           |\\n|  :random-state |           |\\n|        :copy-x |      true |\\n|    :precompute |      auto |\\n| :fit-intercept |      true |\\n|            :cv |           |\\n|     :selection |    cyclic |\\n|     :l-1-ratio |    0.5000 |\\n|       :verbose |         0 |\\n\"]]] [:span [:p/markdown \"Elastic Net model with iterative fitting along a regularization path.\\n\\n    See glossary entry for :term:`cross-validation estimator`.\\n\\n    Read more in the User Guide: `elastic_net`.\\n\\n    Parameters\\n    ----------\\n- `l1_ratio`: float or list of float, default=0.5\\n        float between 0 and 1 passed to ElasticNet (scaling between\\n        l1 and l2 penalties). For ``l1_ratio = 0``\\n        the penalty is an L2 penalty. For ``l1_ratio = 1`` it is an L1 penalty.\\n        For ``0 < l1_ratio < 1``, the penalty is a combination of L1 and L2\\n        This parameter can be a list, in which case the different\\n        values are tested by cross-validation and the one giving the best\\n        prediction score is used. Note that a good choice of list of\\n        values for l1_ratio is often to put more values close to 1\\n        (i.e. Lasso) and less close to 0 (i.e. Ridge), as in ``[.1, .5, .7,\\n        .9, .95, .99, 1]``\\n\\n- `eps`: float, default=1e-3\\n        Length of the path. ``eps=1e-3`` means that\\n        ``alpha_min / alpha_max = 1e-3``.\\n\\n- `n_alphas`: int, default=100\\n        Number of alphas along the regularization path, used for each l1_ratio.\\n\\n- `alphas`: ndarray, default=None\\n        List of alphas where to compute the models.\\n        If None alphas are set automatically\\n\\n- `fit_intercept`: bool, default=True\\n        whether to calculate the intercept for this model. If set\\n        to false, no intercept will be used in calculations\\n        (i.e. data is expected to be centered).\\n\\n- `normalize`: bool, default=False\\n        This parameter is ignored when ``fit_intercept`` is set to False.\\n        If True, the regressors X will be normalized before regression by\\n        subtracting the mean and dividing by the l2-norm.\\n        If you wish to standardize, please use\\n        `sklearn.preprocessing.StandardScaler` before calling ``fit``\\n        on an estimator with ``normalize=False``.\\n\\n- `precompute`: 'auto', bool or array-like of shape (n_features, n_features),                 default='auto'\\n        Whether to use a precomputed Gram matrix to speed up\\n        calculations. If set to ``'auto'`` let us decide. The Gram\\n        matrix can also be passed as argument.\\n\\n- `max_iter`: int, default=1000\\n        The maximum number of iterations\\n\\n- `tol`: float, default=1e-4\\n        The tolerance for the optimization: if the updates are\\n        smaller than ``tol``, the optimization code checks the\\n        dual gap for optimality and continues until it is smaller\\n        than ``tol``.\\n\\n- `cv`: int, cross-validation generator or iterable, default=None\\n        Determines the cross-validation splitting strategy.\\n        Possible inputs for cv are:\\n\\n        - None, to use the default 5-fold cross-validation,\\n        - int, to specify the number of folds.\\n        - :term:`CV splitter`,\\n        - An iterable yielding (train, test) splits as arrays of indices.\\n\\n        For int/None inputs, `KFold` is used.\\n\\n        Refer User Guide: `cross_validation` for the various\\n        cross-validation strategies that can be used here.\\n\\n        *Changed in 0.22*\\n            ``cv`` default value if None changed from 3-fold to 5-fold.\\n\\n- `copy_X`: bool, default=True\\n        If ``True``, X will be copied; else, it may be overwritten.\\n\\n- `verbose`: bool or int, default=0\\n        Amount of verbosity.\\n\\n- `n_jobs`: int, default=None\\n        Number of CPUs to use during the cross validation.\\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\\n        for more details.\\n\\n- `positive`: bool, default=False\\n        When set to ``True``, forces the coefficients to be positive.\\n\\n- `random_state`: int, RandomState instance, default=None\\n        The seed of the pseudo random number generator that selects a random\\n        feature to update. Used when ``selection`` == 'random'.\\n        Pass an int for reproducible output across multiple function calls.\\n        See :term:`Glossary <random_state>`.\\n\\n- `selection`: {'cyclic', 'random'}, default='cyclic'\\n        If set to 'random', a random coefficient is updated every iteration\\n        rather than looping over features sequentially by default. This\\n        (setting to 'random') often leads to significantly faster convergence\\n        especially when tol is higher than 1e-4.\\n\\n    Attributes\\n    ----------\\n- `alpha_`: float\\n        The amount of penalization chosen by cross validation\\n\\n- `l1_ratio_`: float\\n        The compromise between l1 and l2 penalization chosen by\\n        cross validation\\n\\n- `coef_`: ndarray of shape (n_features,) or (n_targets, n_features)\\n        Parameter vector (w in the cost function formula),\\n\\n- `intercept_`: float or ndarray of shape (n_targets, n_features)\\n        Independent term in the decision function.\\n\\n- `mse_path_`: ndarray of shape (n_l1_ratio, n_alpha, n_folds)\\n        Mean square error for the test set on each fold, varying l1_ratio and\\n        alpha.\\n\\n- `alphas_`: ndarray of shape (n_alphas,) or (n_l1_ratio, n_alphas)\\n        The grid of alphas used for fitting, for each l1_ratio.\\n\\n- `n_iter_`: int\\n        number of iterations run by the coordinate descent solver to reach\\n        the specified tolerance for the optimal alpha.\\n\\n    Examples\\n    --------\\n    >>> from sklearn.linear_model import ElasticNetCV\\n    >>> from sklearn.datasets import make_regression\\n\\n    >>> X, y = make_regression(n_features=2, random_state=0)\\n    >>> regr = ElasticNetCV(cv=5, random_state=0)\\n    >>> regr.fit(X, y)\\n    ElasticNetCV(cv=5, random_state=0)\\n    >>> print(regr.alpha_)\\n    0.199...\\n    >>> print(regr.intercept_)\\n    0.398...\\n    >>> print(regr.predict([[0, 0]]))\\n    [0.398...]\\n\\n\\n    Notes\\n    -----\\n    For an example, see\\n    examples/linear_model/plot_lasso_model_selection.py: `sphx_glr_auto_examples_linear_model_plot_lasso_model_selection.py`.\\n\\n    To avoid unnecessary memory duplication the X argument of the fit method\\n    should be directly passed as a Fortran-contiguous numpy array.\\n\\n    The parameter l1_ratio corresponds to alpha in the glmnet R package\\n    while alpha corresponds to the lambda parameter in glmnet.\\n    More specifically, the optimization objective is\\n\\n```python\\n1 / (2 * n_samples) * ||y - Xw||^2_2\\n+ alpha * l1_ratio * ||w||_1\\n+ 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\\n\\nou are interested in controlling the L1 and L2 penalty\\nrately, keep in mind that this is equivalent to::\\n\\na * L1 + b * L2\\n\\n:\\n\\nalpha = a + b and l1_ratio = a / (a + b).\\n\\nalso\\n----\\n_path\\nticNet\\n\\n```\\n\"]] [:hr] [:hr]] [:div [:h3 \":sklearn.regression/extra-tree-regressor\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [12 2]:\\n\\n|                     :name | :default |\\n|---------------------------|----------|\\n| :min-weight-fraction-leaf |    0.000 |\\n|           :max-leaf-nodes |          |\\n|    :min-impurity-decrease |    0.000 |\\n|        :min-samples-split |    2.000 |\\n|                :ccp-alpha |    0.000 |\\n|                 :splitter |   random |\\n|             :random-state |          |\\n|         :min-samples-leaf |        1 |\\n|             :max-features |     auto |\\n|       :min-impurity-split |          |\\n|                :max-depth |          |\\n|                :criterion |      mse |\\n\"]]] [:span [:p/markdown \"An extremely randomized tree regressor.\\n\\n    Extra-trees differ from classic decision trees in the way they are built.\\n    When looking for the best split to separate the samples of a node into two\\n    groups, random splits are drawn for each of the `max_features` randomly\\n    selected features and the best split among those is chosen. When\\n    `max_features` is set 1, this amounts to building a totally random\\n    decision tree.\\n\\n    Warning: Extra-trees should only be used within ensemble methods.\\n\\n    Read more in the User Guide: `tree`.\\n\\n    Parameters\\n    ----------\\n- `criterion`: {\\\"mse\\\", \\\"friedman_mse\\\", \\\"mae\\\"}, default=\\\"mse\\\"\\n        The function to measure the quality of a split. Supported criteria\\n        are \\\"mse\\\" for the mean squared error, which is equal to variance\\n        reduction as feature selection criterion, and \\\"mae\\\" for the mean\\n        absolute error.\\n\\n        *Added in 0.18*\\n           Mean Absolute Error (MAE) criterion.\\n\\n- `splitter`: {\\\"random\\\", \\\"best\\\"}, default=\\\"random\\\"\\n        The strategy used to choose the split at each node. Supported\\n        strategies are \\\"best\\\" to choose the best split and \\\"random\\\" to choose\\n        the best random split.\\n\\n- `max_depth`: int, default=None\\n        The maximum depth of the tree. If None, then nodes are expanded until\\n        all leaves are pure or until all leaves contain less than\\n        min_samples_split samples.\\n\\n- `min_samples_split`: int or float, default=2\\n        The minimum number of samples required to split an internal node:\\n\\n        - If int, then consider `min_samples_split` as the minimum number.\\n        - If float, then `min_samples_split` is a fraction and\\n          `ceil(min_samples_split * n_samples)` are the minimum\\n          number of samples for each split.\\n\\n        *Changed in 0.18*\\n           Added float values for fractions.\\n\\n- `min_samples_leaf`: int or float, default=1\\n        The minimum number of samples required to be at a leaf node.\\n        A split point at any depth will only be considered if it leaves at\\n        least ``min_samples_leaf`` training samples in each of the left and\\n        right branches.  This may have the effect of smoothing the model,\\n        especially in regression.\\n\\n        - If int, then consider `min_samples_leaf` as the minimum number.\\n        - If float, then `min_samples_leaf` is a fraction and\\n          `ceil(min_samples_leaf * n_samples)` are the minimum\\n          number of samples for each node.\\n\\n        *Changed in 0.18*\\n           Added float values for fractions.\\n\\n- `min_weight_fraction_leaf`: float, default=0.0\\n        The minimum weighted fraction of the sum total of weights (of all\\n        the input samples) required to be at a leaf node. Samples have\\n        equal weight when sample_weight is not provided.\\n\\n- `max_features`: int, float, {\\\"auto\\\", \\\"sqrt\\\", \\\"log2\\\"} or None, default=\\\"auto\\\"\\n        The number of features to consider when looking for the best split:\\n\\n        - If int, then consider `max_features` features at each split.\\n        - If float, then `max_features` is a fraction and\\n          `int(max_features * n_features)` features are considered at each\\n          split.\\n        - If \\\"auto\\\", then `max_features=n_features`.\\n        - If \\\"sqrt\\\", then `max_features=sqrt(n_features)`.\\n        - If \\\"log2\\\", then `max_features=log2(n_features)`.\\n        - If None, then `max_features=n_features`.\\n\\n        Note: the search for a split does not stop until at least one\\n        valid partition of the node samples is found, even if it requires to\\n        effectively inspect more than ``max_features`` features.\\n\\n- `random_state`: int, RandomState instance, default=None\\n        Used to pick randomly the `max_features` used at each split.\\n        See :term:`Glossary <random_state>` for details.\\n\\n- `min_impurity_decrease`: float, default=0.0\\n        A node will be split if this split induces a decrease of the impurity\\n        greater than or equal to this value.\\n\\n        The weighted impurity decrease equation is the following\\n\\n```python\\nN_t / N * (impurity - N_t_R / N_t * right_impurity\\n                    - N_t_L / N_t * left_impurity)\\n\\ne ``N`` is the total number of samples, ``N_t`` is the number of\\nles at the current node, ``N_t_L`` is the number of samples in the\\n child, and ``N_t_R`` is the number of samples in the right child.\\n\\n`, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\\n`sample_weight`` is passed.\\n\\nersionadded:: 0.19\\n\\nrity_split : float, (default=0)\\nshold for early stopping in tree growth. A node will split\\nts impurity is above the threshold, otherwise it is a leaf.\\n\\neprecated:: 0.19\\n`min_impurity_split`` has been deprecated in favor of\\n`min_impurity_decrease`` in 0.19. The default value of\\n`min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\\nill be removed in 0.25. Use ``min_impurity_decrease`` instead.\\n\\n_nodes : int, default=None\\n a tree with ``max_leaf_nodes`` in best-first fashion.\\n nodes are defined as relative reduction in impurity.\\none then unlimited number of leaf nodes.\\n\\na : non-negative float, default=0.0\\nlexity parameter used for Minimal Cost-Complexity Pruning. The\\nree with the largest cost complexity that is smaller than\\np_alpha`` will be chosen. By default, no pruning is performed. See\\n:`minimal_cost_complexity_pruning` for details.\\n\\nersionadded:: 0.22\\n\\nes\\n--\\nures_ : int\\ninferred value of max_features.\\n\\nes_ : int\\nnumber of features when ``fit`` is performed.\\n\\nimportances_ : ndarray of shape (n_features,)\\nrn impurity-based feature importances (the higher, the more\\nrtant the feature).\\n\\ning: impurity-based feature importances can be misleading for\\n cardinality features (many unique values). See\\nc:`sklearn.inspection.permutation_importance` as an alternative.\\n\\ns_ : int\\nnumber of outputs when ``fit`` is performed.\\n\\nTree\\nunderlying Tree object. Please refer to\\nlp(sklearn.tree._tree.Tree)`` for attributes of Tree object and\\n:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py`\\nbasic usage of these attributes.\\n\\n\\n\\neClassifier : An extremely randomized tree classifier.\\nensemble.ExtraTreesClassifier : An extra-trees classifier.\\nensemble.ExtraTreesRegressor : An extra-trees regressor.\\n\\n\\n\\nult values for the parameters controlling the size of the trees\\nmax_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\\n trees which can potentially be very large on some data sets. To\\nemory consumption, the complexity and size of the trees should be\\ned by setting those parameter values.\\n\\nes\\n--\\n\\n. Geurts, D. Ernst., and L. Wehenkel, \\\"Extremely randomized trees\\\",\\nachine Learning, 63(1), 3-42, 2006.\\n\\n\\n\\n sklearn.datasets import load_diabetes\\n sklearn.model_selection import train_test_split\\n sklearn.ensemble import BaggingRegressor\\n sklearn.tree import ExtraTreeRegressor\\n = load_diabetes(return_X_y=True)\\nain, X_test, y_train, y_test = train_test_split(\\nX, y, random_state=0)\\na_tree = ExtraTreeRegressor(random_state=0)\\n= BaggingRegressor(extra_tree, random_state=0).fit(\\nX_train, y_train)\\nscore(X_test, y_test)\\n\\n```\\n\"]] [:hr] [:hr]] [:div [:h3 \":sklearn.regression/extra-trees-regressor\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [18 2]:\\n\\n|                     :name | :default |\\n|---------------------------|----------|\\n| :min-weight-fraction-leaf |    0.000 |\\n|           :max-leaf-nodes |          |\\n|    :min-impurity-decrease |    0.000 |\\n|        :min-samples-split |    2.000 |\\n|                :bootstrap |    false |\\n|                :ccp-alpha |    0.000 |\\n|                   :n-jobs |          |\\n|             :random-state |          |\\n|                :oob-score |    false |\\n|         :min-samples-leaf |        1 |\\n|             :max-features |     auto |\\n|       :min-impurity-split |          |\\n|               :warm-start |    false |\\n|                :max-depth |          |\\n|             :n-estimators |      100 |\\n|              :max-samples |          |\\n|                :criterion |      mse |\\n|                  :verbose |        0 |\\n\"]]] [:span [:p/markdown \"\\n    An extra-trees regressor.\\n\\n    This class implements a meta estimator that fits a number of\\n    randomized decision trees (a.k.a. extra-trees) on various sub-samples\\n    of the dataset and uses averaging to improve the predictive accuracy\\n    and control over-fitting.\\n\\n    Read more in the User Guide: `forest`.\\n\\n    Parameters\\n    ----------\\n- `n_estimators`: int, default=100\\n        The number of trees in the forest.\\n\\n        *Changed in 0.22*\\n           The default value of ``n_estimators`` changed from 10 to 100\\n           in 0.22.\\n\\n- `criterion`: {\\\"mse\\\", \\\"mae\\\"}, default=\\\"mse\\\"\\n        The function to measure the quality of a split. Supported criteria\\n        are \\\"mse\\\" for the mean squared error, which is equal to variance\\n        reduction as feature selection criterion, and \\\"mae\\\" for the mean\\n        absolute error.\\n\\n        *Added in 0.18*\\n           Mean Absolute Error (MAE) criterion.\\n\\n- `max_depth`: int, default=None\\n        The maximum depth of the tree. If None, then nodes are expanded until\\n        all leaves are pure or until all leaves contain less than\\n        min_samples_split samples.\\n\\n- `min_samples_split`: int or float, default=2\\n        The minimum number of samples required to split an internal node:\\n\\n        - If int, then consider `min_samples_split` as the minimum number.\\n        - If float, then `min_samples_split` is a fraction and\\n          `ceil(min_samples_split * n_samples)` are the minimum\\n          number of samples for each split.\\n\\n        *Changed in 0.18*\\n           Added float values for fractions.\\n\\n- `min_samples_leaf`: int or float, default=1\\n        The minimum number of samples required to be at a leaf node.\\n        A split point at any depth will only be considered if it leaves at\\n        least ``min_samples_leaf`` training samples in each of the left and\\n        right branches.  This may have the effect of smoothing the model,\\n        especially in regression.\\n\\n        - If int, then consider `min_samples_leaf` as the minimum number.\\n        - If float, then `min_samples_leaf` is a fraction and\\n          `ceil(min_samples_leaf * n_samples)` are the minimum\\n          number of samples for each node.\\n\\n        *Changed in 0.18*\\n           Added float values for fractions.\\n\\n- `min_weight_fraction_leaf`: float, default=0.0\\n        The minimum weighted fraction of the sum total of weights (of all\\n        the input samples) required to be at a leaf node. Samples have\\n        equal weight when sample_weight is not provided.\\n\\n- `max_features`: {\\\"auto\\\", \\\"sqrt\\\", \\\"log2\\\"} int or float, default=\\\"auto\\\"\\n        The number of features to consider when looking for the best split:\\n\\n        - If int, then consider `max_features` features at each split.\\n        - If float, then `max_features` is a fraction and\\n          `int(max_features * n_features)` features are considered at each\\n          split.\\n        - If \\\"auto\\\", then `max_features=n_features`.\\n        - If \\\"sqrt\\\", then `max_features=sqrt(n_features)`.\\n        - If \\\"log2\\\", then `max_features=log2(n_features)`.\\n        - If None, then `max_features=n_features`.\\n\\n        Note: the search for a split does not stop until at least one\\n        valid partition of the node samples is found, even if it requires to\\n        effectively inspect more than ``max_features`` features.\\n\\n- `max_leaf_nodes`: int, default=None\\n        Grow trees with ``max_leaf_nodes`` in best-first fashion.\\n        Best nodes are defined as relative reduction in impurity.\\n        If None then unlimited number of leaf nodes.\\n\\n- `min_impurity_decrease`: float, default=0.0\\n        A node will be split if this split induces a decrease of the impurity\\n        greater than or equal to this value.\\n\\n        The weighted impurity decrease equation is the following\\n\\n```python\\nN_t / N * (impurity - N_t_R / N_t * right_impurity\\n                    - N_t_L / N_t * left_impurity)\\n\\ne ``N`` is the total number of samples, ``N_t`` is the number of\\nles at the current node, ``N_t_L`` is the number of samples in the\\n child, and ``N_t_R`` is the number of samples in the right child.\\n\\n`, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\\n`sample_weight`` is passed.\\n\\nersionadded:: 0.19\\n\\nrity_split : float, default=None\\nshold for early stopping in tree growth. A node will split\\nts impurity is above the threshold, otherwise it is a leaf.\\n\\neprecated:: 0.19\\n`min_impurity_split`` has been deprecated in favor of\\n`min_impurity_decrease`` in 0.19. The default value of\\n`min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\\nill be removed in 0.25. Use ``min_impurity_decrease`` instead.\\n\\np : bool, default=False\\nher bootstrap samples are used when building trees. If False, the\\ne dataset is used to build each tree.\\n\\ne : bool, default=False\\nher to use out-of-bag samples to estimate the R^2 on unseen data.\\n\\n int, default=None\\nnumber of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\\nh:`decision_path` and :meth:`apply` are all parallelized over the\\ns. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\\next. ``-1`` means using all processors. See :term:`Glossary\\nobs>` for more details.\\n\\ntate : int or RandomState, default=None\\nrols 3 sources of randomness:\\n\\ne bootstrapping of the samples used when building trees\\nf ``bootstrap=True``)\\ne sampling of the features to consider when looking for the best\\nlit at each node (if ``max_features < n_features``)\\ne draw of the splits for each of the `max_features`\\n\\n:term:`Glossary <random_state>` for details.\\n\\n: int, default=0\\nrols the verbosity when fitting and predicting.\\n\\nrt : bool, default=False\\n set to ``True``, reuse the solution of the previous call to fit\\nadd more estimators to the ensemble, otherwise, just fit a whole\\nforest. See :term:`the Glossary <warm_start>`.\\n\\na : non-negative float, default=0.0\\nlexity parameter used for Minimal Cost-Complexity Pruning. The\\nree with the largest cost complexity that is smaller than\\np_alpha`` will be chosen. By default, no pruning is performed. See\\n:`minimal_cost_complexity_pruning` for details.\\n\\nersionadded:: 0.22\\n\\nles : int or float, default=None\\nootstrap is True, the number of samples to draw from X\\nrain each base estimator.\\n\\n None (default), then draw `X.shape[0]` samples.\\n int, then draw `max_samples` samples.\\n float, then draw `max_samples * X.shape[0]` samples. Thus,\\nax_samples` should be in the interval `(0, 1)`.\\n\\nersionadded:: 0.22\\n\\nes\\n--\\nimator_ : ExtraTreeRegressor\\nchild estimator template used to create the collection of fitted\\nestimators.\\n\\nrs_ : list of DecisionTreeRegressor\\ncollection of fitted sub-estimators.\\n\\nimportances_ : ndarray of shape (n_features,)\\nimpurity-based feature importances.\\nhigher, the more important the feature.\\nimportance of a feature is computed as the (normalized)\\nl reduction of the criterion brought by that feature.  It is also\\nn as the Gini importance.\\n\\ning: impurity-based feature importances can be misleading for\\n cardinality features (many unique values). See\\nc:`sklearn.inspection.permutation_importance` as an alternative.\\n\\nes_ : int\\nnumber of features.\\n\\ns_ : int\\nnumber of outputs.\\n\\ne_ : float\\ne of the training dataset obtained using an out-of-bag estimate.\\n attribute exists only when ``oob_score`` is True.\\n\\niction_ : ndarray of shape (n_samples,)\\niction computed with out-of-bag estimate on the training set.\\n attribute exists only when ``oob_score`` is True.\\n\\n\\n\\ntree.ExtraTreeRegressor: Base estimator for this ensemble.\\nrestRegressor: Ensemble regressor using trees with optimal splits.\\n\\n\\n\\nult values for the parameters controlling the size of the trees\\nmax_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\\n trees which can potentially be very large on some data sets. To\\nemory consumption, the complexity and size of the trees should be\\ned by setting those parameter values.\\n\\nes\\n--\\n. Geurts, D. Ernst., and L. Wehenkel, \\\"Extremely randomized trees\\\",\\nachine Learning, 63(1), 3-42, 2006.\\n\\n\\n\\n sklearn.datasets import load_diabetes\\n sklearn.model_selection import train_test_split\\n sklearn.ensemble import ExtraTreesRegressor\\n = load_diabetes(return_X_y=True)\\nain, X_test, y_train, y_test = train_test_split(\\nX, y, random_state=0)\\n= ExtraTreesRegressor(n_estimators=100, random_state=0).fit(\\n_train, y_train)\\nscore(X_test, y_test)\\n.\\n```\\n\"]] [:hr] [:hr]] [:div [:h3 \":sklearn.regression/gamma-regressor\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [6 2]:\\n\\n|          :name |  :default |\\n|----------------|-----------|\\n|         :alpha |     1.000 |\\n| :fit-intercept |      true |\\n|      :max-iter |       100 |\\n|           :tol | 0.0001000 |\\n|       :verbose |         0 |\\n|    :warm-start |     false |\\n\"]]] [:span [:p/markdown \"Generalized Linear Model with a Gamma distribution.\\n\\n    Read more in the User Guide: `Generalized_linear_regression`.\\n\\n    Parameters\\n    ----------\\n- `alpha`: float, default=1\\n        Constant that multiplies the penalty term and thus determines the\\n        regularization strength. ``alpha = 0`` is equivalent to unpenalized\\n        GLMs. In this case, the design matrix `X` must have full column rank\\n        (no collinearities).\\n\\n- `fit_intercept`: bool, default=True\\n        Specifies if a constant (a.k.a. bias or intercept) should be\\n        added to the linear predictor (X @ coef + intercept).\\n\\n- `max_iter`: int, default=100\\n        The maximal number of iterations for the solver.\\n\\n- `tol`: float, default=1e-4\\n        Stopping criterion. For the lbfgs solver,\\n        the iteration will stop when ``max{|g_j|, j = 1, ..., d} <= tol``\\n        where ``g_j`` is the j-th component of the gradient (derivative) of\\n        the objective function.\\n\\n- `warm_start`: bool, default=False\\n        If set to ``True``, reuse the solution of the previous call to ``fit``\\n        as initialization for ``coef_`` and ``intercept_`` .\\n\\n- `verbose`: int, default=0\\n        For the lbfgs solver set verbose to any positive number for verbosity.\\n\\n    Attributes\\n    ----------\\n- `coef_`: array of shape (n_features,)\\n        Estimated coefficients for the linear predictor (`X * coef_ +\\n        intercept_`) in the GLM.\\n\\n- `intercept_`: float\\n        Intercept (a.k.a. bias) added to linear predictor.\\n\\n- `n_iter_`: int\\n        Actual number of iterations used in the solver.\\n    \"]] [:hr] [:hr]] [:div [:h3 \":sklearn.regression/gaussian-process-regressor\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [7 2]:\\n\\n|                 :name |      :default |\\n|-----------------------|---------------|\\n|                :alpha |     1.000E-10 |\\n|         :copy-x-train |          true |\\n|               :kernel |               |\\n| :n-restarts-optimizer |             0 |\\n|          :normalize-y |         false |\\n|            :optimizer | fmin_l_bfgs_b |\\n|         :random-state |               |\\n\"]]] [:span [:p/markdown \"Gaussian process regression (GPR).\\n\\n    The implementation is based on Algorithm 2.1 of Gaussian Processes\\n    for Machine Learning (GPML) by Rasmussen and Williams.\\n\\n    In addition to standard scikit-learn estimator API,\\n    GaussianProcessRegressor:\\n\\n       * allows prediction without prior fitting (based on the GP prior)\\n       * provides an additional method sample_y(X), which evaluates samples\\n         drawn from the GPR (prior or posterior) at given inputs\\n       * exposes a method log_marginal_likelihood(theta), which can be used\\n         externally for other ways of selecting hyperparameters, e.g., via\\n         Markov chain Monte Carlo.\\n\\n    Read more in the User Guide: `gaussian_process`.\\n\\n    *Added in 0.18*\\n\\n    Parameters\\n    ----------\\n- `kernel`: kernel instance, default=None\\n        The kernel specifying the covariance function of the GP. If None is\\n        passed, the kernel \\\"1.0 * RBF(1.0)\\\" is used as default. Note that\\n        the kernel's hyperparameters are optimized during fitting.\\n\\n- `alpha`: float or array-like of shape (n_samples), default=1e-10\\n        Value added to the diagonal of the kernel matrix during fitting.\\n        Larger values correspond to increased noise level in the observations.\\n        This can also prevent a potential numerical issue during fitting, by\\n        ensuring that the calculated values form a positive definite matrix.\\n        If an array is passed, it must have the same number of entries as the\\n        data used for fitting and is used as datapoint-dependent noise level.\\n        Note that this is equivalent to adding a WhiteKernel with c=alpha.\\n        Allowing to specify the noise level directly as a parameter is mainly\\n        for convenience and for consistency with Ridge.\\n\\n- `optimizer`: \\\"fmin_l_bfgs_b\\\" or callable, default=\\\"fmin_l_bfgs_b\\\"\\n        Can either be one of the internally supported optimizers for optimizing\\n        the kernel's parameters, specified by a string, or an externally\\n        defined optimizer passed as a callable. If a callable is passed, it\\n        must have the signature\\n\\n```python\\ndef optimizer(obj_func, initial_theta, bounds):\\n    # * 'obj_func' is the objective function to be minimized, which\\n    #   takes the hyperparameters theta as parameter and an\\n    #   optional flag eval_gradient, which determines if the\\n    #   gradient is returned additionally to the function value\\n    # * 'initial_theta': the initial value for theta, which can be\\n    #   used by local optimizers\\n    # * 'bounds': the bounds on the values of theta\\n    ....\\n    # Returned are the best found hyperparameters theta and\\n    # the corresponding value of the target function.\\n    return theta_opt, func_min\\n\\ndefault, the 'L-BGFS-B' algorithm from scipy.optimize.minimize\\nsed. If None is passed, the kernel's parameters are kept fixed.\\nlable internal optimizers are::\\n\\n'fmin_l_bfgs_b'\\n\\nts_optimizer : int, default=0\\nnumber of restarts of the optimizer for finding the kernel's\\nmeters which maximize the log-marginal likelihood. The first run\\nhe optimizer is performed from the kernel's initial parameters,\\nremaining ones (if any) from thetas sampled log-uniform randomly\\n the space of allowed theta-values. If greater than 0, all bounds\\n be finite. Note that n_restarts_optimizer == 0 implies that one\\nis performed.\\n\\ne_y : boolean, optional (default: False)\\nher the target values y are normalized, the mean and variance of\\ntarget values are set equal to 0 and 1 respectively. This is\\nmmended for cases where zero-mean, unit-variance priors are used.\\n that, in this implementation, the normalisation is reversed\\nre the GP predictions are reported.\\n\\nersionchanged:: 0.23\\n\\nrain : bool, default=True\\nrue, a persistent copy of the training data is stored in the\\nct. Otherwise, just a reference to the training data is stored,\\nh might cause predictions to change if the data is modified\\nrnally.\\n\\ntate : int or RandomState, default=None\\nrmines random number generation used to initialize the centers.\\n an int for reproducible results across multiple function calls.\\n:term: `Glossary <random_state>`.\\n\\nes\\n--\\n : array-like of shape (n_samples, n_features) or list of object\\nure vectors or other representations of training data (also\\nired for prediction).\\n\\n : array-like of shape (n_samples,) or (n_samples, n_targets)\\net values in training data (also required for prediction)\\n\\n: kernel instance\\nkernel used for prediction. The structure of the kernel is the\\n as the one passed as parameter but with optimized hyperparameters\\n\\nay-like of shape (n_samples, n_samples)\\nr-triangular Cholesky decomposition of the kernel in ``X_train_``\\n\\n array-like of shape (n_samples,)\\n coefficients of training data points in kernel space\\n\\ninal_likelihood_value_ : float\\nlog-marginal-likelihood of ``self.kernel_.theta``\\n\\n\\n\\n sklearn.datasets import make_friedman2\\n sklearn.gaussian_process import GaussianProcessRegressor\\n sklearn.gaussian_process.kernels import DotProduct, WhiteKernel\\n = make_friedman2(n_samples=500, noise=0, random_state=0)\\nel = DotProduct() + WhiteKernel()\\n= GaussianProcessRegressor(kernel=kernel,\\n    random_state=0).fit(X, y)\\nscore(X, y)\\n.\\npredict(X[:2,:], return_std=True)\\n653.0..., 592.1...]), array([316.6..., 316.6...]))\\n\\n```\\n\"]] [:hr] [:hr]] [:div [:h3 \":sklearn.regression/gradient-boosting-regressor\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [23 2]:\\n\\n|                     :name |     :default |\\n|---------------------------|--------------|\\n|         :n-iter-no-change |              |\\n|            :learning-rate |       0.1000 |\\n| :min-weight-fraction-leaf |        0.000 |\\n|           :max-leaf-nodes |              |\\n|    :min-impurity-decrease |        0.000 |\\n|        :min-samples-split |        2.000 |\\n|                      :tol |    0.0001000 |\\n|                  :presort |   deprecated |\\n|                :subsample |        1.000 |\\n|                :ccp-alpha |        0.000 |\\n|             :random-state |              |\\n|         :min-samples-leaf |            1 |\\n|             :max-features |              |\\n|                     :init |              |\\n|       :min-impurity-split |              |\\n|                    :alpha |       0.9000 |\\n|               :warm-start |        false |\\n|                :max-depth |            3 |\\n|      :validation-fraction |       0.1000 |\\n|             :n-estimators |          100 |\\n|                :criterion | friedman_mse |\\n|                     :loss |           ls |\\n|                  :verbose |            0 |\\n\"]]] [:span [:p/markdown \"Gradient Boosting for regression.\\n\\n    GB builds an additive model in a forward stage-wise fashion;\\n    it allows for the optimization of arbitrary differentiable loss functions.\\n    In each stage a regression tree is fit on the negative gradient of the\\n    given loss function.\\n\\n    Read more in the User Guide: `gradient_boosting`.\\n\\n    Parameters\\n    ----------\\n- `loss`: {'ls', 'lad', 'huber', 'quantile'}, default='ls'\\n        loss function to be optimized. 'ls' refers to least squares\\n        regression. 'lad' (least absolute deviation) is a highly robust\\n        loss function solely based on order information of the input\\n        variables. 'huber' is a combination of the two. 'quantile'\\n        allows quantile regression (use `alpha` to specify the quantile).\\n\\n- `learning_rate`: float, default=0.1\\n        learning rate shrinks the contribution of each tree by `learning_rate`.\\n        There is a trade-off between learning_rate and n_estimators.\\n\\n- `n_estimators`: int, default=100\\n        The number of boosting stages to perform. Gradient boosting\\n        is fairly robust to over-fitting so a large number usually\\n        results in better performance.\\n\\n- `subsample`: float, default=1.0\\n        The fraction of samples to be used for fitting the individual base\\n        learners. If smaller than 1.0 this results in Stochastic Gradient\\n        Boosting. `subsample` interacts with the parameter `n_estimators`.\\n        Choosing `subsample < 1.0` leads to a reduction of variance\\n        and an increase in bias.\\n\\n- `criterion`: {'friedman_mse', 'mse', 'mae'}, default='friedman_mse'\\n        The function to measure the quality of a split. Supported criteria\\n        are \\\"friedman_mse\\\" for the mean squared error with improvement\\n        score by Friedman, \\\"mse\\\" for mean squared error, and \\\"mae\\\" for\\n        the mean absolute error. The default value of \\\"friedman_mse\\\" is\\n        generally the best as it can provide a better approximation in\\n        some cases.\\n\\n        *Added in 0.18*\\n\\n- `min_samples_split`: int or float, default=2\\n        The minimum number of samples required to split an internal node:\\n\\n        - If int, then consider `min_samples_split` as the minimum number.\\n        - If float, then `min_samples_split` is a fraction and\\n          `ceil(min_samples_split * n_samples)` are the minimum\\n          number of samples for each split.\\n\\n        *Changed in 0.18*\\n           Added float values for fractions.\\n\\n- `min_samples_leaf`: int or float, default=1\\n        The minimum number of samples required to be at a leaf node.\\n        A split point at any depth will only be considered if it leaves at\\n        least ``min_samples_leaf`` training samples in each of the left and\\n        right branches.  This may have the effect of smoothing the model,\\n        especially in regression.\\n\\n        - If int, then consider `min_samples_leaf` as the minimum number.\\n        - If float, then `min_samples_leaf` is a fraction and\\n          `ceil(min_samples_leaf * n_samples)` are the minimum\\n          number of samples for each node.\\n\\n        *Changed in 0.18*\\n           Added float values for fractions.\\n\\n- `min_weight_fraction_leaf`: float, default=0.0\\n        The minimum weighted fraction of the sum total of weights (of all\\n        the input samples) required to be at a leaf node. Samples have\\n        equal weight when sample_weight is not provided.\\n\\n- `max_depth`: int, default=3\\n        maximum depth of the individual regression estimators. The maximum\\n        depth limits the number of nodes in the tree. Tune this parameter\\n        for best performance; the best value depends on the interaction\\n        of the input variables.\\n\\n- `min_impurity_decrease`: float, default=0.0\\n        A node will be split if this split induces a decrease of the impurity\\n        greater than or equal to this value.\\n\\n        The weighted impurity decrease equation is the following\\n\\n```python\\nN_t / N * (impurity - N_t_R / N_t * right_impurity\\n                    - N_t_L / N_t * left_impurity)\\n\\ne ``N`` is the total number of samples, ``N_t`` is the number of\\nles at the current node, ``N_t_L`` is the number of samples in the\\n child, and ``N_t_R`` is the number of samples in the right child.\\n\\n`, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\\n`sample_weight`` is passed.\\n\\nersionadded:: 0.19\\n\\nrity_split : float, default=None\\nshold for early stopping in tree growth. A node will split\\nts impurity is above the threshold, otherwise it is a leaf.\\n\\neprecated:: 0.19\\n`min_impurity_split`` has been deprecated in favor of\\n`min_impurity_decrease`` in 0.19. The default value of\\n`min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\\nill be removed in 0.25. Use ``min_impurity_decrease`` instead.\\n\\nstimator or 'zero', default=None\\nstimator object that is used to compute the initial predictions.\\nit`` has to provide :term:`fit` and :term:`predict`. If 'zero', the\\nial raw predictions are set to zero. By default a\\nmmyEstimator`` is used, predicting either the average target value\\n loss='ls'), or a quantile for the other losses.\\n\\ntate : int or RandomState, default=None\\nrols the random seed given to each Tree estimator at each\\nting iteration.\\nddition, it controls the random permutation of the features at\\n split (see Notes for more details).\\nlso controls the random spliting of the training data to obtain a\\ndation set if `n_iter_no_change` is not None.\\n an int for reproducible output across multiple function calls.\\n:term:`Glossary <random_state>`.\\n\\nures : {'auto', 'sqrt', 'log2'}, int or float, default=None\\nnumber of features to consider when looking for the best split:\\n\\n int, then consider `max_features` features at each split.\\n float, then `max_features` is a fraction and\\nnt(max_features * n_features)` features are considered at each\\nlit.\\n \\\"auto\\\", then `max_features=n_features`.\\n \\\"sqrt\\\", then `max_features=sqrt(n_features)`.\\n \\\"log2\\\", then `max_features=log2(n_features)`.\\n None, then `max_features=n_features`.\\n\\nsing `max_features < n_features` leads to a reduction of variance\\nan increase in bias.\\n\\n: the search for a split does not stop until at least one\\nd partition of the node samples is found, even if it requires to\\nctively inspect more than ``max_features`` features.\\n\\nfloat, default=0.9\\nalpha-quantile of the huber loss function and the quantile\\n function. Only if ``loss='huber'`` or ``loss='quantile'``.\\n\\n: int, default=0\\nle verbose output. If 1 then it prints progress and performance\\n in a while (the more trees the lower the frequency). If greater\\n 1 then it prints progress and performance for every tree.\\n\\n_nodes : int, default=None\\n trees with ``max_leaf_nodes`` in best-first fashion.\\n nodes are defined as relative reduction in impurity.\\none then unlimited number of leaf nodes.\\n\\nrt : bool, default=False\\n set to ``True``, reuse the solution of the previous call to fit\\nadd more estimators to the ensemble, otherwise, just erase the\\nious solution. See :term:`the Glossary <warm_start>`.\\n\\n: deprecated, default='deprecated'\\n parameter is deprecated and will be removed in v0.24.\\n\\neprecated :: 0.22\\n\\non_fraction : float, default=0.1\\nproportion of training data to set aside as validation set for\\ny stopping. Must be between 0 and 1.\\n used if ``n_iter_no_change`` is set to an integer.\\n\\nersionadded:: 0.20\\n\\no_change : int, default=None\\niter_no_change`` is used to decide if early stopping will be used\\nerminate training when validation score is not improving. By\\nult it is set to None to disable early stopping. If set to a\\ner, it will set aside ``validation_fraction`` size of the training\\n as validation and terminate training when validation score is not\\noving in all of the previous ``n_iter_no_change`` numbers of\\nations.\\n\\nersionadded:: 0.20\\n\\noat, default=1e-4\\nrance for the early stopping. When the loss is not improving\\nt least tol for ``n_iter_no_change`` iterations (if set to a\\ner), the training stops.\\n\\nersionadded:: 0.20\\n\\na : non-negative float, default=0.0\\nlexity parameter used for Minimal Cost-Complexity Pruning. The\\nree with the largest cost complexity that is smaller than\\np_alpha`` will be chosen. By default, no pruning is performed. See\\n:`minimal_cost_complexity_pruning` for details.\\n\\nersionadded:: 0.22\\n\\nes\\n--\\nimportances_ : ndarray of shape (n_features,)\\nimpurity-based feature importances.\\nhigher, the more important the feature.\\nimportance of a feature is computed as the (normalized)\\nl reduction of the criterion brought by that feature.  It is also\\nn as the Gini importance.\\n\\ning: impurity-based feature importances can be misleading for\\n cardinality features (many unique values). See\\nc:`sklearn.inspection.permutation_importance` as an alternative.\\n\\novement_ : ndarray of shape (n_estimators,)\\nimprovement in loss (= deviance) on the out-of-bag samples\\ntive to the previous iteration.\\nb_improvement_[0]`` is the improvement in\\n of the first stage over the ``init`` estimator.\\n available if ``subsample < 1.0``\\n\\nore_ : ndarray of shape (n_estimators,)\\ni-th score ``train_score_[i]`` is the deviance (= loss) of the\\nl at iteration ``i`` on the in-bag sample.\\n`subsample == 1`` this is the deviance on the training data.\\n\\nLossFunction\\nconcrete ``LossFunction`` object.\\n\\nestimator\\nestimator that provides the initial predictions.\\nvia the ``init`` argument or ``loss.init_estimator``.\\n\\nrs_ : ndarray of DecisionTreeRegressor of shape (n_estimators, 1)\\ncollection of fitted sub-estimators.\\n\\nes_ : int\\nnumber of data features.\\n\\nures_ : int\\ninferred value of max_features.\\n\\n\\n\\nures are always randomly permuted at each split. Therefore,\\n found split may vary, even with the same training data and\\natures=n_features``, if the improvement of the criterion is\\nl for several splits enumerated during the search of the best\\no obtain a deterministic behaviour during fitting,\\n_state`` has to be fixed.\\n\\n\\n\\n sklearn.datasets import make_regression\\n sklearn.ensemble import GradientBoostingRegressor\\n sklearn.model_selection import train_test_split\\n = make_regression(random_state=0)\\nain, X_test, y_train, y_test = train_test_split(\\nX, y, random_state=0)\\n= GradientBoostingRegressor(random_state=0)\\nfit(X_train, y_train)\\nBoostingRegressor(random_state=0)\\npredict(X_test[1:2])\\n61...])\\nscore(X_test, y_test)\\n\\n\\n\\n\\nensemble.HistGradientBoostingRegressor,\\ntree.DecisionTreeRegressor, RandomForestRegressor\\n\\nes\\n--\\nman, Greedy Function Approximation: A Gradient Boosting\\n The Annals of Statistics, Vol. 29, No. 5, 2001.\\n\\nman, Stochastic Gradient Boosting, 1999\\n\\ne, R. Tibshirani and J. Friedman.\\n of Statistical Learning Ed. 2, Springer, 2009.\\n```\\n\"]] [:hr] [:hr]] [:div [:h3 \":sklearn.regression/hist-gradient-boosting-regressor\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [17 2]:\\n\\n|                :name |      :default |\\n|----------------------|---------------|\\n|    :n-iter-no-change |         10.00 |\\n|       :learning-rate |        0.1000 |\\n|      :max-leaf-nodes |         31.00 |\\n|             :scoring |          loss |\\n|                 :tol |     1.000E-07 |\\n|      :early-stopping |          auto |\\n|            :max-iter |           100 |\\n|        :random-state |               |\\n|            :max-bins |           255 |\\n|    :min-samples-leaf |            20 |\\n|       :monotonic-cst |               |\\n|          :warm-start |         false |\\n|           :max-depth |               |\\n| :validation-fraction |        0.1000 |\\n|                :loss | least_squares |\\n|             :verbose |             0 |\\n|  :l-2-regularization |         0.000 |\\n\"]]] [:span [:p/markdown \"Histogram-based Gradient Boosting Regression Tree.\\n\\n    This estimator is much faster than\\n    `GradientBoostingRegressor<sklearn.ensemble.GradientBoostingRegressor>`\\n    for big datasets (n_samples >= 10 000).\\n\\n    This estimator has native support for missing values (NaNs). During\\n    training, the tree grower learns at each split point whether samples\\n    with missing values should go to the left or right child, based on the\\n    potential gain. When predicting, samples with missing values are\\n    assigned to the left or right child consequently. If no missing values\\n    were encountered for a given feature during training, then samples with\\n    missing values are mapped to whichever child has the most samples.\\n\\n    This implementation is inspired by\\n    [LightGBM ](https://github.com/Microsoft/LightGBM).\\n\\n\\n---\\n**Note**\\n\\nThis estimator is still **experimental** for now: the predictions\\nand the API might change without any deprecation cycle. To use it,\\nyou need to explicitly import ``enable_hist_gradient_boosting``::\\n\\n  >>> # explicitly require this experimental feature\\n  >>> from sklearn.experimental import enable_hist_gradient_boosting  # noqa\\n  >>> # now you can import normally from ensemble\\n  >>> from sklearn.ensemble import HistGradientBoostingClassifier\\n\\nad more in the :ref:`User Guide <histogram_based_gradient_boosting>`.\\n\\n versionadded:: 0.21\\n\\nrameters\\n--------\\nss : {'least_squares', 'least_absolute_deviation', 'poisson'},             optional (default='least_squares')\\n  The loss function to use in the boosting process. Note that the\\n  \\\"least squares\\\" and \\\"poisson\\\" losses actually implement\\n  \\\"half least squares loss\\\" and \\\"half poisson deviance\\\" to simplify the\\n  computation of the gradient. Furthermore, \\\"poisson\\\" loss internally\\n  uses a log-link and requires ``y >= 0``\\narning_rate : float, optional (default=0.1)\\n  The learning rate, also known as *shrinkage*. This is used as a\\n  multiplicative factor for the leaves values. Use ``1`` for no\\n  shrinkage.\\nx_iter : int, optional (default=100)\\n  The maximum number of iterations of the boosting process, i.e. the\\n  maximum number of trees.\\nx_leaf_nodes : int or None, optional (default=31)\\n  The maximum number of leaves for each tree. Must be strictly greater\\n  than 1. If None, there is no maximum limit.\\nx_depth : int or None, optional (default=None)\\n  The maximum depth of each tree. The depth of a tree is the number of\\n  edges to go from the root to the deepest leaf.\\n  Depth isn't constrained by default.\\nn_samples_leaf : int, optional (default=20)\\n  The minimum number of samples per leaf. For small datasets with less\\n  than a few hundred samples, it is recommended to lower this value\\n  since only very shallow trees would be built.\\n_regularization : float, optional (default=0)\\n  The L2 regularization parameter. Use ``0`` for no regularization\\n  (default).\\nx_bins : int, optional (default=255)\\n  The maximum number of bins to use for non-missing values. Before\\n  training, each feature of the input array `X` is binned into\\n  integer-valued bins, which allows for a much faster training stage.\\n  Features with a small number of unique values may use less than\\n  ``max_bins`` bins. In addition to the ``max_bins`` bins, one more bin\\n  is always reserved for missing values. Must be no larger than 255.\\nnotonic_cst : array-like of int of shape (n_features), default=None\\n  Indicates the monotonic constraint to enforce on each feature. -1, 1\\n  and 0 respectively correspond to a positive constraint, negative\\n  constraint and no constraint. Read more in the :ref:`User Guide\\n  <monotonic_cst_gbdt>`.\\nrm_start : bool, optional (default=False)\\n  When set to ``True``, reuse the solution of the previous call to fit\\n  and add more estimators to the ensemble. For results to be valid, the\\n  estimator should be re-trained on the same data only.\\n  See :term:`the Glossary <warm_start>`.\\nrly_stopping : 'auto' or bool (default='auto')\\n  If 'auto', early stopping is enabled if the sample size is larger than\\n  10000. If True, early stopping is enabled, otherwise early stopping is\\n  disabled.\\noring : str or callable or None, optional (default='loss')\\n  Scoring parameter to use for early stopping. It can be a single\\n  string (see :ref:`scoring_parameter`) or a callable (see\\n  :ref:`scoring`). If None, the estimator's default scorer is used. If\\n  ``scoring='loss'``, early stopping is checked w.r.t the loss value.\\n  Only used if early stopping is performed.\\nlidation_fraction : int or float or None, optional (default=0.1)\\n  Proportion (or absolute size) of training data to set aside as\\n  validation data for early stopping. If None, early stopping is done on\\n  the training data. Only used if early stopping is performed.\\niter_no_change : int, optional (default=10)\\n  Used to determine when to \\\"early stop\\\". The fitting process is\\n  stopped when none of the last ``n_iter_no_change`` scores are better\\n  than the ``n_iter_no_change - 1`` -th-to-last one, up to some\\n  tolerance. Only used if early stopping is performed.\\nl : float or None, optional (default=1e-7)\\n  The absolute tolerance to use when comparing scores during early\\n  stopping. The higher the tolerance, the more likely we are to early\\n  stop: higher tolerance means that it will be harder for subsequent\\n  iterations to be considered an improvement upon the reference score.\\nrbose: int, optional (default=0)\\n  The verbosity level. If not zero, print some information about the\\n  fitting process.\\nndom_state : int, np.random.RandomStateInstance or None,         optional (default=None)\\n  Pseudo-random number generator to control the subsampling in the\\n  binning process, and the train/validation data split if early stopping\\n  is enabled.\\n  Pass an int for reproducible output across multiple function calls.\\n  See :term:`Glossary <random_state>`.\\n\\ntributes\\n--------\\niter_ : int\\n  The number of iterations as selected by early stopping, depending on\\n  the `early_stopping` parameter. Otherwise it corresponds to max_iter.\\ntrees_per_iteration_ : int\\n  The number of tree that are built at each iteration. For regressors,\\n  this is always 1.\\nain_score_ : ndarray, shape (n_iter_+1,)\\n  The scores at each iteration on the training data. The first entry\\n  is the score of the ensemble before the first iteration. Scores are\\n  computed according to the ``scoring`` parameter. If ``scoring`` is\\n  not 'loss', scores are computed on a subset of at most 10 000\\n  samples. Empty if no early stopping.\\nlidation_score_ : ndarray, shape (n_iter_+1,)\\n  The scores at each iteration on the held-out validation data. The\\n  first entry is the score of the ensemble before the first iteration.\\n  Scores are computed according to the ``scoring`` parameter. Empty if\\n  no early stopping or if ``validation_fraction`` is None.\\n\\namples\\n------\\n> # To use this experimental feature, we need to explicitly ask for it:\\n> from sklearn.experimental import enable_hist_gradient_boosting  # noqa\\n> from sklearn.ensemble import HistGradientBoostingRegressor\\n> from sklearn.datasets import load_diabetes\\n> X, y = load_diabetes(return_X_y=True)\\n> est = HistGradientBoostingRegressor().fit(X, y)\\n> est.score(X, y)\\n92...\\n\\n---\\n\"]] [:hr] [:hr]] [:div [:h3 \":sklearn.regression/huber-regressor\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [6 2]:\\n\\n|          :name |  :default |\\n|----------------|-----------|\\n|         :alpha | 0.0001000 |\\n|       :epsilon |     1.350 |\\n| :fit-intercept |      true |\\n|      :max-iter |       100 |\\n|           :tol | 1.000E-05 |\\n|    :warm-start |     false |\\n\"]]] [:span [:p/markdown \"Linear regression model that is robust to outliers.\\n\\n    The Huber Regressor optimizes the squared loss for the samples where\\n    ``|(y - X'w) / sigma| < epsilon`` and the absolute loss for the samples\\n    where ``|(y - X'w) / sigma| > epsilon``, where w and sigma are parameters\\n    to be optimized. The parameter sigma makes sure that if y is scaled up\\n    or down by a certain factor, one does not need to rescale epsilon to\\n    achieve the same robustness. Note that this does not take into account\\n    the fact that the different features of X may be of different scales.\\n\\n    This makes sure that the loss function is not heavily influenced by the\\n    outliers while not completely ignoring their effect.\\n\\n    Read more in the User Guide: `huber_regression`\\n\\n    *Added in 0.18*\\n\\n    Parameters\\n    ----------\\n- `epsilon`: float, greater than 1.0, default 1.35\\n        The parameter epsilon controls the number of samples that should be\\n        classified as outliers. The smaller the epsilon, the more robust it is\\n        to outliers.\\n\\n- `max_iter`: int, default 100\\n        Maximum number of iterations that\\n        ``scipy.optimize.minimize(method=\\\"L-BFGS-B\\\")`` should run for.\\n\\n- `alpha`: float, default 0.0001\\n        Regularization parameter.\\n\\n- `warm_start`: bool, default False\\n        This is useful if the stored attributes of a previously used model\\n        has to be reused. If set to False, then the coefficients will\\n        be rewritten for every call to fit.\\n        See :term:`the Glossary <warm_start>`.\\n\\n- `fit_intercept`: bool, default True\\n        Whether or not to fit the intercept. This can be set to False\\n        if the data is already centered around the origin.\\n\\n- `tol`: float, default 1e-5\\n        The iteration will stop when\\n        ``max{|proj g_i | i = 1, ..., n}`` <= ``tol``\\n        where pg_i is the i-th component of the projected gradient.\\n\\n    Attributes\\n    ----------\\n- `coef_`: array, shape (n_features,)\\n        Features got by optimizing the Huber loss.\\n\\n- `intercept_`: float\\n        Bias.\\n\\n- `scale_`: float\\n        The value by which ``|y - X'w - c|`` is scaled down.\\n\\n- `n_iter_`: int\\n        Number of iterations that\\n        ``scipy.optimize.minimize(method=\\\"L-BFGS-B\\\")`` has run for.\\n\\n        *Changed in 0.20*\\n\\n            In SciPy <= 1.0.0 the number of lbfgs iterations may exceed\\n            ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.\\n\\n- `outliers_`: array, shape (n_samples,)\\n        A boolean mask which is set to True where the samples are identified\\n        as outliers.\\n\\n    Examples\\n    --------\\n    >>> import numpy as np\\n    >>> from sklearn.linear_model import HuberRegressor, LinearRegression\\n    >>> from sklearn.datasets import make_regression\\n    >>> rng = np.random.RandomState(0)\\n    >>> X, y, coef = make_regression(\\n    ...     n_samples=200, n_features=2, noise=4.0, coef=True, random_state=0)\\n    >>> X[:4] = rng.uniform(10, 20, (4, 2))\\n    >>> y[:4] = rng.uniform(10, 20, 4)\\n    >>> huber = HuberRegressor().fit(X, y)\\n    >>> huber.score(X, y)\\n    -7.284...\\n    >>> huber.predict(X[:1,])\\n    array([806.7200...])\\n    >>> linear = LinearRegression().fit(X, y)\\n    >>> print(\\\"True coefficients:\\\", coef)\\n    True coefficients: [20.4923...  34.1698...]\\n    >>> print(\\\"Huber coefficients:\\\", huber.coef_)\\n    Huber coefficients: [17.7906... 31.0106...]\\n    >>> print(\\\"Linear Regression coefficients:\\\", linear.coef_)\\n    Linear Regression coefficients: [-1.9221...  7.0226...]\\n\\n    References\\n    ----------\\n - [1] Peter J. Huber, Elvezio M. Ronchetti, Robust Statistics\\n           Concomitant scale estimates, pg 172\\n - [2] Art B. Owen (2006), A robust hybrid of lasso and ridge regression.\\n           https://statweb.stanford.edu/~owen/reports/hhu.pdf\\n    \"]] [:hr] [:hr]] [:div [:h3 \":sklearn.regression/isotonic-regression\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [4 2]:\\n\\n|          :name | :default |\\n|----------------|----------|\\n|    :increasing |     true |\\n| :out-of-bounds |      nan |\\n|         :y-max |          |\\n|         :y-min |          |\\n\"]]] [:span [:p/markdown \"Isotonic regression model.\\n\\n    Read more in the User Guide: `isotonic`.\\n\\n    *Added in 0.13*\\n\\n    Parameters\\n    ----------\\n- `y_min`: float, default=None\\n        Lower bound on the lowest predicted value (the minimum value may\\n        still be higher). If not set, defaults to -inf.\\n\\n- `y_max`: float, default=None\\n        Upper bound on the highest predicted value (the maximum may still be\\n        lower). If not set, defaults to +inf.\\n\\n- `increasing`: bool or 'auto', default=True\\n        Determines whether the predictions should be constrained to increase\\n        or decrease with `X`. 'auto' will decide based on the Spearman\\n        correlation estimate's sign.\\n\\n- `out_of_bounds`: str, default=\\\"nan\\\"\\n        The ``out_of_bounds`` parameter handles how `X` values outside of the\\n        training domain are handled.  When set to \\\"nan\\\", predictions\\n        will be NaN.  When set to \\\"clip\\\", predictions will be\\n        set to the value corresponding to the nearest train interval endpoint.\\n        When set to \\\"raise\\\" a `ValueError` is raised.\\n\\n\\n    Attributes\\n    ----------\\n- `X_min_`: float\\n        Minimum value of input array `X_` for left bound.\\n\\n- `X_max_`: float\\n        Maximum value of input array `X_` for right bound.\\n\\n- `f_`: function\\n        The stepwise interpolating function that covers the input domain ``X``.\\n\\n- `increasing_`: bool\\n        Inferred value for ``increasing``.\\n\\n    Notes\\n    -----\\n    Ties are broken using the secondary method from Leeuw, 1977.\\n\\n    References\\n    ----------\\n    Isotonic Median Regression: A Linear Programming Approach\\n    Nilotpal Chakravarti\\n    Mathematics of Operations Research\\n    Vol. 14, No. 2 (May, 1989), pp. 303-308\\n\\n    Isotone Optimization in R : Pool-Adjacent-Violators\\n    Algorithm (PAVA) and Active Set Methods\\n    Leeuw, Hornik, Mair\\n    Journal of Statistical Software 2009\\n\\n    Correctness of Kruskal's algorithms for monotone regression with ties\\n    Leeuw, Psychometrica, 1977\\n\\n    Examples\\n    --------\\n    >>> from sklearn.datasets import make_regression\\n    >>> from sklearn.isotonic import IsotonicRegression\\n    >>> X, y = make_regression(n_samples=10, n_features=1, random_state=41)\\n    >>> iso_reg = IsotonicRegression().fit(X.flatten(), y)\\n    >>> iso_reg.predict([.1, .2])\\n    array([1.8628..., 3.7256...])\\n    \"]] [:hr] [:hr]] [:div [:h3 \":sklearn.regression/k-neighbors-regressor\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [8 2]:\\n\\n|          :name |  :default |\\n|----------------|-----------|\\n|     :algorithm |      auto |\\n|     :leaf-size |        30 |\\n|        :metric | minkowski |\\n| :metric-params |           |\\n|        :n-jobs |           |\\n|   :n-neighbors |         5 |\\n|             :p |         2 |\\n|       :weights |   uniform |\\n\"]]] [:span [:p/markdown \"Regression based on k-nearest neighbors.\\n\\n    The target is predicted by local interpolation of the targets\\n    associated of the nearest neighbors in the training set.\\n\\n    Read more in the User Guide: `regression`.\\n\\n    *Added in 0.9*\\n\\n    Parameters\\n    ----------\\n- `n_neighbors`: int, default=5\\n        Number of neighbors to use by default for `kneighbors` queries.\\n\\n- `weights`: {'uniform', 'distance'} or callable, default='uniform'\\n        weight function used in prediction.  Possible values:\\n\\n        - 'uniform' : uniform weights.  All points in each neighborhood\\n          are weighted equally.\\n        - 'distance' : weight points by the inverse of their distance.\\n          in this case, closer neighbors of a query point will have a\\n          greater influence than neighbors which are further away.\\n        - [callable] : a user-defined function which accepts an\\n          array of distances, and returns an array of the same shape\\n          containing the weights.\\n\\n        Uniform weights are used by default.\\n\\n- `algorithm`: {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\\n        Algorithm used to compute the nearest neighbors:\\n\\n        - 'ball_tree' will use `BallTree`\\n        - 'kd_tree' will use `KDTree`\\n        - 'brute' will use a brute-force search.\\n        - 'auto' will attempt to decide the most appropriate algorithm\\n          based on the values passed to `fit` method.\\n\\n        Note: fitting on sparse input will override the setting of\\n        this parameter, using brute force.\\n\\n- `leaf_size`: int, default=30\\n        Leaf size passed to BallTree or KDTree.  This can affect the\\n        speed of the construction and query, as well as the memory\\n        required to store the tree.  The optimal value depends on the\\n        nature of the problem.\\n\\n- `p`: int, default=2\\n        Power parameter for the Minkowski metric. When p = 1, this is\\n        equivalent to using manhattan_distance (l1), and euclidean_distance\\n        (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\\n\\n- `metric`: str or callable, default='minkowski'\\n        the distance metric to use for the tree.  The default metric is\\n        minkowski, and with p=2 is equivalent to the standard Euclidean\\n        metric. See the documentation of `DistanceMetric` for a\\n        list of available metrics.\\n        If metric is \\\"precomputed\\\", X is assumed to be a distance matrix and\\n        must be square during fit. X may be a :term:`sparse graph`,\\n        in which case only \\\"nonzero\\\" elements may be considered neighbors.\\n\\n- `metric_params`: dict, default=None\\n        Additional keyword arguments for the metric function.\\n\\n- `n_jobs`: int, default=None\\n        The number of parallel jobs to run for neighbors search.\\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\\n        for more details.\\n        Doesn't affect `fit` method.\\n\\n    Attributes\\n    ----------\\n- `effective_metric_`: str or callable\\n        The distance metric to use. It will be same as the `metric` parameter\\n        or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\\n        'minkowski' and `p` parameter set to 2.\\n\\n- `effective_metric_params_`: dict\\n        Additional keyword arguments for the metric function. For most metrics\\n        will be same with `metric_params` parameter, but may also contain the\\n        `p` parameter value if the `effective_metric_` attribute is set to\\n        'minkowski'.\\n\\n    Examples\\n    --------\\n    >>> X = [[0], [1], [2], [3]]\\n    >>> y = [0, 0, 1, 1]\\n    >>> from sklearn.neighbors import KNeighborsRegressor\\n    >>> neigh = KNeighborsRegressor(n_neighbors=2)\\n    >>> neigh.fit(X, y)\\n    KNeighborsRegressor(...)\\n    >>> print(neigh.predict([[1.5]]))\\n    [0.5]\\n\\n    See also\\n    --------\\n    NearestNeighbors\\n    RadiusNeighborsRegressor\\n    KNeighborsClassifier\\n    RadiusNeighborsClassifier\\n\\n    Notes\\n    -----\\n    See Nearest Neighbors: `neighbors` in the online documentation\\n    for a discussion of the choice of ``algorithm`` and ``leaf_size``.\\n\\n\\n---\\n**Warning**\\n\\nRegarding the Nearest Neighbors algorithms, if it is found that two\\nneighbors, neighbor `k+1` and `k`, have identical distances but\\ndifferent labels, the results will depend on the ordering of the\\ntraining data.\\n\\nps://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm\\n\\n---\\n\"]] [:hr] [:hr]] [:div [:h3 \":sklearn.regression/kernel-ridge\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [6 2]:\\n\\n|          :name | :default |\\n|----------------|----------|\\n|         :alpha |        1 |\\n|        :coef-0 |        1 |\\n|        :degree |        3 |\\n|         :gamma |          |\\n|        :kernel |   linear |\\n| :kernel-params |          |\\n\"]]] [:span [:p/markdown \"Kernel ridge regression.\\n\\n    Kernel ridge regression (KRR) combines ridge regression (linear least\\n    squares with l2-norm regularization) with the kernel trick. It thus\\n    learns a linear function in the space induced by the respective kernel and\\n    the data. For non-linear kernels, this corresponds to a non-linear\\n    function in the original space.\\n\\n    The form of the model learned by KRR is identical to support vector\\n    regression (SVR). However, different loss functions are used: KRR uses\\n    squared error loss while support vector regression uses epsilon-insensitive\\n    loss, both combined with l2 regularization. In contrast to SVR, fitting a\\n    KRR model can be done in closed-form and is typically faster for\\n    medium-sized datasets. On the other hand, the learned model is non-sparse\\n    and thus slower than SVR, which learns a sparse model for epsilon > 0, at\\n    prediction-time.\\n\\n    This estimator has built-in support for multi-variate regression\\n    (i.e., when y is a 2d-array of shape [n_samples, n_targets]).\\n\\n    Read more in the User Guide: `kernel_ridge`.\\n\\n    Parameters\\n    ----------\\n- `alpha`: float or array-like of shape (n_targets,)\\n        Regularization strength; must be a positive float. Regularization\\n        improves the conditioning of the problem and reduces the variance of\\n        the estimates. Larger values specify stronger regularization.\\n        Alpha corresponds to ``1 / (2C)`` in other linear models such as\\n        `~sklearn.linear_model.LogisticRegression` or\\n        `sklearn.svm.LinearSVC`. If an array is passed, penalties are\\n        assumed to be specific to the targets. Hence they must correspond in\\n        number. See :ref:`ridge_regression` for formula.\\n\\n- `kernel`: string or callable, default=\\\"linear\\\"\\n        Kernel mapping used internally. This parameter is directly passed to\\n        `sklearn.metrics.pairwise.pairwise_kernel`.\\n        If `kernel` is a string, it must be one of the metrics\\n        in `pairwise.PAIRWISE_KERNEL_FUNCTIONS`.\\n        If `kernel` is \\\"precomputed\\\", X is assumed to be a kernel matrix.\\n        Alternatively, if `kernel` is a callable function, it is called on\\n        each pair of instances (rows) and the resulting value recorded. The\\n        callable should take two rows from X as input and return the\\n        corresponding kernel value as a single number. This means that\\n        callables from `sklearn.metrics.pairwise` are not allowed, as\\n        they operate on matrices, not single samples. Use the string\\n        identifying the kernel instead.\\n\\n- `gamma`: float, default=None\\n        Gamma parameter for the RBF, laplacian, polynomial, exponential chi2\\n        and sigmoid kernels. Interpretation of the default value is left to\\n        the kernel; see the documentation for sklearn.metrics.pairwise.\\n        Ignored by other kernels.\\n\\n- `degree`: float, default=3\\n        Degree of the polynomial kernel. Ignored by other kernels.\\n\\n- `coef0`: float, default=1\\n        Zero coefficient for polynomial and sigmoid kernels.\\n        Ignored by other kernels.\\n\\n- `kernel_params`: mapping of string to any, optional\\n        Additional parameters (keyword arguments) for kernel function passed\\n        as callable object.\\n\\n    Attributes\\n    ----------\\n- `dual_coef_`: ndarray of shape (n_samples,) or (n_samples, n_targets)\\n        Representation of weight vector(s) in kernel space\\n\\n- `X_fit_`: {ndarray, sparse matrix} of shape (n_samples, n_features)\\n        Training data, which is also required for prediction. If\\n        kernel == \\\"precomputed\\\" this is instead the precomputed\\n        training matrix, of shape (n_samples, n_samples).\\n\\n    References\\n    ----------\\n    * Kevin P. Murphy\\n      \\\"Machine Learning: A Probabilistic Perspective\\\", The MIT Press\\n      chapter 14.4.3, pp. 492-493\\n\\n    See also\\n    --------\\n    sklearn.linear_model.Ridge:\\n        Linear ridge regression.\\n    sklearn.svm.SVR:\\n        Support Vector Regression implemented using libsvm.\\n\\n    Examples\\n    --------\\n    >>> from sklearn.kernel_ridge import KernelRidge\\n    >>> import numpy as np\\n    >>> n_samples, n_features = 10, 5\\n    >>> rng = np.random.RandomState(0)\\n    >>> y = rng.randn(n_samples)\\n    >>> X = rng.randn(n_samples, n_features)\\n    >>> clf = KernelRidge(alpha=1.0)\\n    >>> clf.fit(X, y)\\n    KernelRidge(alpha=1.0)\\n    \"]] [:hr] [:hr]] [:div [:h3 \":sklearn.regression/lars\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [10 2]:\\n\\n|            :name |  :default |\\n|------------------|-----------|\\n|       :normalize |      true |\\n|        :fit-path |      true |\\n|             :eps | 2.220E-16 |\\n|    :random-state |           |\\n|          :jitter |           |\\n|          :copy-x |      true |\\n|      :precompute |      auto |\\n|   :fit-intercept |      true |\\n| :n-nonzero-coefs |       500 |\\n|         :verbose |     false |\\n\"]]] [:span [:p/markdown \"Least Angle Regression model a.k.a. LAR\\n\\n    Read more in the User Guide: `least_angle_regression`.\\n\\n    Parameters\\n    ----------\\n- `fit_intercept`: bool, default=True\\n        Whether to calculate the intercept for this model. If set\\n        to false, no intercept will be used in calculations\\n        (i.e. data is expected to be centered).\\n\\n- `verbose`: bool or int, default=False\\n        Sets the verbosity amount\\n\\n- `normalize`: bool, default=True\\n        This parameter is ignored when ``fit_intercept`` is set to False.\\n        If True, the regressors X will be normalized before regression by\\n        subtracting the mean and dividing by the l2-norm.\\n        If you wish to standardize, please use\\n        `sklearn.preprocessing.StandardScaler` before calling ``fit``\\n        on an estimator with ``normalize=False``.\\n\\n- `precompute`: bool, 'auto' or array-like , default='auto'\\n        Whether to use a precomputed Gram matrix to speed up\\n        calculations. If set to ``'auto'`` let us decide. The Gram\\n        matrix can also be passed as argument.\\n\\n- `n_nonzero_coefs`: int, default=500\\n        Target number of non-zero coefficients. Use ``np.inf`` for no limit.\\n\\n- `eps`: float, optional\\n        The machine-precision regularization in the computation of the\\n        Cholesky diagonal factors. Increase this for very ill-conditioned\\n        systems. Unlike the ``tol`` parameter in some iterative\\n        optimization-based algorithms, this parameter does not control\\n        the tolerance of the optimization.\\n        By default, ``np.finfo(np.float).eps`` is used.\\n\\n- `copy_X`: bool, default=True\\n        If ``True``, X will be copied; else, it may be overwritten.\\n\\n- `fit_path`: bool, default=True\\n        If True the full path is stored in the ``coef_path_`` attribute.\\n        If you compute the solution for a large problem or many targets,\\n        setting ``fit_path`` to ``False`` will lead to a speedup, especially\\n        with a small alpha.\\n\\n- `jitter`: float, default=None\\n        Upper bound on a uniform noise parameter to be added to the\\n        `y` values, to satisfy the model's assumption of\\n        one-at-a-time computations. Might help with stability.\\n\\n- `random_state`: int, RandomState instance or None (default)\\n        Determines random number generation for jittering. Pass an int\\n        for reproducible output across multiple function calls.\\n        See :term:`Glossary <random_state>`. Ignored if `jitter` is None.\\n\\n    Attributes\\n    ----------\\n- `alphas_`: array-like of shape (n_alphas + 1,) | list of n_targets such             arrays\\n        Maximum of covariances (in absolute value) at each iteration.         ``n_alphas`` is either ``n_nonzero_coefs`` or ``n_features``,         whichever is smaller.\\n\\n- `active_`: list, length = n_alphas | list of n_targets such lists\\n        Indices of active variables at the end of the path.\\n\\n- `coef_path_`: array-like of shape (n_features, n_alphas + 1)         | list of n_targets such arrays\\n        The varying values of the coefficients along the path. It is not\\n        present if the ``fit_path`` parameter is ``False``.\\n\\n- `coef_`: array-like of shape (n_features,) or (n_targets, n_features)\\n        Parameter vector (w in the formulation formula).\\n\\n- `intercept_`: float or array-like of shape (n_targets,)\\n        Independent term in decision function.\\n\\n- `n_iter_`: array-like or int\\n        The number of iterations taken by lars_path to find the\\n        grid of alphas for each target.\\n\\n    Examples\\n    --------\\n    >>> from sklearn import linear_model\\n    >>> reg = linear_model.Lars(n_nonzero_coefs=1)\\n    >>> reg.fit([[-1, 1], [0, 0], [1, 1]], [-1.1111, 0, -1.1111])\\n    Lars(n_nonzero_coefs=1)\\n    >>> print(reg.coef_)\\n    [ 0. -1.11...]\\n\\n    See also\\n    --------\\n    lars_path, LarsCV\\n    sklearn.decomposition.sparse_encode\\n\\n    \"]] [:hr] [:hr]] [:div [:h3 \":sklearn.regression/lars-cv\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [10 2]:\\n\\n|          :name |  :default |\\n|----------------|-----------|\\n|     :normalize |      true |\\n|           :eps | 2.220E-16 |\\n|  :max-n-alphas |      1000 |\\n|      :max-iter |       500 |\\n|        :n-jobs |           |\\n|        :copy-x |      true |\\n|    :precompute |      auto |\\n| :fit-intercept |      true |\\n|            :cv |           |\\n|       :verbose |     false |\\n\"]]] [:span [:p/markdown \"Cross-validated Least Angle Regression model.\\n\\n    See glossary entry for :term:`cross-validation estimator`.\\n\\n    Read more in the User Guide: `least_angle_regression`.\\n\\n    Parameters\\n    ----------\\n- `fit_intercept`: bool, default=True\\n        whether to calculate the intercept for this model. If set\\n        to false, no intercept will be used in calculations\\n        (i.e. data is expected to be centered).\\n\\n- `verbose`: bool or int, default=False\\n        Sets the verbosity amount\\n\\n- `max_iter`: int, default=500\\n        Maximum number of iterations to perform.\\n\\n- `normalize`: bool, default=True\\n        This parameter is ignored when ``fit_intercept`` is set to False.\\n        If True, the regressors X will be normalized before regression by\\n        subtracting the mean and dividing by the l2-norm.\\n        If you wish to standardize, please use\\n        `sklearn.preprocessing.StandardScaler` before calling ``fit``\\n        on an estimator with ``normalize=False``.\\n\\n- `precompute`: bool, 'auto' or array-like , default='auto'\\n        Whether to use a precomputed Gram matrix to speed up\\n        calculations. If set to ``'auto'`` let us decide. The Gram matrix\\n        cannot be passed as argument since we will use only subsets of X.\\n\\n- `cv`: int, cross-validation generator or an iterable, default=None\\n        Determines the cross-validation splitting strategy.\\n        Possible inputs for cv are:\\n\\n        - None, to use the default 5-fold cross-validation,\\n        - integer, to specify the number of folds.\\n        - :term:`CV splitter`,\\n        - An iterable yielding (train, test) splits as arrays of indices.\\n\\n        For integer/None inputs, `KFold` is used.\\n\\n        Refer User Guide: `cross_validation` for the various\\n        cross-validation strategies that can be used here.\\n\\n        *Changed in 0.22*\\n            ``cv`` default value if None changed from 3-fold to 5-fold.\\n\\n- `max_n_alphas`: int, default=1000\\n        The maximum number of points on the path used to compute the\\n        residuals in the cross-validation\\n\\n- `n_jobs`: int or None, default=None\\n        Number of CPUs to use during the cross validation.\\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\\n        for more details.\\n\\n- `eps`: float, optional\\n        The machine-precision regularization in the computation of the\\n        Cholesky diagonal factors. Increase this for very ill-conditioned\\n        systems. By default, ``np.finfo(np.float).eps`` is used.\\n\\n- `copy_X`: bool, default=True\\n        If ``True``, X will be copied; else, it may be overwritten.\\n\\n    Attributes\\n    ----------\\n- `coef_`: array-like of shape (n_features,)\\n        parameter vector (w in the formulation formula)\\n\\n- `intercept_`: float\\n        independent term in decision function\\n\\n- `coef_path_`: array-like of shape (n_features, n_alphas)\\n        the varying values of the coefficients along the path\\n\\n- `alpha_`: float\\n        the estimated regularization parameter alpha\\n\\n- `alphas_`: array-like of shape (n_alphas,)\\n        the different values of alpha along the path\\n\\n- `cv_alphas_`: array-like of shape (n_cv_alphas,)\\n        all the values of alpha along the path for the different folds\\n\\n- `mse_path_`: array-like of shape (n_folds, n_cv_alphas)\\n        the mean square error on left-out for each fold along the path\\n        (alpha values given by ``cv_alphas``)\\n\\n- `n_iter_`: array-like or int\\n        the number of iterations run by Lars with the optimal alpha.\\n\\n    Examples\\n    --------\\n    >>> from sklearn.linear_model import LarsCV\\n    >>> from sklearn.datasets import make_regression\\n    >>> X, y = make_regression(n_samples=200, noise=4.0, random_state=0)\\n    >>> reg = LarsCV(cv=5).fit(X, y)\\n    >>> reg.score(X, y)\\n    0.9996...\\n    >>> reg.alpha_\\n    0.0254...\\n    >>> reg.predict(X[:1,])\\n    array([154.0842...])\\n\\n    See also\\n    --------\\n    lars_path, LassoLars, LassoLarsCV\\n    \"]] [:hr] [:hr]] [:div [:h3 \":sklearn.regression/lasso\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [11 2]:\\n\\n|          :name |  :default |\\n|----------------|-----------|\\n|     :normalize |     false |\\n|      :positive |     false |\\n|           :tol | 0.0001000 |\\n|      :max-iter |      1000 |\\n|  :random-state |           |\\n|        :copy-x |      true |\\n|    :precompute |     false |\\n| :fit-intercept |      true |\\n|         :alpha |     1.000 |\\n|    :warm-start |     false |\\n|     :selection |    cyclic |\\n\"]]] [:span [:p/markdown \"Linear Model trained with L1 prior as regularizer (aka the Lasso)\\n\\n    The optimization objective for Lasso is\\n\\n```python\\n(1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\\n\\nnically the Lasso model is optimizing the same objective function as\\nElastic Net with ``l1_ratio=1.0`` (no L2 penalty).\\n\\n more in the :ref:`User Guide <lasso>`.\\n\\nmeters\\n------\\na : float, default=1.0\\nConstant that multiplies the L1 term. Defaults to 1.0.\\n``alpha = 0`` is equivalent to an ordinary least square, solved\\nby the :class:`LinearRegression` object. For numerical\\nreasons, using ``alpha = 0`` with the ``Lasso`` object is not advised.\\nGiven this, you should use the :class:`LinearRegression` object.\\n\\nintercept : bool, default=True\\nWhether to calculate the intercept for this model. If set\\nto False, no intercept will be used in calculations\\n(i.e. data is expected to be centered).\\n\\nalize : bool, default=False\\nThis parameter is ignored when ``fit_intercept`` is set to False.\\nIf True, the regressors X will be normalized before regression by\\nsubtracting the mean and dividing by the l2-norm.\\nIf you wish to standardize, please use\\n:class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\\non an estimator with ``normalize=False``.\\n\\nompute : 'auto', bool or array-like of shape (n_features, n_features),                 default=False\\nWhether to use a precomputed Gram matrix to speed up\\ncalculations. If set to ``'auto'`` let us decide. The Gram\\nmatrix can also be passed as argument. For sparse input\\nthis option is always ``True`` to preserve sparsity.\\n\\n_X : bool, default=True\\nIf ``True``, X will be copied; else, it may be overwritten.\\n\\niter : int, default=1000\\nThe maximum number of iterations\\n\\n: float, default=1e-4\\nThe tolerance for the optimization: if the updates are\\nsmaller than ``tol``, the optimization code checks the\\ndual gap for optimality and continues until it is smaller\\nthan ``tol``.\\n\\n_start : bool, default=False\\nWhen set to True, reuse the solution of the previous call to fit as\\ninitialization, otherwise, just erase the previous solution.\\nSee :term:`the Glossary <warm_start>`.\\n\\ntive : bool, default=False\\nWhen set to ``True``, forces the coefficients to be positive.\\n\\nom_state : int, RandomState instance, default=None\\nThe seed of the pseudo random number generator that selects a random\\nfeature to update. Used when ``selection`` == 'random'.\\nPass an int for reproducible output across multiple function calls.\\nSee :term:`Glossary <random_state>`.\\n\\nction : {'cyclic', 'random'}, default='cyclic'\\nIf set to 'random', a random coefficient is updated every iteration\\nrather than looping over features sequentially by default. This\\n(setting to 'random') often leads to significantly faster convergence\\nespecially when tol is higher than 1e-4.\\n\\nibutes\\n------\\n_ : ndarray of shape (n_features,) or (n_targets, n_features)\\nparameter vector (w in the cost function formula)\\n\\nse_coef_ : sparse matrix of shape (n_features, 1) or             (n_targets, n_features)\\n``sparse_coef_`` is a readonly property derived from ``coef_``\\n\\nrcept_ : float or ndarray of shape (n_targets,)\\nindependent term in decision function.\\n\\ner_ : int or list of int\\nnumber of iterations run by the coordinate descent solver to reach\\nthe specified tolerance.\\n\\nples\\n----\\nfrom sklearn import linear_model\\nclf = linear_model.Lasso(alpha=0.1)\\nclf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])\\no(alpha=0.1)\\nprint(clf.coef_)\\n5 0.  ]\\nprint(clf.intercept_)\\n...\\n\\nalso\\n----\\n_path\\no_path\\noLars\\noCV\\noLarsCV\\narn.decomposition.sparse_encode\\n\\ns\\n-\\nalgorithm used to fit the model is coordinate descent.\\n\\nvoid unnecessary memory duplication the X argument of the fit method\\nld be directly passed as a Fortran-contiguous numpy array.\\n```\\n\"]] [:hr] [:hr]] [:div [:h3 \":sklearn.regression/lasso-cv\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [15 2]:\\n\\n|          :name |  :default |\\n|----------------|-----------|\\n|     :normalize |     false |\\n|      :positive |     false |\\n|           :tol | 0.0001000 |\\n|      :n-alphas |       100 |\\n|           :eps |  0.001000 |\\n|        :alphas |           |\\n|      :max-iter |      1000 |\\n|        :n-jobs |           |\\n|  :random-state |           |\\n|        :copy-x |      true |\\n|    :precompute |      auto |\\n| :fit-intercept |      true |\\n|            :cv |           |\\n|     :selection |    cyclic |\\n|       :verbose |     false |\\n\"]]] [:span [:p/markdown \"Lasso linear model with iterative fitting along a regularization path.\\n\\n    See glossary entry for :term:`cross-validation estimator`.\\n\\n    The best model is selected by cross-validation.\\n\\n    The optimization objective for Lasso is\\n\\n```python\\n(1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\\n\\n more in the :ref:`User Guide <lasso>`.\\n\\nmeters\\n------\\n: float, default=1e-3\\nLength of the path. ``eps=1e-3`` means that\\n``alpha_min / alpha_max = 1e-3``.\\n\\nphas : int, default=100\\nNumber of alphas along the regularization path\\n\\nas : ndarray, default=None\\nList of alphas where to compute the models.\\nIf ``None`` alphas are set automatically\\n\\nintercept : bool, default=True\\nwhether to calculate the intercept for this model. If set\\nto false, no intercept will be used in calculations\\n(i.e. data is expected to be centered).\\n\\nalize : bool, default=False\\nThis parameter is ignored when ``fit_intercept`` is set to False.\\nIf True, the regressors X will be normalized before regression by\\nsubtracting the mean and dividing by the l2-norm.\\nIf you wish to standardize, please use\\n:class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\\non an estimator with ``normalize=False``.\\n\\nompute : 'auto', bool or array-like of shape (n_features, n_features),                 default='auto'\\nWhether to use a precomputed Gram matrix to speed up\\ncalculations. If set to ``'auto'`` let us decide. The Gram\\nmatrix can also be passed as argument.\\n\\niter : int, default=1000\\nThe maximum number of iterations\\n\\n: float, default=1e-4\\nThe tolerance for the optimization: if the updates are\\nsmaller than ``tol``, the optimization code checks the\\ndual gap for optimality and continues until it is smaller\\nthan ``tol``.\\n\\n_X : bool, default=True\\nIf ``True``, X will be copied; else, it may be overwritten.\\n\\n int, cross-validation generator or iterable, default=None\\nDetermines the cross-validation splitting strategy.\\nPossible inputs for cv are:\\n\\n- None, to use the default 5-fold cross-validation,\\n- int, to specify the number of folds.\\n- :term:`CV splitter`,\\n- An iterable yielding (train, test) splits as arrays of indices.\\n\\nFor int/None inputs, :class:`KFold` is used.\\n\\nRefer :ref:`User Guide <cross_validation>` for the various\\ncross-validation strategies that can be used here.\\n\\n.. versionchanged:: 0.22\\n    ``cv`` default value if None changed from 3-fold to 5-fold.\\n\\nose : bool or int, default=False\\nAmount of verbosity.\\n\\nbs : int, default=None\\nNumber of CPUs to use during the cross validation.\\n``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\\n``-1`` means using all processors. See :term:`Glossary <n_jobs>`\\nfor more details.\\n\\ntive : bool, default=False\\nIf positive, restrict regression coefficients to be positive\\n\\nom_state : int, RandomState instance, default=None\\nThe seed of the pseudo random number generator that selects a random\\nfeature to update. Used when ``selection`` == 'random'.\\nPass an int for reproducible output across multiple function calls.\\nSee :term:`Glossary <random_state>`.\\n\\nction : {'cyclic', 'random'}, default='cyclic'\\nIf set to 'random', a random coefficient is updated every iteration\\nrather than looping over features sequentially by default. This\\n(setting to 'random') often leads to significantly faster convergence\\nespecially when tol is higher than 1e-4.\\n\\nibutes\\n------\\na_ : float\\nThe amount of penalization chosen by cross validation\\n\\n_ : ndarray of shape (n_features,) or (n_targets, n_features)\\nparameter vector (w in the cost function formula)\\n\\nrcept_ : float or ndarray of shape (n_targets,)\\nindependent term in decision function.\\n\\npath_ : ndarray of shape (n_alphas, n_folds)\\nmean square error for the test set on each fold, varying alpha\\n\\nas_ : ndarray of shape (n_alphas,)\\nThe grid of alphas used for fitting\\n\\n_gap_ : float or ndarray of shape (n_targets,)\\nThe dual gap at the end of the optimization for the optimal alpha\\n(``alpha_``).\\n\\ner_ : int\\nnumber of iterations run by the coordinate descent solver to reach\\nthe specified tolerance for the optimal alpha.\\n\\nples\\n----\\nfrom sklearn.linear_model import LassoCV\\nfrom sklearn.datasets import make_regression\\nX, y = make_regression(noise=4, random_state=0)\\nreg = LassoCV(cv=5, random_state=0).fit(X, y)\\nreg.score(X, y)\\n93...\\nreg.predict(X[:1,])\\ny([-78.4951...])\\n\\ns\\n-\\nan example, see\\n:`examples/linear_model/plot_lasso_model_selection.py\\nx_glr_auto_examples_linear_model_plot_lasso_model_selection.py>`.\\n\\nvoid unnecessary memory duplication the X argument of the fit method\\nld be directly passed as a Fortran-contiguous numpy array.\\n\\nalso\\n----\\n_path\\no_path\\noLars\\no\\noLarsCV\\n```\\n\"]] [:hr] [:hr]] [:div [:h3 \":sklearn.regression/lasso-lars\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [12 2]:\\n\\n|          :name |  :default |\\n|----------------|-----------|\\n|     :normalize |      true |\\n|      :positive |     false |\\n|      :fit-path |      true |\\n|           :eps | 2.220E-16 |\\n|      :max-iter |       500 |\\n|  :random-state |           |\\n|        :jitter |           |\\n|        :copy-x |      true |\\n|    :precompute |      auto |\\n| :fit-intercept |      true |\\n|         :alpha |     1.000 |\\n|       :verbose |     false |\\n\"]]] [:span [:p/markdown \"Lasso model fit with Least Angle Regression a.k.a. Lars\\n\\n    It is a Linear Model trained with an L1 prior as regularizer.\\n\\n    The optimization objective for Lasso is\\n\\n```python\\n(1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\\n\\nRead more in the :ref:`User Guide <least_angle_regression>`.\\n\\nParameters\\n----------\\nalpha : float, default=1.0\\n    Constant that multiplies the penalty term. Defaults to 1.0.\\n    ``alpha = 0`` is equivalent to an ordinary least square, solved\\n    by :class:`LinearRegression`. For numerical reasons, using\\n    ``alpha = 0`` with the LassoLars object is not advised and you\\n    should prefer the LinearRegression object.\\n\\nfit_intercept : bool, default=True\\n    whether to calculate the intercept for this model. If set\\n    to false, no intercept will be used in calculations\\n    (i.e. data is expected to be centered).\\n\\nverbose : bool or int, default=False\\n    Sets the verbosity amount\\n\\nnormalize : bool, default=True\\n    This parameter is ignored when ``fit_intercept`` is set to False.\\n    If True, the regressors X will be normalized before regression by\\n    subtracting the mean and dividing by the l2-norm.\\n    If you wish to standardize, please use\\n    :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\\n    on an estimator with ``normalize=False``.\\n\\nprecompute : bool, 'auto' or array-like, default='auto'\\n    Whether to use a precomputed Gram matrix to speed up\\n    calculations. If set to ``'auto'`` let us decide. The Gram\\n    matrix can also be passed as argument.\\n\\nmax_iter : int, default=500\\n    Maximum number of iterations to perform.\\n\\neps : float, optional\\n    The machine-precision regularization in the computation of the\\n    Cholesky diagonal factors. Increase this for very ill-conditioned\\n    systems. Unlike the ``tol`` parameter in some iterative\\n    optimization-based algorithms, this parameter does not control\\n    the tolerance of the optimization.\\n    By default, ``np.finfo(np.float).eps`` is used.\\n\\ncopy_X : bool, default=True\\n    If True, X will be copied; else, it may be overwritten.\\n\\nfit_path : bool, default=True\\n    If ``True`` the full path is stored in the ``coef_path_`` attribute.\\n    If you compute the solution for a large problem or many targets,\\n    setting ``fit_path`` to ``False`` will lead to a speedup, especially\\n    with a small alpha.\\n\\npositive : bool, default=False\\n    Restrict coefficients to be >= 0. Be aware that you might want to\\n    remove fit_intercept which is set True by default.\\n    Under the positive restriction the model coefficients will not converge\\n    to the ordinary-least-squares solution for small values of alpha.\\n    Only coefficients up to the smallest alpha value (``alphas_[alphas_ >\\n    0.].min()`` when fit_path=True) reached by the stepwise Lars-Lasso\\n    algorithm are typically in congruence with the solution of the\\n    coordinate descent Lasso estimator.\\n\\njitter : float, default=None\\n    Upper bound on a uniform noise parameter to be added to the\\n    `y` values, to satisfy the model's assumption of\\n    one-at-a-time computations. Might help with stability.\\n\\nrandom_state : int, RandomState instance or None (default)\\n    Determines random number generation for jittering. Pass an int\\n    for reproducible output across multiple function calls.\\n    See :term:`Glossary <random_state>`. Ignored if `jitter` is None.\\n\\nAttributes\\n----------\\nalphas_ : array-like of shape (n_alphas + 1,) | list of n_targets such             arrays\\n    Maximum of covariances (in absolute value) at each iteration.         ``n_alphas`` is either ``max_iter``, ``n_features``, or the number of         nodes in the path with correlation greater than ``alpha``, whichever         is smaller.\\n\\nactive_ : list, length = n_alphas | list of n_targets such lists\\n    Indices of active variables at the end of the path.\\n\\ncoef_path_ : array-like of shape (n_features, n_alphas + 1) or list\\n    If a list is passed it's expected to be one of n_targets such arrays.\\n    The varying values of the coefficients along the path. It is not\\n    present if the ``fit_path`` parameter is ``False``.\\n\\ncoef_ : array-like of shape (n_features,) or (n_targets, n_features)\\n    Parameter vector (w in the formulation formula).\\n\\nintercept_ : float or array-like of shape (n_targets,)\\n    Independent term in decision function.\\n\\nn_iter_ : array-like or int.\\n    The number of iterations taken by lars_path to find the\\n    grid of alphas for each target.\\n\\nExamples\\n--------\\n>>> from sklearn import linear_model\\n>>> reg = linear_model.LassoLars(alpha=0.01)\\n>>> reg.fit([[-1, 1], [0, 0], [1, 1]], [-1, 0, -1])\\nLassoLars(alpha=0.01)\\n>>> print(reg.coef_)\\n[ 0.         -0.963257...]\\n\\nSee also\\n--------\\nlars_path\\nlasso_path\\nLasso\\nLassoCV\\nLassoLarsCV\\nLassoLarsIC\\nsklearn.decomposition.sparse_encode\\n\\n```\\n\"]] [:hr] [:hr]] [:div [:h3 \":sklearn.regression/lasso-lars-cv\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [11 2]:\\n\\n|          :name |  :default |\\n|----------------|-----------|\\n|     :normalize |      true |\\n|      :positive |     false |\\n|           :eps | 2.220E-16 |\\n|  :max-n-alphas |      1000 |\\n|      :max-iter |       500 |\\n|        :n-jobs |           |\\n|        :copy-x |      true |\\n|    :precompute |      auto |\\n| :fit-intercept |      true |\\n|            :cv |           |\\n|       :verbose |     false |\\n\"]]] [:span [:p/markdown \"Cross-validated Lasso, using the LARS algorithm.\\n\\n    See glossary entry for :term:`cross-validation estimator`.\\n\\n    The optimization objective for Lasso is\\n\\n```python\\n(1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\\n\\nRead more in the :ref:`User Guide <least_angle_regression>`.\\n\\nParameters\\n----------\\nfit_intercept : bool, default=True\\n    whether to calculate the intercept for this model. If set\\n    to false, no intercept will be used in calculations\\n    (i.e. data is expected to be centered).\\n\\nverbose : bool or int, default=False\\n    Sets the verbosity amount\\n\\nmax_iter : int, default=500\\n    Maximum number of iterations to perform.\\n\\nnormalize : bool, default=True\\n    This parameter is ignored when ``fit_intercept`` is set to False.\\n    If True, the regressors X will be normalized before regression by\\n    subtracting the mean and dividing by the l2-norm.\\n    If you wish to standardize, please use\\n    :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\\n    on an estimator with ``normalize=False``.\\n\\nprecompute : bool or 'auto' , default='auto'\\n    Whether to use a precomputed Gram matrix to speed up\\n    calculations. If set to ``'auto'`` let us decide. The Gram matrix\\n    cannot be passed as argument since we will use only subsets of X.\\n\\ncv : int, cross-validation generator or an iterable, default=None\\n    Determines the cross-validation splitting strategy.\\n    Possible inputs for cv are:\\n\\n    - None, to use the default 5-fold cross-validation,\\n    - integer, to specify the number of folds.\\n    - :term:`CV splitter`,\\n    - An iterable yielding (train, test) splits as arrays of indices.\\n\\n    For integer/None inputs, :class:`KFold` is used.\\n\\n    Refer :ref:`User Guide <cross_validation>` for the various\\n    cross-validation strategies that can be used here.\\n\\n    .. versionchanged:: 0.22\\n        ``cv`` default value if None changed from 3-fold to 5-fold.\\n\\nmax_n_alphas : int, default=1000\\n    The maximum number of points on the path used to compute the\\n    residuals in the cross-validation\\n\\nn_jobs : int or None, default=None\\n    Number of CPUs to use during the cross validation.\\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\\n    for more details.\\n\\neps : float, optional\\n    The machine-precision regularization in the computation of the\\n    Cholesky diagonal factors. Increase this for very ill-conditioned\\n    systems. By default, ``np.finfo(np.float).eps`` is used.\\n\\ncopy_X : bool, default=True\\n    If True, X will be copied; else, it may be overwritten.\\n\\npositive : bool, default=False\\n    Restrict coefficients to be >= 0. Be aware that you might want to\\n    remove fit_intercept which is set True by default.\\n    Under the positive restriction the model coefficients do not converge\\n    to the ordinary-least-squares solution for small values of alpha.\\n    Only coefficients up to the smallest alpha value (``alphas_[alphas_ >\\n    0.].min()`` when fit_path=True) reached by the stepwise Lars-Lasso\\n    algorithm are typically in congruence with the solution of the\\n    coordinate descent Lasso estimator.\\n    As a consequence using LassoLarsCV only makes sense for problems where\\n    a sparse solution is expected and/or reached.\\n\\nAttributes\\n----------\\ncoef_ : array-like of shape (n_features,)\\n    parameter vector (w in the formulation formula)\\n\\nintercept_ : float\\n    independent term in decision function.\\n\\ncoef_path_ : array-like of shape (n_features, n_alphas)\\n    the varying values of the coefficients along the path\\n\\nalpha_ : float\\n    the estimated regularization parameter alpha\\n\\nalphas_ : array-like of shape (n_alphas,)\\n    the different values of alpha along the path\\n\\ncv_alphas_ : array-like of shape (n_cv_alphas,)\\n    all the values of alpha along the path for the different folds\\n\\nmse_path_ : array-like of shape (n_folds, n_cv_alphas)\\n    the mean square error on left-out for each fold along the path\\n    (alpha values given by ``cv_alphas``)\\n\\nn_iter_ : array-like or int\\n    the number of iterations run by Lars with the optimal alpha.\\n\\nExamples\\n--------\\n>>> from sklearn.linear_model import LassoLarsCV\\n>>> from sklearn.datasets import make_regression\\n>>> X, y = make_regression(noise=4.0, random_state=0)\\n>>> reg = LassoLarsCV(cv=5).fit(X, y)\\n>>> reg.score(X, y)\\n0.9992...\\n>>> reg.alpha_\\n0.0484...\\n>>> reg.predict(X[:1,])\\narray([-77.8723...])\\n\\nNotes\\n-----\\n\\nThe object solves the same problem as the LassoCV object. However,\\nunlike the LassoCV, it find the relevant alphas values by itself.\\nIn general, because of this property, it will be more stable.\\nHowever, it is more fragile to heavily multicollinear datasets.\\n\\nIt is more efficient than the LassoCV if only a small number of\\nfeatures are selected compared to the total number, for instance if\\nthere are very few samples compared to the number of features.\\n\\nSee also\\n--------\\nlars_path, LassoLars, LarsCV, LassoCV\\n```\\n\"]] [:hr] [:hr]] [:div [:h3 \":sklearn.regression/lasso-lars-ic\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [9 2]:\\n\\n|          :name |  :default |\\n|----------------|-----------|\\n|     :normalize |      true |\\n|      :positive |     false |\\n|           :eps | 2.220E-16 |\\n|      :max-iter |       500 |\\n|        :copy-x |      true |\\n|    :precompute |      auto |\\n| :fit-intercept |      true |\\n|     :criterion |       aic |\\n|       :verbose |     false |\\n\"]]] [:span [:p/markdown \"Lasso model fit with Lars using BIC or AIC for model selection\\n\\n    The optimization objective for Lasso is\\n\\n```python\\n(1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\\n\\nAIC is the Akaike information criterion and BIC is the Bayes\\nInformation criterion. Such criteria are useful to select the value\\nof the regularization parameter by making a trade-off between the\\ngoodness of fit and the complexity of the model. A good model should\\nexplain well the data while being simple.\\n\\nRead more in the :ref:`User Guide <least_angle_regression>`.\\n\\nParameters\\n----------\\ncriterion : {'bic' , 'aic'}, default='aic'\\n    The type of criterion to use.\\n\\nfit_intercept : bool, default=True\\n    whether to calculate the intercept for this model. If set\\n    to false, no intercept will be used in calculations\\n    (i.e. data is expected to be centered).\\n\\nverbose : bool or int, default=False\\n    Sets the verbosity amount\\n\\nnormalize : bool, default=True\\n    This parameter is ignored when ``fit_intercept`` is set to False.\\n    If True, the regressors X will be normalized before regression by\\n    subtracting the mean and dividing by the l2-norm.\\n    If you wish to standardize, please use\\n    :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\\n    on an estimator with ``normalize=False``.\\n\\nprecompute : bool, 'auto' or array-like, default='auto'\\n    Whether to use a precomputed Gram matrix to speed up\\n    calculations. If set to ``'auto'`` let us decide. The Gram\\n    matrix can also be passed as argument.\\n\\nmax_iter : int, default=500\\n    Maximum number of iterations to perform. Can be used for\\n    early stopping.\\n\\neps : float, optional\\n    The machine-precision regularization in the computation of the\\n    Cholesky diagonal factors. Increase this for very ill-conditioned\\n    systems. Unlike the ``tol`` parameter in some iterative\\n    optimization-based algorithms, this parameter does not control\\n    the tolerance of the optimization.\\n    By default, ``np.finfo(np.float).eps`` is used\\n\\ncopy_X : bool, default=True\\n    If True, X will be copied; else, it may be overwritten.\\n\\npositive : bool, default=False\\n    Restrict coefficients to be >= 0. Be aware that you might want to\\n    remove fit_intercept which is set True by default.\\n    Under the positive restriction the model coefficients do not converge\\n    to the ordinary-least-squares solution for small values of alpha.\\n    Only coefficients up to the smallest alpha value (``alphas_[alphas_ >\\n    0.].min()`` when fit_path=True) reached by the stepwise Lars-Lasso\\n    algorithm are typically in congruence with the solution of the\\n    coordinate descent Lasso estimator.\\n    As a consequence using LassoLarsIC only makes sense for problems where\\n    a sparse solution is expected and/or reached.\\n\\nAttributes\\n----------\\ncoef_ : array-like of shape (n_features,)\\n    parameter vector (w in the formulation formula)\\n\\nintercept_ : float\\n    independent term in decision function.\\n\\nalpha_ : float\\n    the alpha parameter chosen by the information criterion\\n\\nn_iter_ : int\\n    number of iterations run by lars_path to find the grid of\\n    alphas.\\n\\ncriterion_ : array-like of shape (n_alphas,)\\n    The value of the information criteria ('aic', 'bic') across all\\n    alphas. The alpha which has the smallest information criterion is\\n    chosen. This value is larger by a factor of ``n_samples`` compared to\\n    Eqns. 2.15 and 2.16 in (Zou et al, 2007).\\n\\n\\nExamples\\n--------\\n>>> from sklearn import linear_model\\n>>> reg = linear_model.LassoLarsIC(criterion='bic')\\n>>> reg.fit([[-1, 1], [0, 0], [1, 1]], [-1.1111, 0, -1.1111])\\nLassoLarsIC(criterion='bic')\\n>>> print(reg.coef_)\\n[ 0.  -1.11...]\\n\\nNotes\\n-----\\nThe estimation of the number of degrees of freedom is given by:\\n\\n\\\"On the degrees of freedom of the lasso\\\"\\nHui Zou, Trevor Hastie, and Robert Tibshirani\\nAnn. Statist. Volume 35, Number 5 (2007), 2173-2192.\\n\\nhttps://en.wikipedia.org/wiki/Akaike_information_criterion\\nhttps://en.wikipedia.org/wiki/Bayesian_information_criterion\\n\\nSee also\\n--------\\nlars_path, LassoLars, LassoLarsCV\\n```\\n\"]] [:hr] [:hr]] [:div [:h3 \":sklearn.regression/linear-regression\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [4 2]:\\n\\n|          :name | :default |\\n|----------------|----------|\\n|        :copy-x |     true |\\n| :fit-intercept |     true |\\n|        :n-jobs |          |\\n|     :normalize |    false |\\n\"]]] [:span [:p/markdown \"\\n    Ordinary least squares Linear Regression.\\n\\n    LinearRegression fits a linear model with coefficients w = (w1, ..., wp)\\n    to minimize the residual sum of squares between the observed targets in\\n    the dataset, and the targets predicted by the linear approximation.\\n\\n    Parameters\\n    ----------\\n- `fit_intercept`: bool, default=True\\n        Whether to calculate the intercept for this model. If set\\n        to False, no intercept will be used in calculations\\n        (i.e. data is expected to be centered).\\n\\n- `normalize`: bool, default=False\\n        This parameter is ignored when ``fit_intercept`` is set to False.\\n        If True, the regressors X will be normalized before regression by\\n        subtracting the mean and dividing by the l2-norm.\\n        If you wish to standardize, please use\\n        `sklearn.preprocessing.StandardScaler` before calling ``fit`` on\\n        an estimator with ``normalize=False``.\\n\\n- `copy_X`: bool, default=True\\n        If True, X will be copied; else, it may be overwritten.\\n\\n- `n_jobs`: int, default=None\\n        The number of jobs to use for the computation. This will only provide\\n        speedup for n_targets > 1 and sufficient large problems.\\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\\n        for more details.\\n\\n    Attributes\\n    ----------\\n- `coef_`: array of shape (n_features, ) or (n_targets, n_features)\\n        Estimated coefficients for the linear regression problem.\\n        If multiple targets are passed during the fit (y 2D), this\\n        is a 2D array of shape (n_targets, n_features), while if only\\n        one target is passed, this is a 1D array of length n_features.\\n\\n- `rank_`: int\\n        Rank of matrix `X`. Only available when `X` is dense.\\n\\n- `singular_`: array of shape (min(X, y),)\\n        Singular values of `X`. Only available when `X` is dense.\\n\\n- `intercept_`: float or array of shape (n_targets,)\\n        Independent term in the linear model. Set to 0.0 if\\n        `fit_intercept = False`.\\n\\n    See Also\\n    --------\\n- `sklearn.linear_model.Ridge`: Ridge regression addresses some of the\\n        problems of Ordinary Least Squares by imposing a penalty on the\\n        size of the coefficients with l2 regularization.\\n- `sklearn.linear_model.Lasso`: The Lasso is a linear model that estimates\\n        sparse coefficients with l1 regularization.\\n- `sklearn.linear_model.ElasticNet`: Elastic-Net is a linear regression\\n        model trained with both l1 and l2 -norm regularization of the\\n        coefficients.\\n\\n    Notes\\n    -----\\n    From the implementation point of view, this is just plain Ordinary\\n    Least Squares (scipy.linalg.lstsq) wrapped as a predictor object.\\n\\n    Examples\\n    --------\\n    >>> import numpy as np\\n    >>> from sklearn.linear_model import LinearRegression\\n    >>> X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\\n    >>> # y = 1 * x_0 + 2 * x_1 + 3\\n    >>> y = np.dot(X, np.array([1, 2])) + 3\\n    >>> reg = LinearRegression().fit(X, y)\\n    >>> reg.score(X, y)\\n    1.0\\n    >>> reg.coef_\\n    array([1., 2.])\\n    >>> reg.intercept_\\n    3.0000...\\n    >>> reg.predict(np.array([[3, 5]]))\\n    array([16.])\\n    \"]] [:hr] [:hr]] [:div [:h3 \":sklearn.regression/linear-svr\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [10 2]:\\n\\n|              :name |            :default |\\n|--------------------|---------------------|\\n|               :tol |           0.0001000 |\\n| :intercept-scaling |               1.000 |\\n|                 :c |               1.000 |\\n|          :max-iter |                1000 |\\n|      :random-state |                     |\\n|              :dual |                true |\\n|     :fit-intercept |                true |\\n|              :loss | epsilon_insensitive |\\n|           :verbose |                   0 |\\n|           :epsilon |               0.000 |\\n\"]]] [:span [:p/markdown \"Linear Support Vector Regression.\\n\\n    Similar to SVR with parameter kernel='linear', but implemented in terms of\\n    liblinear rather than libsvm, so it has more flexibility in the choice of\\n    penalties and loss functions and should scale better to large numbers of\\n    samples.\\n\\n    This class supports both dense and sparse input.\\n\\n    Read more in the User Guide: `svm_regression`.\\n\\n    *Added in 0.16*\\n\\n    Parameters\\n    ----------\\n- `epsilon`: float, default=0.0\\n        Epsilon parameter in the epsilon-insensitive loss function. Note\\n        that the value of this parameter depends on the scale of the target\\n        variable y. If unsure, set ``epsilon=0``.\\n\\n- `tol`: float, default=1e-4\\n        Tolerance for stopping criteria.\\n\\n- `C`: float, default=1.0\\n        Regularization parameter. The strength of the regularization is\\n        inversely proportional to C. Must be strictly positive.\\n\\n- `loss`: {'epsilon_insensitive', 'squared_epsilon_insensitive'},             default='epsilon_insensitive'\\n        Specifies the loss function. The epsilon-insensitive loss\\n        (standard SVR) is the L1 loss, while the squared epsilon-insensitive\\n        loss ('squared_epsilon_insensitive') is the L2 loss.\\n\\n- `fit_intercept`: bool, default=True\\n        Whether to calculate the intercept for this model. If set\\n        to false, no intercept will be used in calculations\\n        (i.e. data is expected to be already centered).\\n\\n- `intercept_scaling`: float, default=1.\\n        When self.fit_intercept is True, instance vector x becomes\\n        [x, self.intercept_scaling],\\n        i.e. a \\\"synthetic\\\" feature with constant value equals to\\n        intercept_scaling is appended to the instance vector.\\n        The intercept becomes intercept_scaling * synthetic feature weight\\n        Note! the synthetic feature weight is subject to l1/l2 regularization\\n        as all other features.\\n        To lessen the effect of regularization on synthetic feature weight\\n        (and therefore on the intercept) intercept_scaling has to be increased.\\n\\n- `dual`: bool, default=True\\n        Select the algorithm to either solve the dual or primal\\n        optimization problem. Prefer dual=False when n_samples > n_features.\\n\\n- `verbose`: int, default=0\\n        Enable verbose output. Note that this setting takes advantage of a\\n        per-process runtime setting in liblinear that, if enabled, may not work\\n        properly in a multithreaded context.\\n\\n- `random_state`: int or RandomState instance, default=None\\n        Controls the pseudo random number generation for shuffling the data.\\n        Pass an int for reproducible output across multiple function calls.\\n        See :term:`Glossary <random_state>`.\\n\\n- `max_iter`: int, default=1000\\n        The maximum number of iterations to be run.\\n\\n    Attributes\\n    ----------\\n- `coef_`: ndarray of shape (n_features) if n_classes == 2             else (n_classes, n_features)\\n        Weights assigned to the features (coefficients in the primal\\n        problem). This is only available in the case of a linear kernel.\\n\\n        `coef_` is a readonly property derived from `raw_coef_` that\\n        follows the internal memory layout of liblinear.\\n\\n- `intercept_`: ndarray of shape (1) if n_classes == 2 else (n_classes)\\n        Constants in decision function.\\n\\n- `n_iter_`: int\\n        Maximum number of iterations run across all classes.\\n\\n    Examples\\n    --------\\n    >>> from sklearn.svm import LinearSVR\\n    >>> from sklearn.pipeline import make_pipeline\\n    >>> from sklearn.preprocessing import StandardScaler\\n    >>> from sklearn.datasets import make_regression\\n    >>> X, y = make_regression(n_features=4, random_state=0)\\n    >>> regr = make_pipeline(StandardScaler(),\\n    ...                      LinearSVR(random_state=0, tol=1e-5))\\n    >>> regr.fit(X, y)\\n    Pipeline(steps=[('standardscaler', StandardScaler()),\\n                    ('linearsvr', LinearSVR(random_state=0, tol=1e-05))])\\n\\n    >>> print(regr.named_steps['linearsvr'].coef_)\\n    [18.582... 27.023... 44.357... 64.522...]\\n    >>> print(regr.named_steps['linearsvr'].intercept_)\\n    [-4...]\\n    >>> print(regr.predict([[0, 0, 0, 0]]))\\n    [-2.384...]\\n\\n\\n    See also\\n    --------\\n    LinearSVC\\n        Implementation of Support Vector Machine classifier using the\\n        same library as this class (liblinear).\\n\\n    SVR\\n        Implementation of Support Vector Machine regression using libsvm:\\n        the kernel can be non-linear but its SMO algorithm does not\\n        scale to large number of samples as LinearSVC does.\\n\\n    sklearn.linear_model.SGDRegressor\\n        SGDRegressor can optimize the same cost function as LinearSVR\\n        by adjusting the penalty and loss parameters. In addition it requires\\n        less memory, allows incremental (online) learning, and implements\\n        various loss functions and regularization regimes.\\n    \"]] [:hr] [:hr]] [:div [:h3 \":sklearn.regression/mlp-regressor\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [23 2]:\\n\\n|                :name |  :default |\\n|----------------------|-----------|\\n|    :n-iter-no-change |        10 |\\n|       :learning-rate |  constant |\\n|          :activation |      relu |\\n|  :hidden-layer-sizes |     [100] |\\n|                 :tol | 0.0001000 |\\n|              :beta-2 |    0.9990 |\\n|      :early-stopping |     false |\\n|  :nesterovs-momentum |      true |\\n|          :batch-size |      auto |\\n|              :solver |      adam |\\n|             :shuffle |      true |\\n|             :power-t |    0.5000 |\\n|             :max-fun |     15000 |\\n|              :beta-1 |    0.9000 |\\n|            :max-iter |       200 |\\n|        :random-state |           |\\n|            :momentum |    0.9000 |\\n|  :learning-rate-init |  0.001000 |\\n|               :alpha | 0.0001000 |\\n|          :warm-start |     false |\\n| :validation-fraction |    0.1000 |\\n|             :verbose |     false |\\n|             :epsilon | 1.000E-08 |\\n\"]]] [:span [:p/markdown \"Multi-layer Perceptron regressor.\\n\\n    This model optimizes the squared-loss using LBFGS or stochastic gradient\\n    descent.\\n\\n    *Added in 0.18*\\n\\n    Parameters\\n    ----------\\n- `hidden_layer_sizes`: tuple, length = n_layers - 2, default=(100,)\\n        The ith element represents the number of neurons in the ith\\n        hidden layer.\\n\\n- `activation`: {'identity', 'logistic', 'tanh', 'relu'}, default='relu'\\n        Activation function for the hidden layer.\\n\\n        - 'identity', no-op activation, useful to implement linear bottleneck,\\n          returns f(x) = x\\n\\n        - 'logistic', the logistic sigmoid function,\\n          returns f(x) = 1 / (1 + exp(-x)).\\n\\n        - 'tanh', the hyperbolic tan function,\\n          returns f(x) = tanh(x).\\n\\n        - 'relu', the rectified linear unit function,\\n          returns f(x) = max(0, x)\\n\\n- `solver`: {'lbfgs', 'sgd', 'adam'}, default='adam'\\n        The solver for weight optimization.\\n\\n        - 'lbfgs' is an optimizer in the family of quasi-Newton methods.\\n\\n        - 'sgd' refers to stochastic gradient descent.\\n\\n        - 'adam' refers to a stochastic gradient-based optimizer proposed by\\n          Kingma, Diederik, and Jimmy Ba\\n\\n        Note: The default solver 'adam' works pretty well on relatively\\n        large datasets (with thousands of training samples or more) in terms of\\n        both training time and validation score.\\n        For small datasets, however, 'lbfgs' can converge faster and perform\\n        better.\\n\\n- `alpha`: float, default=0.0001\\n        L2 penalty (regularization term) parameter.\\n\\n- `batch_size`: int, default='auto'\\n        Size of minibatches for stochastic optimizers.\\n        If the solver is 'lbfgs', the classifier will not use minibatch.\\n        When set to \\\"auto\\\", `batch_size=min(200, n_samples)`\\n\\n- `learning_rate`: {'constant', 'invscaling', 'adaptive'}, default='constant'\\n        Learning rate schedule for weight updates.\\n\\n        - 'constant' is a constant learning rate given by\\n          'learning_rate_init'.\\n\\n        - 'invscaling' gradually decreases the learning rate ``learning_rate_``\\n          at each time step 't' using an inverse scaling exponent of 'power_t'.\\n          effective_learning_rate = learning_rate_init / pow(t, power_t)\\n\\n        - 'adaptive' keeps the learning rate constant to\\n          'learning_rate_init' as long as training loss keeps decreasing.\\n          Each time two consecutive epochs fail to decrease training loss by at\\n          least tol, or fail to increase validation score by at least tol if\\n          'early_stopping' is on, the current learning rate is divided by 5.\\n\\n        Only used when solver='sgd'.\\n\\n- `learning_rate_init`: double, default=0.001\\n        The initial learning rate used. It controls the step-size\\n        in updating the weights. Only used when solver='sgd' or 'adam'.\\n\\n- `power_t`: double, default=0.5\\n        The exponent for inverse scaling learning rate.\\n        It is used in updating effective learning rate when the learning_rate\\n        is set to 'invscaling'. Only used when solver='sgd'.\\n\\n- `max_iter`: int, default=200\\n        Maximum number of iterations. The solver iterates until convergence\\n        (determined by 'tol') or this number of iterations. For stochastic\\n        solvers ('sgd', 'adam'), note that this determines the number of epochs\\n        (how many times each data point will be used), not the number of\\n        gradient steps.\\n\\n- `shuffle`: bool, default=True\\n        Whether to shuffle samples in each iteration. Only used when\\n        solver='sgd' or 'adam'.\\n\\n- `random_state`: int, RandomState instance, default=None\\n        Determines random number generation for weights and bias\\n        initialization, train-test split if early stopping is used, and batch\\n        sampling when solver='sgd' or 'adam'.\\n        Pass an int for reproducible results across multiple function calls.\\n        See :term:`Glossary <random_state>`.\\n\\n- `tol`: float, default=1e-4\\n        Tolerance for the optimization. When the loss or score is not improving\\n        by at least ``tol`` for ``n_iter_no_change`` consecutive iterations,\\n        unless ``learning_rate`` is set to 'adaptive', convergence is\\n        considered to be reached and training stops.\\n\\n- `verbose`: bool, default=False\\n        Whether to print progress messages to stdout.\\n\\n- `warm_start`: bool, default=False\\n        When set to True, reuse the solution of the previous\\n        call to fit as initialization, otherwise, just erase the\\n        previous solution. See :term:`the Glossary <warm_start>`.\\n\\n- `momentum`: float, default=0.9\\n        Momentum for gradient descent update.  Should be between 0 and 1. Only\\n        used when solver='sgd'.\\n\\n- `nesterovs_momentum`: boolean, default=True\\n        Whether to use Nesterov's momentum. Only used when solver='sgd' and\\n        momentum > 0.\\n\\n- `early_stopping`: bool, default=False\\n        Whether to use early stopping to terminate training when validation\\n        score is not improving. If set to true, it will automatically set\\n        aside 10% of training data as validation and terminate training when\\n        validation score is not improving by at least ``tol`` for\\n        ``n_iter_no_change`` consecutive epochs.\\n        Only effective when solver='sgd' or 'adam'\\n\\n- `validation_fraction`: float, default=0.1\\n        The proportion of training data to set aside as validation set for\\n        early stopping. Must be between 0 and 1.\\n        Only used if early_stopping is True\\n\\n- `beta_1`: float, default=0.9\\n        Exponential decay rate for estimates of first moment vector in adam,\\n        should be in [0, 1). Only used when solver='adam'\\n\\n- `beta_2`: float, default=0.999\\n        Exponential decay rate for estimates of second moment vector in adam,\\n        should be in [0, 1). Only used when solver='adam'\\n\\n- `epsilon`: float, default=1e-8\\n        Value for numerical stability in adam. Only used when solver='adam'\\n\\n- `n_iter_no_change`: int, default=10\\n        Maximum number of epochs to not meet ``tol`` improvement.\\n        Only effective when solver='sgd' or 'adam'\\n\\n        *Added in 0.20*\\n\\n- `max_fun`: int, default=15000\\n        Only used when solver='lbfgs'. Maximum number of function calls.\\n        The solver iterates until convergence (determined by 'tol'), number\\n        of iterations reaches max_iter, or this number of function calls.\\n        Note that number of function calls will be greater than or equal to\\n        the number of iterations for the MLPRegressor.\\n\\n        *Added in 0.22*\\n\\n    Attributes\\n    ----------\\n- `loss_`: float\\n        The current loss computed with the loss function.\\n\\n- `coefs_`: list, length n_layers - 1\\n        The ith element in the list represents the weight matrix corresponding\\n        to layer i.\\n\\n- `intercepts_`: list, length n_layers - 1\\n        The ith element in the list represents the bias vector corresponding to\\n        layer i + 1.\\n\\n- `n_iter_`: int,\\n        The number of iterations the solver has ran.\\n\\n- `n_layers_`: int\\n        Number of layers.\\n\\n- `n_outputs_`: int\\n        Number of outputs.\\n\\n- `out_activation_`: string\\n        Name of the output activation function.\\n\\n    Examples\\n    --------\\n    >>> from sklearn.neural_network import MLPRegressor\\n    >>> from sklearn.datasets import make_regression\\n    >>> from sklearn.model_selection import train_test_split\\n    >>> X, y = make_regression(n_samples=200, random_state=1)\\n    >>> X_train, X_test, y_train, y_test = train_test_split(X, y,\\n    ...                                                     random_state=1)\\n    >>> regr = MLPRegressor(random_state=1, max_iter=500).fit(X_train, y_train)\\n    >>> regr.predict(X_test[:2])\\n    array([-0.9..., -7.1...])\\n    >>> regr.score(X_test, y_test)\\n    0.4...\\n\\n    Notes\\n    -----\\n    MLPRegressor trains iteratively since at each time step\\n    the partial derivatives of the loss function with respect to the model\\n    parameters are computed to update the parameters.\\n\\n    It can also have a regularization term added to the loss function\\n    that shrinks model parameters to prevent overfitting.\\n\\n    This implementation works with data represented as dense and sparse numpy\\n    arrays of floating point values.\\n\\n    References\\n    ----------\\n    Hinton, Geoffrey E.\\n        \\\"Connectionist learning procedures.\\\" Artificial intelligence 40.1\\n        (1989): 185-234.\\n\\n    Glorot, Xavier, and Yoshua Bengio. \\\"Understanding the difficulty of\\n        training deep feedforward neural networks.\\\" International Conference\\n        on Artificial Intelligence and Statistics. 2010.\\n\\n    He, Kaiming, et al. \\\"Delving deep into rectifiers: Surpassing human-level\\n        performance on imagenet classification.\\\" arXiv preprint\\n        arXiv:1502.01852 (2015).\\n\\n    Kingma, Diederik, and Jimmy Ba. \\\"Adam: A method for stochastic\\n        optimization.\\\" arXiv preprint arXiv:1412.6980 (2014).\\n    \"]] [:hr] [:hr]] [:div [:h3 \":sklearn.regression/multi-task-elastic-net\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [10 2]:\\n\\n|          :name |  :default |\\n|----------------|-----------|\\n|     :normalize |     false |\\n|           :tol | 0.0001000 |\\n|      :max-iter |      1000 |\\n|  :random-state |           |\\n|        :copy-x |      true |\\n| :fit-intercept |      true |\\n|         :alpha |     1.000 |\\n|    :warm-start |     false |\\n|     :selection |    cyclic |\\n|     :l-1-ratio |    0.5000 |\\n\"]]] [:span [:p/markdown \"Multi-task ElasticNet model trained with L1/L2 mixed-norm as regularizer\\n\\n    The optimization objective for MultiTaskElasticNet is\\n\\n```python\\n(1 / (2 * n_samples)) * ||Y - XW||_Fro^2\\n+ alpha * l1_ratio * ||W||_21\\n+ 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\\n\\ne::\\n\\n||W||_21 = sum_i sqrt(sum_j W_ij ^ 2)\\n\\n the sum of norms of each row.\\n\\n more in the :ref:`User Guide <multi_task_elastic_net>`.\\n\\nmeters\\n------\\na : float, default=1.0\\nConstant that multiplies the L1/L2 term. Defaults to 1.0\\n\\natio : float, default=0.5\\nThe ElasticNet mixing parameter, with 0 < l1_ratio <= 1.\\nFor l1_ratio = 1 the penalty is an L1/L2 penalty. For l1_ratio = 0 it\\nis an L2 penalty.\\nFor ``0 < l1_ratio < 1``, the penalty is a combination of L1/L2 and L2.\\n\\nintercept : bool, default=True\\nwhether to calculate the intercept for this model. If set\\nto false, no intercept will be used in calculations\\n(i.e. data is expected to be centered).\\n\\nalize : bool, default=False\\nThis parameter is ignored when ``fit_intercept`` is set to False.\\nIf True, the regressors X will be normalized before regression by\\nsubtracting the mean and dividing by the l2-norm.\\nIf you wish to standardize, please use\\n:class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\\non an estimator with ``normalize=False``.\\n\\n_X : bool, default=True\\nIf ``True``, X will be copied; else, it may be overwritten.\\n\\niter : int, default=1000\\nThe maximum number of iterations\\n\\n: float, default=1e-4\\nThe tolerance for the optimization: if the updates are\\nsmaller than ``tol``, the optimization code checks the\\ndual gap for optimality and continues until it is smaller\\nthan ``tol``.\\n\\n_start : bool, default=False\\nWhen set to ``True``, reuse the solution of the previous call to fit as\\ninitialization, otherwise, just erase the previous solution.\\nSee :term:`the Glossary <warm_start>`.\\n\\nom_state : int, RandomState instance, default=None\\nThe seed of the pseudo random number generator that selects a random\\nfeature to update. Used when ``selection`` == 'random'.\\nPass an int for reproducible output across multiple function calls.\\nSee :term:`Glossary <random_state>`.\\n\\nction : {'cyclic', 'random'}, default='cyclic'\\nIf set to 'random', a random coefficient is updated every iteration\\nrather than looping over features sequentially by default. This\\n(setting to 'random') often leads to significantly faster convergence\\nespecially when tol is higher than 1e-4.\\n\\nibutes\\n------\\nrcept_ : ndarray of shape (n_tasks,)\\nIndependent term in decision function.\\n\\n_ : ndarray of shape (n_tasks, n_features)\\nParameter vector (W in the cost function formula). If a 1D y is\\npassed in at fit (non multi-task usage), ``coef_`` is then a 1D array.\\nNote that ``coef_`` stores the transpose of ``W``, ``W.T``.\\n\\ner_ : int\\nnumber of iterations run by the coordinate descent solver to reach\\nthe specified tolerance.\\n\\nples\\n----\\nfrom sklearn import linear_model\\nclf = linear_model.MultiTaskElasticNet(alpha=0.1)\\nclf.fit([[0,0], [1, 1], [2, 2]], [[0, 0], [1, 1], [2, 2]])\\niTaskElasticNet(alpha=0.1)\\nprint(clf.coef_)\\n45663524 0.45612256]\\n45663524 0.45612256]]\\nprint(clf.intercept_)\\n872422 0.0872422]\\n\\nalso\\n----\\niTaskElasticNet : Multi-task L1/L2 ElasticNet with built-in\\ncross-validation.\\nticNet\\niTaskLasso\\n\\ns\\n-\\nalgorithm used to fit the model is coordinate descent.\\n\\nvoid unnecessary memory duplication the X and y arguments of the fit\\nod should be directly passed as Fortran-contiguous numpy arrays.\\n```\\n\"]] [:hr] [:hr]] [:div [:h3 \":sklearn.regression/multi-task-elastic-net-cv\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [14 2]:\\n\\n|          :name |  :default |\\n|----------------|-----------|\\n|     :normalize |     false |\\n|           :tol | 0.0001000 |\\n|      :n-alphas |       100 |\\n|           :eps |  0.001000 |\\n|        :alphas |           |\\n|      :max-iter |      1000 |\\n|        :n-jobs |           |\\n|  :random-state |           |\\n|        :copy-x |      true |\\n| :fit-intercept |      true |\\n|            :cv |           |\\n|     :selection |    cyclic |\\n|     :l-1-ratio |    0.5000 |\\n|       :verbose |         0 |\\n\"]]] [:span [:p/markdown \"Multi-task L1/L2 ElasticNet with built-in cross-validation.\\n\\n    See glossary entry for :term:`cross-validation estimator`.\\n\\n    The optimization objective for MultiTaskElasticNet is\\n\\n```python\\n(1 / (2 * n_samples)) * ||Y - XW||^Fro_2\\n+ alpha * l1_ratio * ||W||_21\\n+ 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\\n\\ne::\\n\\n||W||_21 = \\\\sum_i \\\\sqrt{\\\\sum_j w_{ij}^2}\\n\\n the sum of norm of each row.\\n\\n more in the :ref:`User Guide <multi_task_elastic_net>`.\\n\\nersionadded:: 0.15\\n\\nmeters\\n------\\natio : float or list of float, default=0.5\\nThe ElasticNet mixing parameter, with 0 < l1_ratio <= 1.\\nFor l1_ratio = 1 the penalty is an L1/L2 penalty. For l1_ratio = 0 it\\nis an L2 penalty.\\nFor ``0 < l1_ratio < 1``, the penalty is a combination of L1/L2 and L2.\\nThis parameter can be a list, in which case the different\\nvalues are tested by cross-validation and the one giving the best\\nprediction score is used. Note that a good choice of list of\\nvalues for l1_ratio is often to put more values close to 1\\n(i.e. Lasso) and less close to 0 (i.e. Ridge), as in ``[.1, .5, .7,\\n.9, .95, .99, 1]``\\n\\n: float, default=1e-3\\nLength of the path. ``eps=1e-3`` means that\\n``alpha_min / alpha_max = 1e-3``.\\n\\nphas : int, default=100\\nNumber of alphas along the regularization path\\n\\nas : array-like, default=None\\nList of alphas where to compute the models.\\nIf not provided, set automatically.\\n\\nintercept : bool, default=True\\nwhether to calculate the intercept for this model. If set\\nto false, no intercept will be used in calculations\\n(i.e. data is expected to be centered).\\n\\nalize : bool, default=False\\nThis parameter is ignored when ``fit_intercept`` is set to False.\\nIf True, the regressors X will be normalized before regression by\\nsubtracting the mean and dividing by the l2-norm.\\nIf you wish to standardize, please use\\n:class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\\non an estimator with ``normalize=False``.\\n\\niter : int, default=1000\\nThe maximum number of iterations\\n\\n: float, default=1e-4\\nThe tolerance for the optimization: if the updates are\\nsmaller than ``tol``, the optimization code checks the\\ndual gap for optimality and continues until it is smaller\\nthan ``tol``.\\n\\n int, cross-validation generator or iterable, default=None\\nDetermines the cross-validation splitting strategy.\\nPossible inputs for cv are:\\n\\n- None, to use the default 5-fold cross-validation,\\n- int, to specify the number of folds.\\n- :term:`CV splitter`,\\n- An iterable yielding (train, test) splits as arrays of indices.\\n\\nFor int/None inputs, :class:`KFold` is used.\\n\\nRefer :ref:`User Guide <cross_validation>` for the various\\ncross-validation strategies that can be used here.\\n\\n.. versionchanged:: 0.22\\n    ``cv`` default value if None changed from 3-fold to 5-fold.\\n\\n_X : bool, default=True\\nIf ``True``, X will be copied; else, it may be overwritten.\\n\\nose : bool or int, default=0\\nAmount of verbosity.\\n\\nbs : int, default=None\\nNumber of CPUs to use during the cross validation. Note that this is\\nused only if multiple values for l1_ratio are given.\\n``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\\n``-1`` means using all processors. See :term:`Glossary <n_jobs>`\\nfor more details.\\n\\nom_state : int, RandomState instance, default=None\\nThe seed of the pseudo random number generator that selects a random\\nfeature to update. Used when ``selection`` == 'random'.\\nPass an int for reproducible output across multiple function calls.\\nSee :term:`Glossary <random_state>`.\\n\\nction : {'cyclic', 'random'}, default='cyclic'\\nIf set to 'random', a random coefficient is updated every iteration\\nrather than looping over features sequentially by default. This\\n(setting to 'random') often leads to significantly faster convergence\\nespecially when tol is higher than 1e-4.\\n\\nibutes\\n------\\nrcept_ : ndarray of shape (n_tasks,)\\nIndependent term in decision function.\\n\\n_ : ndarray of shape (n_tasks, n_features)\\nParameter vector (W in the cost function formula).\\nNote that ``coef_`` stores the transpose of ``W``, ``W.T``.\\n\\na_ : float\\nThe amount of penalization chosen by cross validation\\n\\npath_ : ndarray of shape (n_alphas, n_folds) or                 (n_l1_ratio, n_alphas, n_folds)\\nmean square error for the test set on each fold, varying alpha\\n\\nas_ : ndarray of shape (n_alphas,) or (n_l1_ratio, n_alphas)\\nThe grid of alphas used for fitting, for each l1_ratio\\n\\natio_ : float\\nbest l1_ratio obtained by cross-validation.\\n\\ner_ : int\\nnumber of iterations run by the coordinate descent solver to reach\\nthe specified tolerance for the optimal alpha.\\n\\nples\\n----\\nfrom sklearn import linear_model\\nclf = linear_model.MultiTaskElasticNetCV(cv=3)\\nclf.fit([[0,0], [1, 1], [2, 2]],\\n        [[0, 0], [1, 1], [2, 2]])\\niTaskElasticNetCV(cv=3)\\nprint(clf.coef_)\\n52875032 0.46958558]\\n52875032 0.46958558]]\\nprint(clf.intercept_)\\n0166409 0.00166409]\\n\\nalso\\n----\\niTaskElasticNet\\nticNetCV\\niTaskLassoCV\\n\\ns\\n-\\nalgorithm used to fit the model is coordinate descent.\\n\\nvoid unnecessary memory duplication the X and y arguments of the fit\\nod should be directly passed as Fortran-contiguous numpy arrays.\\n```\\n\"]] [:hr] [:hr]] [:div [:h3 \":sklearn.regression/multi-task-lasso\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [9 2]:\\n\\n|          :name |  :default |\\n|----------------|-----------|\\n|     :normalize |     false |\\n|           :tol | 0.0001000 |\\n|      :max-iter |      1000 |\\n|  :random-state |           |\\n|        :copy-x |      true |\\n| :fit-intercept |      true |\\n|         :alpha |     1.000 |\\n|    :warm-start |     false |\\n|     :selection |    cyclic |\\n\"]]] [:span [:p/markdown \"Multi-task Lasso model trained with L1/L2 mixed-norm as regularizer.\\n\\n    The optimization objective for Lasso is\\n\\n```python\\n(1 / (2 * n_samples)) * ||Y - XW||^2_Fro + alpha * ||W||_21\\n\\ne::\\n\\n||W||_21 = \\\\sum_i \\\\sqrt{\\\\sum_j w_{ij}^2}\\n\\n the sum of norm of each row.\\n\\n more in the :ref:`User Guide <multi_task_lasso>`.\\n\\nmeters\\n------\\na : float, default=1.0\\nConstant that multiplies the L1/L2 term. Defaults to 1.0\\n\\nintercept : bool, default=True\\nwhether to calculate the intercept for this model. If set\\nto false, no intercept will be used in calculations\\n(i.e. data is expected to be centered).\\n\\nalize : bool, default=False\\nThis parameter is ignored when ``fit_intercept`` is set to False.\\nIf True, the regressors X will be normalized before regression by\\nsubtracting the mean and dividing by the l2-norm.\\nIf you wish to standardize, please use\\n:class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\\non an estimator with ``normalize=False``.\\n\\n_X : bool, default=True\\nIf ``True``, X will be copied; else, it may be overwritten.\\n\\niter : int, default=1000\\nThe maximum number of iterations\\n\\n: float, default=1e-4\\nThe tolerance for the optimization: if the updates are\\nsmaller than ``tol``, the optimization code checks the\\ndual gap for optimality and continues until it is smaller\\nthan ``tol``.\\n\\n_start : bool, default=False\\nWhen set to ``True``, reuse the solution of the previous call to fit as\\ninitialization, otherwise, just erase the previous solution.\\nSee :term:`the Glossary <warm_start>`.\\n\\nom_state : int, RandomState instance, default=None\\nThe seed of the pseudo random number generator that selects a random\\nfeature to update. Used when ``selection`` == 'random'.\\nPass an int for reproducible output across multiple function calls.\\nSee :term:`Glossary <random_state>`.\\n\\nction : {'cyclic', 'random'}, default='cyclic'\\nIf set to 'random', a random coefficient is updated every iteration\\nrather than looping over features sequentially by default. This\\n(setting to 'random') often leads to significantly faster convergence\\nespecially when tol is higher than 1e-4\\n\\nibutes\\n------\\n_ : ndarray of shape (n_tasks, n_features)\\nParameter vector (W in the cost function formula).\\nNote that ``coef_`` stores the transpose of ``W``, ``W.T``.\\n\\nrcept_ : ndarray of shape (n_tasks,)\\nindependent term in decision function.\\n\\ner_ : int\\nnumber of iterations run by the coordinate descent solver to reach\\nthe specified tolerance.\\n\\nples\\n----\\nfrom sklearn import linear_model\\nclf = linear_model.MultiTaskLasso(alpha=0.1)\\nclf.fit([[0, 1], [1, 2], [2, 4]], [[0, 0], [1, 1], [2, 3]])\\niTaskLasso(alpha=0.1)\\nprint(clf.coef_)\\n         0.60809415]\\n        0.94592424]]\\nprint(clf.intercept_)\\n41888636 -0.87382323]\\n\\nalso\\n----\\niTaskLasso : Multi-task L1/L2 Lasso with built-in cross-validation\\no\\niTaskElasticNet\\n\\ns\\n-\\nalgorithm used to fit the model is coordinate descent.\\n\\nvoid unnecessary memory duplication the X and y arguments of the fit\\nod should be directly passed as Fortran-contiguous numpy arrays.\\n```\\n\"]] [:hr] [:hr]] [:div [:h3 \":sklearn.regression/multi-task-lasso-cv\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [13 2]:\\n\\n|          :name |  :default |\\n|----------------|-----------|\\n|     :normalize |     false |\\n|           :tol | 0.0001000 |\\n|      :n-alphas |       100 |\\n|           :eps |  0.001000 |\\n|        :alphas |           |\\n|      :max-iter |      1000 |\\n|        :n-jobs |           |\\n|  :random-state |           |\\n|        :copy-x |      true |\\n| :fit-intercept |      true |\\n|            :cv |           |\\n|     :selection |    cyclic |\\n|       :verbose |     false |\\n\"]]] [:span [:p/markdown \"Multi-task Lasso model trained with L1/L2 mixed-norm as regularizer.\\n\\n    See glossary entry for :term:`cross-validation estimator`.\\n\\n    The optimization objective for MultiTaskLasso is\\n\\n```python\\n(1 / (2 * n_samples)) * ||Y - XW||^Fro_2 + alpha * ||W||_21\\n\\ne::\\n\\n||W||_21 = \\\\sum_i \\\\sqrt{\\\\sum_j w_{ij}^2}\\n\\n the sum of norm of each row.\\n\\n more in the :ref:`User Guide <multi_task_lasso>`.\\n\\nersionadded:: 0.15\\n\\nmeters\\n------\\n: float, default=1e-3\\nLength of the path. ``eps=1e-3`` means that\\n``alpha_min / alpha_max = 1e-3``.\\n\\nphas : int, default=100\\nNumber of alphas along the regularization path\\n\\nas : array-like, default=None\\nList of alphas where to compute the models.\\nIf not provided, set automatically.\\n\\nintercept : bool, default=True\\nwhether to calculate the intercept for this model. If set\\nto false, no intercept will be used in calculations\\n(i.e. data is expected to be centered).\\n\\nalize : bool, default=False\\nThis parameter is ignored when ``fit_intercept`` is set to False.\\nIf True, the regressors X will be normalized before regression by\\nsubtracting the mean and dividing by the l2-norm.\\nIf you wish to standardize, please use\\n:class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\\non an estimator with ``normalize=False``.\\n\\niter : int, default=1000\\nThe maximum number of iterations.\\n\\n: float, default=1e-4\\nThe tolerance for the optimization: if the updates are\\nsmaller than ``tol``, the optimization code checks the\\ndual gap for optimality and continues until it is smaller\\nthan ``tol``.\\n\\n_X : bool, default=True\\nIf ``True``, X will be copied; else, it may be overwritten.\\n\\n int, cross-validation generator or iterable, default=None\\nDetermines the cross-validation splitting strategy.\\nPossible inputs for cv are:\\n\\n- None, to use the default 5-fold cross-validation,\\n- int, to specify the number of folds.\\n- :term:`CV splitter`,\\n- An iterable yielding (train, test) splits as arrays of indices.\\n\\nFor int/None inputs, :class:`KFold` is used.\\n\\nRefer :ref:`User Guide <cross_validation>` for the various\\ncross-validation strategies that can be used here.\\n\\n.. versionchanged:: 0.22\\n    ``cv`` default value if None changed from 3-fold to 5-fold.\\n\\nose : bool or int, default=False\\nAmount of verbosity.\\n\\nbs : int, default=None\\nNumber of CPUs to use during the cross validation. Note that this is\\nused only if multiple values for l1_ratio are given.\\n``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\\n``-1`` means using all processors. See :term:`Glossary <n_jobs>`\\nfor more details.\\n\\nom_state : int, RandomState instance, default=None\\nThe seed of the pseudo random number generator that selects a random\\nfeature to update. Used when ``selection`` == 'random'.\\nPass an int for reproducible output across multiple function calls.\\nSee :term:`Glossary <random_state>`.\\n\\nction : {'cyclic', 'random'}, default='cyclic'\\nIf set to 'random', a random coefficient is updated every iteration\\nrather than looping over features sequentially by default. This\\n(setting to 'random') often leads to significantly faster convergence\\nespecially when tol is higher than 1e-4.\\n\\nibutes\\n------\\nrcept_ : ndarray of shape (n_tasks,)\\nIndependent term in decision function.\\n\\n_ : ndarray of shape (n_tasks, n_features)\\nParameter vector (W in the cost function formula).\\nNote that ``coef_`` stores the transpose of ``W``, ``W.T``.\\n\\na_ : float\\nThe amount of penalization chosen by cross validation\\n\\npath_ : ndarray of shape (n_alphas, n_folds)\\nmean square error for the test set on each fold, varying alpha\\n\\nas_ : ndarray of shape (n_alphas,)\\nThe grid of alphas used for fitting.\\n\\ner_ : int\\nnumber of iterations run by the coordinate descent solver to reach\\nthe specified tolerance for the optimal alpha.\\n\\nples\\n----\\nfrom sklearn.linear_model import MultiTaskLassoCV\\nfrom sklearn.datasets import make_regression\\nfrom sklearn.metrics import r2_score\\nX, y = make_regression(n_targets=2, noise=4, random_state=0)\\nreg = MultiTaskLassoCV(cv=5, random_state=0).fit(X, y)\\nr2_score(y, reg.predict(X))\\n94...\\nreg.alpha_\\n13...\\nreg.predict(X[:1,])\\ny([[153.7971...,  94.9015...]])\\n\\nalso\\n----\\niTaskElasticNet\\nticNetCV\\niTaskElasticNetCV\\n\\ns\\n-\\nalgorithm used to fit the model is coordinate descent.\\n\\nvoid unnecessary memory duplication the X and y arguments of the fit\\nod should be directly passed as Fortran-contiguous numpy arrays.\\n```\\n\"]] [:hr] [:hr]] [:div [:h3 \":sklearn.regression/nu-svr\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [11 2]:\\n\\n|       :name | :default |\\n|-------------|----------|\\n|     :kernel |      rbf |\\n|      :gamma |    scale |\\n|     :degree |        3 |\\n|        :tol | 0.001000 |\\n|         :nu |   0.5000 |\\n|  :shrinking |     true |\\n|          :c |    1.000 |\\n|   :max-iter |       -1 |\\n|     :coef-0 |    0.000 |\\n| :cache-size |      200 |\\n|    :verbose |    false |\\n\"]]] [:span [:p/markdown \"Nu Support Vector Regression.\\n\\n    Similar to NuSVC, for regression, uses a parameter nu to control\\n    the number of support vectors. However, unlike NuSVC, where nu\\n    replaces C, here nu replaces the parameter epsilon of epsilon-SVR.\\n\\n    The implementation is based on libsvm.\\n\\n    Read more in the User Guide: `svm_regression`.\\n\\n    Parameters\\n    ----------\\n- `nu`: float, default=0.5\\n        An upper bound on the fraction of training errors and a lower bound of\\n        the fraction of support vectors. Should be in the interval (0, 1].  By\\n        default 0.5 will be taken.\\n\\n- `C`: float, default=1.0\\n        Penalty parameter C of the error term.\\n\\n- `kernel`: {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'}, default='rbf'\\n         Specifies the kernel type to be used in the algorithm.\\n         It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or\\n         a callable.\\n         If none is given, 'rbf' will be used. If a callable is given it is\\n         used to precompute the kernel matrix.\\n\\n- `degree`: int, default=3\\n        Degree of the polynomial kernel function ('poly').\\n        Ignored by all other kernels.\\n\\n- `gamma`: {'scale', 'auto'} or float, default='scale'\\n        Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.\\n\\n        - if ``gamma='scale'`` (default) is passed then it uses\\n          1 / (n_features * X.var()) as value of gamma,\\n        - if 'auto', uses 1 / n_features.\\n\\n        *Changed in 0.22*\\n           The default value of ``gamma`` changed from 'auto' to 'scale'.\\n\\n- `coef0`: float, default=0.0\\n        Independent term in kernel function.\\n        It is only significant in 'poly' and 'sigmoid'.\\n\\n- `shrinking`: bool, default=True\\n        Whether to use the shrinking heuristic.\\n        See the User Guide: `shrinking_svm`.\\n\\n- `tol`: float, default=1e-3\\n        Tolerance for stopping criterion.\\n\\n- `cache_size`: float, default=200\\n        Specify the size of the kernel cache (in MB).\\n\\n- `verbose`: bool, default=False\\n        Enable verbose output. Note that this setting takes advantage of a\\n        per-process runtime setting in libsvm that, if enabled, may not work\\n        properly in a multithreaded context.\\n\\n- `max_iter`: int, default=-1\\n        Hard limit on iterations within solver, or -1 for no limit.\\n\\n    Attributes\\n    ----------\\n- `support_`: ndarray of shape (n_SV,)\\n        Indices of support vectors.\\n\\n- `support_vectors_`: ndarray of shape (n_SV, n_features)\\n        Support vectors.\\n\\n- `dual_coef_`: ndarray of shape (1, n_SV)\\n        Coefficients of the support vector in the decision function.\\n\\n- `coef_`: ndarray of shape (1, n_features)\\n        Weights assigned to the features (coefficients in the primal\\n        problem). This is only available in the case of a linear kernel.\\n\\n        `coef_` is readonly property derived from `dual_coef_` and\\n        `support_vectors_`.\\n\\n- `intercept_`: ndarray of shape (1,)\\n        Constants in decision function.\\n\\n    Examples\\n    --------\\n    >>> from sklearn.svm import NuSVR\\n    >>> from sklearn.pipeline import make_pipeline\\n    >>> from sklearn.preprocessing import StandardScaler\\n    >>> import numpy as np\\n    >>> n_samples, n_features = 10, 5\\n    >>> np.random.seed(0)\\n    >>> y = np.random.randn(n_samples)\\n    >>> X = np.random.randn(n_samples, n_features)\\n    >>> regr = make_pipeline(StandardScaler(), NuSVR(C=1.0, nu=0.1))\\n    >>> regr.fit(X, y)\\n    Pipeline(steps=[('standardscaler', StandardScaler()),\\n                    ('nusvr', NuSVR(nu=0.1))])\\n\\n    See also\\n    --------\\n    NuSVC\\n        Support Vector Machine for classification implemented with libsvm\\n        with a parameter to control the number of support vectors.\\n\\n    SVR\\n        epsilon Support Vector Machine for regression implemented with libsvm.\\n\\n    Notes\\n    -----\\n    **References:**\\n    [LIBSVM: A Library for Support Vector Machines\\n    ](http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf)\\n    \"]] [:hr] [:hr]] [:div [:h3 \":sklearn.regression/orthogonal-matching-pursuit\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [5 2]:\\n\\n|            :name | :default |\\n|------------------|----------|\\n|   :fit-intercept |     true |\\n| :n-nonzero-coefs |          |\\n|       :normalize |     true |\\n|      :precompute |     auto |\\n|             :tol |          |\\n\"]]] [:span [:p/markdown \"Orthogonal Matching Pursuit model (OMP)\\n\\n    Read more in the User Guide: `omp`.\\n\\n    Parameters\\n    ----------\\n- `n_nonzero_coefs`: int, optional\\n        Desired number of non-zero entries in the solution. If None (by\\n        default) this value is set to 10% of n_features.\\n\\n- `tol`: float, optional\\n        Maximum norm of the residual. If not None, overrides n_nonzero_coefs.\\n\\n- `fit_intercept`: boolean, optional\\n        whether to calculate the intercept for this model. If set\\n        to false, no intercept will be used in calculations\\n        (i.e. data is expected to be centered).\\n\\n- `normalize`: boolean, optional, default True\\n        This parameter is ignored when ``fit_intercept`` is set to False.\\n        If True, the regressors X will be normalized before regression by\\n        subtracting the mean and dividing by the l2-norm.\\n        If you wish to standardize, please use\\n        `sklearn.preprocessing.StandardScaler` before calling ``fit``\\n        on an estimator with ``normalize=False``.\\n\\n- `precompute`: {True, False, 'auto'}, default 'auto'\\n        Whether to use a precomputed Gram and Xy matrix to speed up\\n        calculations. Improves performance when :term:`n_targets` or\\n        :term:`n_samples` is very large. Note that if you already have such\\n        matrices, you can pass them directly to the fit method.\\n\\n    Attributes\\n    ----------\\n- `coef_`: array, shape (n_features,) or (n_targets, n_features)\\n        parameter vector (w in the formula)\\n\\n- `intercept_`: float or array, shape (n_targets,)\\n        independent term in decision function.\\n\\n- `n_iter_`: int or array-like\\n        Number of active features across every target.\\n\\n    Examples\\n    --------\\n    >>> from sklearn.linear_model import OrthogonalMatchingPursuit\\n    >>> from sklearn.datasets import make_regression\\n    >>> X, y = make_regression(noise=4, random_state=0)\\n    >>> reg = OrthogonalMatchingPursuit().fit(X, y)\\n    >>> reg.score(X, y)\\n    0.9991...\\n    >>> reg.predict(X[:1,])\\n    array([-78.3854...])\\n\\n    Notes\\n    -----\\n    Orthogonal matching pursuit was introduced in G. Mallat, Z. Zhang,\\n    Matching pursuits with time-frequency dictionaries, IEEE Transactions on\\n    Signal Processing, Vol. 41, No. 12. (December 1993), pp. 3397-3415.\\n    (http://blanche.polytechnique.fr/~mallat/papiers/MallatPursuit93.pdf)\\n\\n    This implementation is based on Rubinstein, R., Zibulevsky, M. and Elad,\\n    M., Efficient Implementation of the K-SVD Algorithm using Batch Orthogonal\\n    Matching Pursuit Technical Report - CS Technion, April 2008.\\n    https://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf\\n\\n    See also\\n    --------\\n    orthogonal_mp\\n    orthogonal_mp_gram\\n    lars_path\\n    Lars\\n    LassoLars\\n    decomposition.sparse_encode\\n    OrthogonalMatchingPursuitCV\\n    \"]] [:hr] [:hr]] [:div [:h3 \":sklearn.regression/orthogonal-matching-pursuit-cv\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [7 2]:\\n\\n|          :name | :default |\\n|----------------|----------|\\n|          :copy |     true |\\n|            :cv |          |\\n| :fit-intercept |     true |\\n|      :max-iter |          |\\n|        :n-jobs |          |\\n|     :normalize |     true |\\n|       :verbose |    false |\\n\"]]] [:span [:p/markdown \"Cross-validated Orthogonal Matching Pursuit model (OMP).\\n\\n    See glossary entry for :term:`cross-validation estimator`.\\n\\n    Read more in the User Guide: `omp`.\\n\\n    Parameters\\n    ----------\\n- `copy`: bool, optional\\n        Whether the design matrix X must be copied by the algorithm. A false\\n        value is only helpful if X is already Fortran-ordered, otherwise a\\n        copy is made anyway.\\n\\n- `fit_intercept`: boolean, optional\\n        whether to calculate the intercept for this model. If set\\n        to false, no intercept will be used in calculations\\n        (i.e. data is expected to be centered).\\n\\n- `normalize`: boolean, optional, default True\\n        This parameter is ignored when ``fit_intercept`` is set to False.\\n        If True, the regressors X will be normalized before regression by\\n        subtracting the mean and dividing by the l2-norm.\\n        If you wish to standardize, please use\\n        `sklearn.preprocessing.StandardScaler` before calling ``fit``\\n        on an estimator with ``normalize=False``.\\n\\n- `max_iter`: integer, optional\\n        Maximum numbers of iterations to perform, therefore maximum features\\n        to include. 10% of ``n_features`` but at least 5 if available.\\n\\n- `cv`: int, cross-validation generator or an iterable, optional\\n        Determines the cross-validation splitting strategy.\\n        Possible inputs for cv are:\\n\\n        - None, to use the default 5-fold cross-validation,\\n        - integer, to specify the number of folds.\\n        - :term:`CV splitter`,\\n        - An iterable yielding (train, test) splits as arrays of indices.\\n\\n        For integer/None inputs, `KFold` is used.\\n\\n        Refer User Guide: `cross_validation` for the various\\n        cross-validation strategies that can be used here.\\n\\n        *Changed in 0.22*\\n            ``cv`` default value if None changed from 3-fold to 5-fold.\\n\\n- `n_jobs`: int or None, optional (default=None)\\n        Number of CPUs to use during the cross validation.\\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\\n        for more details.\\n\\n- `verbose`: boolean or integer, optional\\n        Sets the verbosity amount\\n\\n    Attributes\\n    ----------\\n- `intercept_`: float or array, shape (n_targets,)\\n        Independent term in decision function.\\n\\n- `coef_`: array, shape (n_features,) or (n_targets, n_features)\\n        Parameter vector (w in the problem formulation).\\n\\n- `n_nonzero_coefs_`: int\\n        Estimated number of non-zero coefficients giving the best mean squared\\n        error over the cross-validation folds.\\n\\n- `n_iter_`: int or array-like\\n        Number of active features across every target for the model refit with\\n        the best hyperparameters got by cross-validating across all folds.\\n\\n    Examples\\n    --------\\n    >>> from sklearn.linear_model import OrthogonalMatchingPursuitCV\\n    >>> from sklearn.datasets import make_regression\\n    >>> X, y = make_regression(n_features=100, n_informative=10,\\n    ...                        noise=4, random_state=0)\\n    >>> reg = OrthogonalMatchingPursuitCV(cv=5).fit(X, y)\\n    >>> reg.score(X, y)\\n    0.9991...\\n    >>> reg.n_nonzero_coefs_\\n    10\\n    >>> reg.predict(X[:1,])\\n    array([-78.3854...])\\n\\n    See also\\n    --------\\n    orthogonal_mp\\n    orthogonal_mp_gram\\n    lars_path\\n    Lars\\n    LassoLars\\n    OrthogonalMatchingPursuit\\n    LarsCV\\n    LassoLarsCV\\n    decomposition.sparse_encode\\n\\n    \"]] [:hr] [:hr]] [:div [:h3 \":sklearn.regression/passive-aggressive-regressor\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [14 2]:\\n\\n|                :name |            :default |\\n|----------------------|---------------------|\\n|    :n-iter-no-change |                   5 |\\n|             :average |               false |\\n|                 :tol |            0.001000 |\\n|      :early-stopping |               false |\\n|             :shuffle |                true |\\n|                   :c |               1.000 |\\n|            :max-iter |                1000 |\\n|        :random-state |                     |\\n|       :fit-intercept |                true |\\n|          :warm-start |               false |\\n| :validation-fraction |              0.1000 |\\n|                :loss | epsilon_insensitive |\\n|             :verbose |                   0 |\\n|             :epsilon |              0.1000 |\\n\"]]] [:span [:p/markdown \"Passive Aggressive Regressor\\n\\n    Read more in the User Guide: `passive_aggressive`.\\n\\n    Parameters\\n    ----------\\n\\n- `C`: float\\n        Maximum step size (regularization). Defaults to 1.0.\\n\\n- `fit_intercept`: bool\\n        Whether the intercept should be estimated or not. If False, the\\n        data is assumed to be already centered. Defaults to True.\\n\\n- `max_iter`: int, optional (default=1000)\\n        The maximum number of passes over the training data (aka epochs).\\n        It only impacts the behavior in the ``fit`` method, and not the\\n        `partial_fit` method.\\n\\n        *Added in 0.19*\\n\\n- `tol`: float or None, optional (default=1e-3)\\n        The stopping criterion. If it is not None, the iterations will stop\\n        when (loss > previous_loss - tol).\\n\\n        *Added in 0.19*\\n\\n- `early_stopping`: bool, default=False\\n        Whether to use early stopping to terminate training when validation.\\n        score is not improving. If set to True, it will automatically set aside\\n        a fraction of training data as validation and terminate\\n        training when validation score is not improving by at least tol for\\n        n_iter_no_change consecutive epochs.\\n\\n        *Added in 0.20*\\n\\n- `validation_fraction`: float, default=0.1\\n        The proportion of training data to set aside as validation set for\\n        early stopping. Must be between 0 and 1.\\n        Only used if early_stopping is True.\\n\\n        *Added in 0.20*\\n\\n- `n_iter_no_change`: int, default=5\\n        Number of iterations with no improvement to wait before early stopping.\\n\\n        *Added in 0.20*\\n\\n- `shuffle`: bool, default=True\\n        Whether or not the training data should be shuffled after each epoch.\\n\\n- `verbose`: integer, optional\\n        The verbosity level\\n\\n- `loss`: string, optional\\n        The loss function to be used:\\n        epsilon_insensitive: equivalent to PA-I in the reference paper.\\n        squared_epsilon_insensitive: equivalent to PA-II in the reference\\n        paper.\\n\\n- `epsilon`: float\\n        If the difference between the current prediction and the correct label\\n        is below this threshold, the model is not updated.\\n\\n- `random_state`: int, RandomState instance, default=None\\n        Used to shuffle the training data, when ``shuffle`` is set to\\n        ``True``. Pass an int for reproducible output across multiple\\n        function calls.\\n        See :term:`Glossary <random_state>`.\\n\\n- `warm_start`: bool, optional\\n        When set to True, reuse the solution of the previous call to fit as\\n        initialization, otherwise, just erase the previous solution.\\n        See :term:`the Glossary <warm_start>`.\\n\\n        Repeatedly calling fit or partial_fit when warm_start is True can\\n        result in a different solution than when calling fit a single time\\n        because of the way the data is shuffled.\\n\\n- `average`: bool or int, optional\\n        When set to True, computes the averaged SGD weights and stores the\\n        result in the ``coef_`` attribute. If set to an int greater than 1,\\n        averaging will begin once the total number of samples seen reaches\\n        average. So average=10 will begin averaging after seeing 10 samples.\\n\\n        *Added in 0.19*\\n           parameter *average* to use weights averaging in SGD\\n\\n    Attributes\\n    ----------\\n- `coef_`: array, shape = [1, n_features] if n_classes == 2 else [n_classes,            n_features]\\n        Weights assigned to the features.\\n\\n- `intercept_`: array, shape = [1] if n_classes == 2 else [n_classes]\\n        Constants in decision function.\\n\\n- `n_iter_`: int\\n        The actual number of iterations to reach the stopping criterion.\\n\\n- `t_`: int\\n        Number of weight updates performed during training.\\n        Same as ``(n_iter_ * n_samples)``.\\n\\n    Examples\\n    --------\\n    >>> from sklearn.linear_model import PassiveAggressiveRegressor\\n    >>> from sklearn.datasets import make_regression\\n\\n    >>> X, y = make_regression(n_features=4, random_state=0)\\n    >>> regr = PassiveAggressiveRegressor(max_iter=100, random_state=0,\\n    ... tol=1e-3)\\n    >>> regr.fit(X, y)\\n    PassiveAggressiveRegressor(max_iter=100, random_state=0)\\n    >>> print(regr.coef_)\\n    [20.48736655 34.18818427 67.59122734 87.94731329]\\n    >>> print(regr.intercept_)\\n    [-0.02306214]\\n    >>> print(regr.predict([[0, 0, 0, 0]]))\\n    [-0.02306214]\\n\\n    See also\\n    --------\\n\\n    SGDRegressor\\n\\n    References\\n    ----------\\n    Online Passive-Aggressive Algorithms\\n    <http://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf>\\n    K. Crammer, O. Dekel, J. Keshat, S. Shalev-Shwartz, Y. Singer - JMLR (2006)\\n\\n    \"]] [:hr] [:hr]] [:div [:h3 \":sklearn.regression/pls-canonical\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [6 2]:\\n\\n|         :name |  :default |\\n|---------------|-----------|\\n|    :algorithm |    nipals |\\n|         :copy |      true |\\n|     :max-iter |       500 |\\n| :n-components |         2 |\\n|        :scale |      true |\\n|          :tol | 1.000E-06 |\\n\"]]] [:span [:p/markdown \" PLSCanonical implements the 2 blocks canonical PLS of the original Wold\\n    algorithm [Tenenhaus 1998] p.204, referred as PLS-C2A in [Wegelin 2000].\\n\\n    This class inherits from PLS with mode=\\\"A\\\" and deflation_mode=\\\"canonical\\\",\\n    norm_y_weights=True and algorithm=\\\"nipals\\\", but svd should provide similar\\n    results up to numerical errors.\\n\\n    Read more in the User Guide: `cross_decomposition`.\\n\\n    *Added in 0.8*\\n\\n    Parameters\\n    ----------\\n- `n_components`: int, (default 2).\\n        Number of components to keep\\n\\n- `scale`: boolean, (default True)\\n        Option to scale data\\n\\n- `algorithm`: string, \\\"nipals\\\" or \\\"svd\\\"\\n        The algorithm used to estimate the weights. It will be called\\n        n_components times, i.e. once for each iteration of the outer loop.\\n\\n- `max_iter`: an integer, (default 500)\\n        the maximum number of iterations of the NIPALS inner loop (used\\n        only if algorithm=\\\"nipals\\\")\\n\\n- `tol`: non-negative real, default 1e-06\\n        the tolerance used in the iterative algorithm\\n\\n- `copy`: boolean, default True\\n        Whether the deflation should be done on a copy. Let the default\\n        value to True unless you don't care about side effect\\n\\n    Attributes\\n    ----------\\n- `x_weights_`: array, shape = [p, n_components]\\n        X block weights vectors.\\n\\n- `y_weights_`: array, shape = [q, n_components]\\n        Y block weights vectors.\\n\\n- `x_loadings_`: array, shape = [p, n_components]\\n        X block loadings vectors.\\n\\n- `y_loadings_`: array, shape = [q, n_components]\\n        Y block loadings vectors.\\n\\n- `x_scores_`: array, shape = [n_samples, n_components]\\n        X scores.\\n\\n- `y_scores_`: array, shape = [n_samples, n_components]\\n        Y scores.\\n\\n- `x_rotations_`: array, shape = [p, n_components]\\n        X block to latents rotations.\\n\\n- `y_rotations_`: array, shape = [q, n_components]\\n        Y block to latents rotations.\\n\\n- `coef_`: array of shape (p, q)\\n        The coefficients of the linear model: ``Y = X coef_ + Err``\\n\\n- `n_iter_`: array-like\\n        Number of iterations of the NIPALS inner loop for each\\n        component. Not useful if the algorithm provided is \\\"svd\\\".\\n\\n    Notes\\n    -----\\n    Matrices\\n\\n```python\\nT: x_scores_\\nU: y_scores_\\nW: x_weights_\\nC: y_weights_\\nP: x_loadings_\\nQ: y_loadings__\\n\\ncomputed such that::\\n\\nX = T P.T + Err and Y = U Q.T + Err\\nT[:, k] = Xk W[:, k] for k in range(n_components)\\nU[:, k] = Yk C[:, k] for k in range(n_components)\\nx_rotations_ = W (P.T W)^(-1)\\ny_rotations_ = C (Q.T C)^(-1)\\n\\ne Xk and Yk are residual matrices at iteration k.\\n\\ndes explaining PLS\\np://www.eigenvector.com/Docs/Wise_pls_properties.pdf>`_\\n\\neach component k, find weights u, v that optimize::\\n\\nmax corr(Xk u, Yk v) * std(Xk u) std(Yk u), such that ``|u| = |v| = 1``\\n\\n that it maximizes both the correlations between the scores and the\\na-block variances.\\n\\nresidual matrix of X (Xk+1) block is obtained by the deflation on the\\nent X score: x_score.\\n\\nresidual matrix of Y (Yk+1) block is obtained by deflation on the\\nent Y score. This performs a canonical symmetric version of the PLS\\nession. But slightly different than the CCA. This is mostly used\\nmodeling.\\n\\n implementation provides the same results that the \\\"plspm\\\" package\\nided in the R language (R-project), using the function plsca(X, Y).\\nlts are equal or collinear with the function\\ns(..., mode = \\\"canonical\\\")`` of the \\\"mixOmics\\\" package. The difference\\nes in the fact that mixOmics implementation does not exactly implement\\nWold algorithm since it does not normalize y_weights to one.\\n\\nples\\n----\\nfrom sklearn.cross_decomposition import PLSCanonical\\nX = [[0., 0., 1.], [1.,0.,0.], [2.,2.,2.], [2.,5.,4.]]\\nY = [[0.1, -0.2], [0.9, 1.1], [6.2, 5.9], [11.9, 12.3]]\\nplsca = PLSCanonical(n_components=2)\\nplsca.fit(X, Y)\\nanonical()\\nX_c, Y_c = plsca.transform(X, Y)\\n\\nrences\\n------\\n\\nb A. Wegelin. A survey of Partial Least Squares (PLS) methods, with\\nasis on the two-block case. Technical Report 371, Department of\\nistics, University of Washington, Seattle, 2000.\\n\\nnhaus, M. (1998). La regression PLS: theorie et pratique. Paris:\\nions Technic.\\n\\nalso\\n----\\n\\nVD\\n```\\n\"]] [:hr] [:hr]] [:div [:h3 \":sklearn.regression/pls-regression\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [5 2]:\\n\\n|         :name |  :default |\\n|---------------|-----------|\\n|         :copy |      true |\\n|     :max-iter |       500 |\\n| :n-components |         2 |\\n|        :scale |      true |\\n|          :tol | 1.000E-06 |\\n\"]]] [:span [:p/markdown \"PLS regression\\n\\n    PLSRegression implements the PLS 2 blocks regression known as PLS2 or PLS1\\n    in case of one dimensional response.\\n    This class inherits from _PLS with mode=\\\"A\\\", deflation_mode=\\\"regression\\\",\\n    norm_y_weights=False and algorithm=\\\"nipals\\\".\\n\\n    Read more in the User Guide: `cross_decomposition`.\\n\\n    *Added in 0.8*\\n\\n    Parameters\\n    ----------\\n- `n_components`: int, (default 2)\\n        Number of components to keep.\\n\\n- `scale`: boolean, (default True)\\n        whether to scale the data\\n\\n- `max_iter`: an integer, (default 500)\\n        the maximum number of iterations of the NIPALS inner loop (used\\n        only if algorithm=\\\"nipals\\\")\\n\\n- `tol`: non-negative real\\n        Tolerance used in the iterative algorithm default 1e-06.\\n\\n- `copy`: boolean, default True\\n        Whether the deflation should be done on a copy. Let the default\\n        value to True unless you don't care about side effect\\n\\n    Attributes\\n    ----------\\n- `x_weights_`: array, [p, n_components]\\n        X block weights vectors.\\n\\n- `y_weights_`: array, [q, n_components]\\n        Y block weights vectors.\\n\\n- `x_loadings_`: array, [p, n_components]\\n        X block loadings vectors.\\n\\n- `y_loadings_`: array, [q, n_components]\\n        Y block loadings vectors.\\n\\n- `x_scores_`: array, [n_samples, n_components]\\n        X scores.\\n\\n- `y_scores_`: array, [n_samples, n_components]\\n        Y scores.\\n\\n- `x_rotations_`: array, [p, n_components]\\n        X block to latents rotations.\\n\\n- `y_rotations_`: array, [q, n_components]\\n        Y block to latents rotations.\\n\\n- `coef_`: array, [p, q]\\n        The coefficients of the linear model: ``Y = X coef_ + Err``\\n\\n- `n_iter_`: array-like\\n        Number of iterations of the NIPALS inner loop for each\\n        component.\\n\\n    Notes\\n    -----\\n    Matrices\\n\\n```python\\nT: x_scores_\\nU: y_scores_\\nW: x_weights_\\nC: y_weights_\\nP: x_loadings_\\nQ: y_loadings_\\n\\ncomputed such that::\\n\\nX = T P.T + Err and Y = U Q.T + Err\\nT[:, k] = Xk W[:, k] for k in range(n_components)\\nU[:, k] = Yk C[:, k] for k in range(n_components)\\nx_rotations_ = W (P.T W)^(-1)\\ny_rotations_ = C (Q.T C)^(-1)\\n\\ne Xk and Yk are residual matrices at iteration k.\\n\\ndes explaining\\n<http://www.eigenvector.com/Docs/Wise_pls_properties.pdf>`_\\n\\n\\neach component k, find weights u, v that optimizes:\\nx corr(Xk u, Yk v) * std(Xk u) std(Yk u)``, such that ``|u| = 1``\\n\\n that it maximizes both the correlations between the scores and the\\na-block variances.\\n\\nresidual matrix of X (Xk+1) block is obtained by the deflation on\\ncurrent X score: x_score.\\n\\nresidual matrix of Y (Yk+1) block is obtained by deflation on the\\nent X score. This performs the PLS regression known as PLS2. This\\n is prediction oriented.\\n\\n implementation provides the same results that 3 PLS packages\\nided in the R language (R-project):\\n\\n- \\\"mixOmics\\\" with function pls(X, Y, mode = \\\"regression\\\")\\n- \\\"plspm \\\" with function plsreg2(X, Y)\\n- \\\"pls\\\" with function oscorespls.fit(X, Y)\\n\\nples\\n----\\nfrom sklearn.cross_decomposition import PLSRegression\\nX = [[0., 0., 1.], [1.,0.,0.], [2.,2.,2.], [2.,5.,4.]]\\nY = [[0.1, -0.2], [0.9, 1.1], [6.2, 5.9], [11.9, 12.3]]\\npls2 = PLSRegression(n_components=2)\\npls2.fit(X, Y)\\negression()\\nY_pred = pls2.predict(X)\\n\\nrences\\n------\\n\\nb A. Wegelin. A survey of Partial Least Squares (PLS) methods, with\\nasis on the two-block case. Technical Report 371, Department of\\nistics, University of Washington, Seattle, 2000.\\n\\nrench but still a reference:\\nnhaus, M. (1998). La regression PLS: theorie et pratique. Paris:\\nions Technic.\\n```\\n\"]] [:hr] [:hr]] [:div [:h3 \":sklearn.regression/poisson-regressor\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [6 2]:\\n\\n|          :name |  :default |\\n|----------------|-----------|\\n|         :alpha |     1.000 |\\n| :fit-intercept |      true |\\n|      :max-iter |       100 |\\n|           :tol | 0.0001000 |\\n|       :verbose |         0 |\\n|    :warm-start |     false |\\n\"]]] [:span [:p/markdown \"Generalized Linear Model with a Poisson distribution.\\n\\n    Read more in the User Guide: `Generalized_linear_regression`.\\n\\n    Parameters\\n    ----------\\n- `alpha`: float, default=1\\n        Constant that multiplies the penalty term and thus determines the\\n        regularization strength. ``alpha = 0`` is equivalent to unpenalized\\n        GLMs. In this case, the design matrix `X` must have full column rank\\n        (no collinearities).\\n\\n- `fit_intercept`: bool, default=True\\n        Specifies if a constant (a.k.a. bias or intercept) should be\\n        added to the linear predictor (X @ coef + intercept).\\n\\n- `max_iter`: int, default=100\\n        The maximal number of iterations for the solver.\\n\\n- `tol`: float, default=1e-4\\n        Stopping criterion. For the lbfgs solver,\\n        the iteration will stop when ``max{|g_j|, j = 1, ..., d} <= tol``\\n        where ``g_j`` is the j-th component of the gradient (derivative) of\\n        the objective function.\\n\\n- `warm_start`: bool, default=False\\n        If set to ``True``, reuse the solution of the previous call to ``fit``\\n        as initialization for ``coef_`` and ``intercept_`` .\\n\\n- `verbose`: int, default=0\\n        For the lbfgs solver set verbose to any positive number for verbosity.\\n\\n    Attributes\\n    ----------\\n- `coef_`: array of shape (n_features,)\\n        Estimated coefficients for the linear predictor (`X @ coef_ +\\n        intercept_`) in the GLM.\\n\\n- `intercept_`: float\\n        Intercept (a.k.a. bias) added to linear predictor.\\n\\n- `n_iter_`: int\\n        Actual number of iterations used in the solver.\\n    \"]] [:hr] [:hr]] [:div [:h3 \":sklearn.regression/radius-neighbors-regressor\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [8 2]:\\n\\n|          :name |  :default |\\n|----------------|-----------|\\n|     :algorithm |      auto |\\n|     :leaf-size |        30 |\\n|        :metric | minkowski |\\n| :metric-params |           |\\n|        :n-jobs |           |\\n|             :p |         2 |\\n|        :radius |     1.000 |\\n|       :weights |   uniform |\\n\"]]] [:span [:p/markdown \"Regression based on neighbors within a fixed radius.\\n\\n    The target is predicted by local interpolation of the targets\\n    associated of the nearest neighbors in the training set.\\n\\n    Read more in the User Guide: `regression`.\\n\\n    *Added in 0.9*\\n\\n    Parameters\\n    ----------\\n- `radius`: float, default=1.0\\n        Range of parameter space to use by default for `radius_neighbors`\\n        queries.\\n\\n- `weights`: {'uniform', 'distance'} or callable, default='uniform'\\n        weight function used in prediction.  Possible values:\\n\\n        - 'uniform' : uniform weights.  All points in each neighborhood\\n          are weighted equally.\\n        - 'distance' : weight points by the inverse of their distance.\\n          in this case, closer neighbors of a query point will have a\\n          greater influence than neighbors which are further away.\\n        - [callable] : a user-defined function which accepts an\\n          array of distances, and returns an array of the same shape\\n          containing the weights.\\n\\n        Uniform weights are used by default.\\n\\n- `algorithm`: {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\\n        Algorithm used to compute the nearest neighbors:\\n\\n        - 'ball_tree' will use `BallTree`\\n        - 'kd_tree' will use `KDTree`\\n        - 'brute' will use a brute-force search.\\n        - 'auto' will attempt to decide the most appropriate algorithm\\n          based on the values passed to `fit` method.\\n\\n        Note: fitting on sparse input will override the setting of\\n        this parameter, using brute force.\\n\\n- `leaf_size`: int, default=30\\n        Leaf size passed to BallTree or KDTree.  This can affect the\\n        speed of the construction and query, as well as the memory\\n        required to store the tree.  The optimal value depends on the\\n        nature of the problem.\\n\\n- `p`: int, default=2\\n        Power parameter for the Minkowski metric. When p = 1, this is\\n        equivalent to using manhattan_distance (l1), and euclidean_distance\\n        (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\\n\\n- `metric`: str or callable, default='minkowski'\\n        the distance metric to use for the tree.  The default metric is\\n        minkowski, and with p=2 is equivalent to the standard Euclidean\\n        metric. See the documentation of `DistanceMetric` for a\\n        list of available metrics.\\n        If metric is \\\"precomputed\\\", X is assumed to be a distance matrix and\\n        must be square during fit. X may be a :term:`sparse graph`,\\n        in which case only \\\"nonzero\\\" elements may be considered neighbors.\\n\\n- `metric_params`: dict, default=None\\n        Additional keyword arguments for the metric function.\\n\\n- `n_jobs`: int, default=None\\n        The number of parallel jobs to run for neighbors search.\\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\\n        for more details.\\n\\n    Attributes\\n    ----------\\n- `effective_metric_`: str or callable\\n        The distance metric to use. It will be same as the `metric` parameter\\n        or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\\n        'minkowski' and `p` parameter set to 2.\\n\\n- `effective_metric_params_`: dict\\n        Additional keyword arguments for the metric function. For most metrics\\n        will be same with `metric_params` parameter, but may also contain the\\n        `p` parameter value if the `effective_metric_` attribute is set to\\n        'minkowski'.\\n\\n    Examples\\n    --------\\n    >>> X = [[0], [1], [2], [3]]\\n    >>> y = [0, 0, 1, 1]\\n    >>> from sklearn.neighbors import RadiusNeighborsRegressor\\n    >>> neigh = RadiusNeighborsRegressor(radius=1.0)\\n    >>> neigh.fit(X, y)\\n    RadiusNeighborsRegressor(...)\\n    >>> print(neigh.predict([[1.5]]))\\n    [0.5]\\n\\n    See also\\n    --------\\n    NearestNeighbors\\n    KNeighborsRegressor\\n    KNeighborsClassifier\\n    RadiusNeighborsClassifier\\n\\n    Notes\\n    -----\\n    See Nearest Neighbors: `neighbors` in the online documentation\\n    for a discussion of the choice of ``algorithm`` and ``leaf_size``.\\n\\n    https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm\\n    \"]] [:hr] [:hr]] [:div [:h3 \":sklearn.regression/random-forest-regressor\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [18 2]:\\n\\n|                     :name | :default |\\n|---------------------------|----------|\\n| :min-weight-fraction-leaf |    0.000 |\\n|           :max-leaf-nodes |          |\\n|    :min-impurity-decrease |    0.000 |\\n|        :min-samples-split |    2.000 |\\n|                :bootstrap |     true |\\n|                :ccp-alpha |    0.000 |\\n|                   :n-jobs |          |\\n|             :random-state |          |\\n|                :oob-score |    false |\\n|         :min-samples-leaf |        1 |\\n|             :max-features |     auto |\\n|       :min-impurity-split |          |\\n|               :warm-start |    false |\\n|                :max-depth |          |\\n|             :n-estimators |      100 |\\n|              :max-samples |          |\\n|                :criterion |      mse |\\n|                  :verbose |        0 |\\n\"]]] [:span [:p/markdown \"\\n    A random forest regressor.\\n\\n    A random forest is a meta estimator that fits a number of classifying\\n    decision trees on various sub-samples of the dataset and uses averaging\\n    to improve the predictive accuracy and control over-fitting.\\n    The sub-sample size is controlled with the `max_samples` parameter if\\n    `bootstrap=True` (default), otherwise the whole dataset is used to build\\n    each tree.\\n\\n    Read more in the User Guide: `forest`.\\n\\n    Parameters\\n    ----------\\n- `n_estimators`: int, default=100\\n        The number of trees in the forest.\\n\\n        *Changed in 0.22*\\n           The default value of ``n_estimators`` changed from 10 to 100\\n           in 0.22.\\n\\n- `criterion`: {\\\"mse\\\", \\\"mae\\\"}, default=\\\"mse\\\"\\n        The function to measure the quality of a split. Supported criteria\\n        are \\\"mse\\\" for the mean squared error, which is equal to variance\\n        reduction as feature selection criterion, and \\\"mae\\\" for the mean\\n        absolute error.\\n\\n        *Added in 0.18*\\n           Mean Absolute Error (MAE) criterion.\\n\\n- `max_depth`: int, default=None\\n        The maximum depth of the tree. If None, then nodes are expanded until\\n        all leaves are pure or until all leaves contain less than\\n        min_samples_split samples.\\n\\n- `min_samples_split`: int or float, default=2\\n        The minimum number of samples required to split an internal node:\\n\\n        - If int, then consider `min_samples_split` as the minimum number.\\n        - If float, then `min_samples_split` is a fraction and\\n          `ceil(min_samples_split * n_samples)` are the minimum\\n          number of samples for each split.\\n\\n        *Changed in 0.18*\\n           Added float values for fractions.\\n\\n- `min_samples_leaf`: int or float, default=1\\n        The minimum number of samples required to be at a leaf node.\\n        A split point at any depth will only be considered if it leaves at\\n        least ``min_samples_leaf`` training samples in each of the left and\\n        right branches.  This may have the effect of smoothing the model,\\n        especially in regression.\\n\\n        - If int, then consider `min_samples_leaf` as the minimum number.\\n        - If float, then `min_samples_leaf` is a fraction and\\n          `ceil(min_samples_leaf * n_samples)` are the minimum\\n          number of samples for each node.\\n\\n        *Changed in 0.18*\\n           Added float values for fractions.\\n\\n- `min_weight_fraction_leaf`: float, default=0.0\\n        The minimum weighted fraction of the sum total of weights (of all\\n        the input samples) required to be at a leaf node. Samples have\\n        equal weight when sample_weight is not provided.\\n\\n- `max_features`: {\\\"auto\\\", \\\"sqrt\\\", \\\"log2\\\"}, int or float, default=\\\"auto\\\"\\n        The number of features to consider when looking for the best split:\\n\\n        - If int, then consider `max_features` features at each split.\\n        - If float, then `max_features` is a fraction and\\n          `int(max_features * n_features)` features are considered at each\\n          split.\\n        - If \\\"auto\\\", then `max_features=n_features`.\\n        - If \\\"sqrt\\\", then `max_features=sqrt(n_features)`.\\n        - If \\\"log2\\\", then `max_features=log2(n_features)`.\\n        - If None, then `max_features=n_features`.\\n\\n        Note: the search for a split does not stop until at least one\\n        valid partition of the node samples is found, even if it requires to\\n        effectively inspect more than ``max_features`` features.\\n\\n- `max_leaf_nodes`: int, default=None\\n        Grow trees with ``max_leaf_nodes`` in best-first fashion.\\n        Best nodes are defined as relative reduction in impurity.\\n        If None then unlimited number of leaf nodes.\\n\\n- `min_impurity_decrease`: float, default=0.0\\n        A node will be split if this split induces a decrease of the impurity\\n        greater than or equal to this value.\\n\\n        The weighted impurity decrease equation is the following\\n\\n```python\\nN_t / N * (impurity - N_t_R / N_t * right_impurity\\n                    - N_t_L / N_t * left_impurity)\\n\\ne ``N`` is the total number of samples, ``N_t`` is the number of\\nles at the current node, ``N_t_L`` is the number of samples in the\\n child, and ``N_t_R`` is the number of samples in the right child.\\n\\n`, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\\n`sample_weight`` is passed.\\n\\nersionadded:: 0.19\\n\\nrity_split : float, default=None\\nshold for early stopping in tree growth. A node will split\\nts impurity is above the threshold, otherwise it is a leaf.\\n\\neprecated:: 0.19\\n`min_impurity_split`` has been deprecated in favor of\\n`min_impurity_decrease`` in 0.19. The default value of\\n`min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\\nill be removed in 0.25. Use ``min_impurity_decrease`` instead.\\n\\np : bool, default=True\\nher bootstrap samples are used when building trees. If False, the\\ne dataset is used to build each tree.\\n\\ne : bool, default=False\\nher to use out-of-bag samples to estimate\\nR^2 on unseen data.\\n\\n int, default=None\\nnumber of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\\nh:`decision_path` and :meth:`apply` are all parallelized over the\\ns. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\\next. ``-1`` means using all processors. See :term:`Glossary\\nobs>` for more details.\\n\\ntate : int or RandomState, default=None\\nrols both the randomness of the bootstrapping of the samples used\\n building trees (if ``bootstrap=True``) and the sampling of the\\nures to consider when looking for the best split at each node\\n``max_features < n_features``).\\n:term:`Glossary <random_state>` for details.\\n\\n: int, default=0\\nrols the verbosity when fitting and predicting.\\n\\nrt : bool, default=False\\n set to ``True``, reuse the solution of the previous call to fit\\nadd more estimators to the ensemble, otherwise, just fit a whole\\nforest. See :term:`the Glossary <warm_start>`.\\n\\na : non-negative float, default=0.0\\nlexity parameter used for Minimal Cost-Complexity Pruning. The\\nree with the largest cost complexity that is smaller than\\np_alpha`` will be chosen. By default, no pruning is performed. See\\n:`minimal_cost_complexity_pruning` for details.\\n\\nersionadded:: 0.22\\n\\nles : int or float, default=None\\nootstrap is True, the number of samples to draw from X\\nrain each base estimator.\\n\\n None (default), then draw `X.shape[0]` samples.\\n int, then draw `max_samples` samples.\\n float, then draw `max_samples * X.shape[0]` samples. Thus,\\nax_samples` should be in the interval `(0, 1)`.\\n\\nersionadded:: 0.22\\n\\nes\\n--\\nimator_ : DecisionTreeRegressor\\nchild estimator template used to create the collection of fitted\\nestimators.\\n\\nrs_ : list of DecisionTreeRegressor\\ncollection of fitted sub-estimators.\\n\\nimportances_ : ndarray of shape (n_features,)\\nimpurity-based feature importances.\\nhigher, the more important the feature.\\nimportance of a feature is computed as the (normalized)\\nl reduction of the criterion brought by that feature.  It is also\\nn as the Gini importance.\\n\\ning: impurity-based feature importances can be misleading for\\n cardinality features (many unique values). See\\nc:`sklearn.inspection.permutation_importance` as an alternative.\\n\\nes_ : int\\nnumber of features when ``fit`` is performed.\\n\\ns_ : int\\nnumber of outputs when ``fit`` is performed.\\n\\ne_ : float\\ne of the training dataset obtained using an out-of-bag estimate.\\n attribute exists only when ``oob_score`` is True.\\n\\niction_ : ndarray of shape (n_samples,)\\niction computed with out-of-bag estimate on the training set.\\n attribute exists only when ``oob_score`` is True.\\n\\n\\n\\nTreeRegressor, ExtraTreesRegressor\\n\\n\\n\\nult values for the parameters controlling the size of the trees\\nmax_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\\n trees which can potentially be very large on some data sets. To\\nemory consumption, the complexity and size of the trees should be\\ned by setting those parameter values.\\n\\nures are always randomly permuted at each split. Therefore,\\n found split may vary, even with the same training data,\\natures=n_features`` and ``bootstrap=False``, if the improvement\\nriterion is identical for several splits enumerated during the\\nf the best split. To obtain a deterministic behaviour during\\n ``random_state`` has to be fixed.\\n\\nult value ``max_features=\\\"auto\\\"`` uses ``n_features``\\nhan ``n_features / 3``. The latter was originally suggested in\\nreas the former was more recently justified empirically in [2].\\n\\nes\\n--\\n. Breiman, \\\"Random Forests\\\", Machine Learning, 45(1), 5-32, 2001.\\n\\n. Geurts, D. Ernst., and L. Wehenkel, \\\"Extremely randomized\\nrees\\\", Machine Learning, 63(1), 3-42, 2006.\\n\\n\\n\\n sklearn.ensemble import RandomForestRegressor\\n sklearn.datasets import make_regression\\n = make_regression(n_features=4, n_informative=2,\\n                   random_state=0, shuffle=False)\\n = RandomForestRegressor(max_depth=2, random_state=0)\\n.fit(X, y)\\nrestRegressor(...)\\nt(regr.predict([[0, 0, 0, 0]]))\\n7858]\\n```\\n\"]] [:hr] [:hr]] [:div [:h3 \":sklearn.regression/ransac-regressor\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [12 2]:\\n\\n|               :name |      :default |\\n|---------------------|---------------|\\n|      :is-data-valid |               |\\n|          :max-skips |      INFINITY |\\n|       :random-state |               |\\n|        :min-samples |               |\\n|   :stop-probability |        0.9900 |\\n|     :stop-n-inliers |      INFINITY |\\n|     :base-estimator |               |\\n|         :max-trials |         100.0 |\\n| :residual-threshold |               |\\n|     :is-model-valid |               |\\n|               :loss | absolute_loss |\\n|         :stop-score |      INFINITY |\\n\"]]] [:span [:p/markdown \"RANSAC (RANdom SAmple Consensus) algorithm.\\n\\n    RANSAC is an iterative algorithm for the robust estimation of parameters\\n    from a subset of inliers from the complete data set.\\n\\n    Read more in the User Guide: `ransac_regression`.\\n\\n    Parameters\\n    ----------\\n- `base_estimator`: object, optional\\n        Base estimator object which implements the following methods:\\n\\n         * `fit(X, y)`: Fit model to given training data and target values.\\n         * `score(X, y)`: Returns the mean accuracy on the given test data,\\n           which is used for the stop criterion defined by `stop_score`.\\n           Additionally, the score is used to decide which of two equally\\n           large consensus sets is chosen as the better one.\\n         * `predict(X)`: Returns predicted values using the linear model,\\n           which is used to compute residual error using loss function.\\n\\n        If `base_estimator` is None, then\\n        ``base_estimator=sklearn.linear_model.LinearRegression()`` is used for\\n        target values of dtype float.\\n\\n        Note that the current implementation only supports regression\\n        estimators.\\n\\n- `min_samples`: int (>= 1) or float ([0, 1]), optional\\n        Minimum number of samples chosen randomly from original data. Treated\\n        as an absolute number of samples for `min_samples >= 1`, treated as a\\n        relative number `ceil(min_samples * X.shape[0]`) for\\n        `min_samples < 1`. This is typically chosen as the minimal number of\\n        samples necessary to estimate the given `base_estimator`. By default a\\n        ``sklearn.linear_model.LinearRegression()`` estimator is assumed and\\n        `min_samples` is chosen as ``X.shape[1] + 1``.\\n\\n- `residual_threshold`: float, optional\\n        Maximum residual for a data sample to be classified as an inlier.\\n        By default the threshold is chosen as the MAD (median absolute\\n        deviation) of the target values `y`.\\n\\n- `is_data_valid`: callable, optional\\n        This function is called with the randomly selected data before the\\n        model is fitted to it: `is_data_valid(X, y)`. If its return value is\\n        False the current randomly chosen sub-sample is skipped.\\n\\n- `is_model_valid`: callable, optional\\n        This function is called with the estimated model and the randomly\\n        selected data: `is_model_valid(model, X, y)`. If its return value is\\n        False the current randomly chosen sub-sample is skipped.\\n        Rejecting samples with this function is computationally costlier than\\n        with `is_data_valid`. `is_model_valid` should therefore only be used if\\n        the estimated model is needed for making the rejection decision.\\n\\n- `max_trials`: int, optional\\n        Maximum number of iterations for random sample selection.\\n\\n- `max_skips`: int, optional\\n        Maximum number of iterations that can be skipped due to finding zero\\n        inliers or invalid data defined by ``is_data_valid`` or invalid models\\n        defined by ``is_model_valid``.\\n\\n        *Added in 0.19*\\n\\n- `stop_n_inliers`: int, optional\\n        Stop iteration if at least this number of inliers are found.\\n\\n- `stop_score`: float, optional\\n        Stop iteration if score is greater equal than this threshold.\\n\\n- `stop_probability`: float in range [0, 1], optional\\n        RANSAC iteration stops if at least one outlier-free set of the training\\n        data is sampled in RANSAC. This requires to generate at least N\\n        samples (iterations)\\n\\n```python\\nN >= log(1 - probability) / log(1 - e**m)\\n\\ne the probability (confidence) is typically set to high value such\\n.99 (the default) and e is the current fraction of inliers w.r.t.\\ntotal number of samples.\\n\\ntring, callable, optional, default \\\"absolute_loss\\\"\\nng inputs, \\\"absolute_loss\\\" and \\\"squared_loss\\\" are supported which\\n the absolute loss and squared loss per sample\\nectively.\\n\\n`loss`` is a callable, then it should be a function that takes\\narrays as inputs, the true and predicted value and returns a 1-D\\ny with the i-th value of the array corresponding to the loss\\n`X[i]``.\\n\\nhe loss on a sample is greater than the ``residual_threshold``,\\n this sample is classified as an outlier.\\n\\nersionadded:: 0.18\\n\\ntate : int, RandomState instance, default=None\\ngenerator used to initialize the centers.\\n an int for reproducible output across multiple function calls.\\n:term:`Glossary <random_state>`.\\n\\nes\\n--\\nr_ : object\\n fitted model (copy of the `base_estimator` object).\\n\\n_ : int\\ner of random selection trials until one of the stop criteria is\\n It is always ``<= max_trials``.\\n\\nask_ : bool array of shape [n_samples]\\nean mask of inliers classified as ``True``.\\n\\nno_inliers_ : int\\ner of iterations skipped due to finding zero inliers.\\n\\nersionadded:: 0.19\\n\\ninvalid_data_ : int\\ner of iterations skipped due to invalid data defined by\\n_data_valid``.\\n\\nersionadded:: 0.19\\n\\ninvalid_model_ : int\\ner of iterations skipped due to an invalid model defined by\\n_model_valid``.\\n\\nersionadded:: 0.19\\n\\n\\n\\n sklearn.linear_model import RANSACRegressor\\n sklearn.datasets import make_regression\\n = make_regression(\\nn_samples=200, n_features=2, noise=4.0, random_state=0)\\n= RANSACRegressor(random_state=0).fit(X, y)\\nscore(X, y)\\n.\\npredict(X[:1,])\\n31.9417...])\\n\\nes\\n--\\nttps://en.wikipedia.org/wiki/RANSAC\\nttps://www.sri.com/sites/default/files/publications/ransac-publication.pdf\\nttp://www.bmva.org/bmvc/2009/Papers/Paper355/Paper355.pdf\\n```\\n\"]] [:hr] [:hr]] [:div [:h3 \":sklearn.regression/ridge\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [8 2]:\\n\\n|          :name | :default |\\n|----------------|----------|\\n|         :alpha |    1.000 |\\n|        :copy-x |     true |\\n| :fit-intercept |     true |\\n|      :max-iter |          |\\n|     :normalize |    false |\\n|  :random-state |          |\\n|        :solver |     auto |\\n|           :tol | 0.001000 |\\n\"]]] [:span [:p/markdown \"Linear least squares with l2 regularization.\\n\\n    Minimizes the objective function\\n\\n```python\\n||y - Xw||^2_2 + alpha * ||w||^2_2\\n\\nThis model solves a regression model where the loss function is\\nthe linear least squares function and regularization is given by\\nthe l2-norm. Also known as Ridge Regression or Tikhonov regularization.\\nThis estimator has built-in support for multi-variate regression\\n(i.e., when y is a 2d-array of shape (n_samples, n_targets)).\\n\\nRead more in the :ref:`User Guide <ridge_regression>`.\\n\\nParameters\\n----------\\nalpha : {float, ndarray of shape (n_targets,)}, default=1.0\\n    Regularization strength; must be a positive float. Regularization\\n    improves the conditioning of the problem and reduces the variance of\\n    the estimates. Larger values specify stronger regularization.\\n    Alpha corresponds to ``1 / (2C)`` in other linear models such as\\n    :class:`~sklearn.linear_model.LogisticRegression` or\\n    :class:`sklearn.svm.LinearSVC`. If an array is passed, penalties are\\n    assumed to be specific to the targets. Hence they must correspond in\\n    number.\\n\\nfit_intercept : bool, default=True\\n    Whether to fit the intercept for this model. If set\\n    to false, no intercept will be used in calculations\\n    (i.e. ``X`` and ``y`` are expected to be centered).\\n\\nnormalize : bool, default=False\\n    This parameter is ignored when ``fit_intercept`` is set to False.\\n    If True, the regressors X will be normalized before regression by\\n    subtracting the mean and dividing by the l2-norm.\\n    If you wish to standardize, please use\\n    :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\\n    on an estimator with ``normalize=False``.\\n\\ncopy_X : bool, default=True\\n    If True, X will be copied; else, it may be overwritten.\\n\\nmax_iter : int, default=None\\n    Maximum number of iterations for conjugate gradient solver.\\n    For 'sparse_cg' and 'lsqr' solvers, the default value is determined\\n    by scipy.sparse.linalg. For 'sag' solver, the default value is 1000.\\n\\ntol : float, default=1e-3\\n    Precision of the solution.\\n\\nsolver : {'auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'},         default='auto'\\n    Solver to use in the computational routines:\\n\\n    - 'auto' chooses the solver automatically based on the type of data.\\n\\n    - 'svd' uses a Singular Value Decomposition of X to compute the Ridge\\n      coefficients. More stable for singular matrices than 'cholesky'.\\n\\n    - 'cholesky' uses the standard scipy.linalg.solve function to\\n      obtain a closed-form solution.\\n\\n    - 'sparse_cg' uses the conjugate gradient solver as found in\\n      scipy.sparse.linalg.cg. As an iterative algorithm, this solver is\\n      more appropriate than 'cholesky' for large-scale data\\n      (possibility to set `tol` and `max_iter`).\\n\\n    - 'lsqr' uses the dedicated regularized least-squares routine\\n      scipy.sparse.linalg.lsqr. It is the fastest and uses an iterative\\n      procedure.\\n\\n    - 'sag' uses a Stochastic Average Gradient descent, and 'saga' uses\\n      its improved, unbiased version named SAGA. Both methods also use an\\n      iterative procedure, and are often faster than other solvers when\\n      both n_samples and n_features are large. Note that 'sag' and\\n      'saga' fast convergence is only guaranteed on features with\\n      approximately the same scale. You can preprocess the data with a\\n      scaler from sklearn.preprocessing.\\n\\n    All last five solvers support both dense and sparse data. However, only\\n    'sag' and 'sparse_cg' supports sparse input when `fit_intercept` is\\n    True.\\n\\n    .. versionadded:: 0.17\\n       Stochastic Average Gradient descent solver.\\n    .. versionadded:: 0.19\\n       SAGA solver.\\n\\nrandom_state : int, RandomState instance, default=None\\n    Used when ``solver`` == 'sag' or 'saga' to shuffle the data.\\n    See :term:`Glossary <random_state>` for details.\\n\\n    .. versionadded:: 0.17\\n       `random_state` to support Stochastic Average Gradient.\\n\\nAttributes\\n----------\\ncoef_ : ndarray of shape (n_features,) or (n_targets, n_features)\\n    Weight vector(s).\\n\\nintercept_ : float or ndarray of shape (n_targets,)\\n    Independent term in decision function. Set to 0.0 if\\n    ``fit_intercept = False``.\\n\\nn_iter_ : None or ndarray of shape (n_targets,)\\n    Actual number of iterations for each target. Available only for\\n    sag and lsqr solvers. Other solvers will return None.\\n\\n    .. versionadded:: 0.17\\n\\nSee also\\n--------\\nRidgeClassifier : Ridge classifier\\nRidgeCV : Ridge regression with built-in cross validation\\n:class:`sklearn.kernel_ridge.KernelRidge` : Kernel ridge regression\\n    combines ridge regression with the kernel trick\\n\\nExamples\\n--------\\n>>> from sklearn.linear_model import Ridge\\n>>> import numpy as np\\n>>> n_samples, n_features = 10, 5\\n>>> rng = np.random.RandomState(0)\\n>>> y = rng.randn(n_samples)\\n>>> X = rng.randn(n_samples, n_features)\\n>>> clf = Ridge(alpha=1.0)\\n>>> clf.fit(X, y)\\nRidge()\\n```\\n\"]] [:hr] [:hr]] [:div [:h3 \":sklearn.regression/ridge-cv\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [7 2]:\\n\\n|            :name |                    :default |\\n|------------------|-----------------------------|\\n|          :alphas | #tech.v3.tensor<float64>[3] |\\n|                  | [0.1000 1.000 10.00]        |\\n|              :cv |                             |\\n|   :fit-intercept |                        true |\\n|        :gcv-mode |                             |\\n|       :normalize |                       false |\\n|         :scoring |                             |\\n| :store-cv-values |                       false |\\n\"]]] [:span [:p/markdown \"Ridge regression with built-in cross-validation.\\n\\n    See glossary entry for :term:`cross-validation estimator`.\\n\\n    By default, it performs Generalized Cross-Validation, which is a form of\\n    efficient Leave-One-Out cross-validation.\\n\\n    Read more in the User Guide: `ridge_regression`.\\n\\n    Parameters\\n    ----------\\n- `alphas`: ndarray of shape (n_alphas,), default=(0.1, 1.0, 10.0)\\n        Array of alpha values to try.\\n        Regularization strength; must be a positive float. Regularization\\n        improves the conditioning of the problem and reduces the variance of\\n        the estimates. Larger values specify stronger regularization.\\n        Alpha corresponds to ``1 / (2C)`` in other linear models such as\\n        `~sklearn.linear_model.LogisticRegression` or\\n        `sklearn.svm.LinearSVC`.\\n        If using generalized cross-validation, alphas must be positive.\\n\\n- `fit_intercept`: bool, default=True\\n        Whether to calculate the intercept for this model. If set\\n        to false, no intercept will be used in calculations\\n        (i.e. data is expected to be centered).\\n\\n- `normalize`: bool, default=False\\n        This parameter is ignored when ``fit_intercept`` is set to False.\\n        If True, the regressors X will be normalized before regression by\\n        subtracting the mean and dividing by the l2-norm.\\n        If you wish to standardize, please use\\n        `sklearn.preprocessing.StandardScaler` before calling ``fit``\\n        on an estimator with ``normalize=False``.\\n\\n- `scoring`: string, callable, default=None\\n        A string (see model evaluation documentation) or\\n        a scorer callable object / function with signature\\n        ``scorer(estimator, X, y)``.\\n        If None, the negative mean squared error if cv is 'auto' or None\\n        (i.e. when using generalized cross-validation), and r2 score otherwise.\\n\\n- `cv`: int, cross-validation generator or an iterable, default=None\\n        Determines the cross-validation splitting strategy.\\n        Possible inputs for cv are:\\n\\n        - None, to use the efficient Leave-One-Out cross-validation\\n          (also known as Generalized Cross-Validation).\\n        - integer, to specify the number of folds.\\n        - :term:`CV splitter`,\\n        - An iterable yielding (train, test) splits as arrays of indices.\\n\\n        For integer/None inputs, if ``y`` is binary or multiclass,\\n        `sklearn.model_selection.StratifiedKFold` is used, else,\\n        `sklearn.model_selection.KFold` is used.\\n\\n        Refer User Guide: `cross_validation` for the various\\n        cross-validation strategies that can be used here.\\n\\n- `gcv_mode`: {'auto', 'svd', eigen'}, default='auto'\\n        Flag indicating which strategy to use when performing\\n        Generalized Cross-Validation. Options are\\n\\n```python\\n'auto' : use 'svd' if n_samples > n_features, otherwise use 'eigen'\\n'svd' : force use of singular value decomposition of X when X is\\n    dense, eigenvalue decomposition of X^T.X when X is sparse.\\n'eigen' : force computation via eigendecomposition of X.X^T\\n\\n'auto' mode is the default and is intended to pick the cheaper\\non of the two depending on the shape of the training data.\\n\\n_values : bool, default=False\\n indicating if the cross-validation values corresponding to\\n alpha should be stored in the ``cv_values_`` attribute (see\\nw). This flag is only compatible with ``cv=None`` (i.e. using\\nralized Cross-Validation).\\n\\nes\\n--\\ns_ : ndarray of shape (n_samples, n_alphas) or         shape (n_samples, n_targets, n_alphas), optional\\ns-validation values for each alpha (only available if         ``store_cv_values=True`` and ``cv=None``). After ``fit()`` has been         called, this attribute will contain the mean squared errors         (by default) or the values of the ``{loss,score}_func`` function         (if provided in the constructor).\\n\\nndarray of shape (n_features) or (n_targets, n_features)\\nht vector(s).\\n\\nt_ : float or ndarray of shape (n_targets,)\\npendent term in decision function. Set to 0.0 if\\nt_intercept = False``.\\n\\n float\\nmated regularization parameter.\\n\\nre_ : float\\ne of base estimator with best alpha.\\n\\n\\n\\n sklearn.datasets import load_diabetes\\n sklearn.linear_model import RidgeCV\\n = load_diabetes(return_X_y=True)\\n= RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X, y)\\nscore(X, y)\\n.\\n\\n\\n\\nRidge regression\\nssifier : Ridge classifier\\nssifierCV : Ridge classifier with built-in cross validation\\n```\\n\"]] [:hr] [:hr]] [:div [:h3 \":sklearn.regression/sgd-regressor\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [19 2]:\\n\\n|                :name |     :default |\\n|----------------------|--------------|\\n|    :n-iter-no-change |            5 |\\n|       :learning-rate |   invscaling |\\n|             :average |        false |\\n|                 :tol |     0.001000 |\\n|      :early-stopping |        false |\\n|               :eta-0 |      0.01000 |\\n|             :shuffle |         true |\\n|             :penalty |           l2 |\\n|             :power-t |       0.2500 |\\n|            :max-iter |         1000 |\\n|        :random-state |              |\\n|       :fit-intercept |         true |\\n|               :alpha |    0.0001000 |\\n|          :warm-start |        false |\\n|           :l-1-ratio |       0.1500 |\\n| :validation-fraction |       0.1000 |\\n|                :loss | squared_loss |\\n|             :verbose |            0 |\\n|             :epsilon |       0.1000 |\\n\"]]] [:span [:p/markdown \"Linear model fitted by minimizing a regularized empirical loss with SGD\\n\\n    SGD stands for Stochastic Gradient Descent: the gradient of the loss is\\n    estimated each sample at a time and the model is updated along the way with\\n    a decreasing strength schedule (aka learning rate).\\n\\n    The regularizer is a penalty added to the loss function that shrinks model\\n    parameters towards the zero vector using either the squared euclidean norm\\n    L2 or the absolute norm L1 or a combination of both (Elastic Net). If the\\n    parameter update crosses the 0.0 value because of the regularizer, the\\n    update is truncated to 0.0 to allow for learning sparse models and achieve\\n    online feature selection.\\n\\n    This implementation works with data represented as dense numpy arrays of\\n    floating point values for the features.\\n\\n    Read more in the User Guide: `sgd`.\\n\\n    Parameters\\n    ----------\\n- `loss`: str, default='squared_loss'\\n        The loss function to be used. The possible values are 'squared_loss',\\n        'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'\\n\\n        The 'squared_loss' refers to the ordinary least squares fit.\\n        'huber' modifies 'squared_loss' to focus less on getting outliers\\n        correct by switching from squared to linear loss past a distance of\\n        epsilon. 'epsilon_insensitive' ignores errors less than epsilon and is\\n        linear past that; this is the loss function used in SVR.\\n        'squared_epsilon_insensitive' is the same but becomes squared loss past\\n        a tolerance of epsilon.\\n\\n        More details about the losses formulas can be found in the\\n        User Guide: `sgd_mathematical_formulation`.\\n\\n- `penalty`: {'l2', 'l1', 'elasticnet'}, default='l2'\\n        The penalty (aka regularization term) to be used. Defaults to 'l2'\\n        which is the standard regularizer for linear SVM models. 'l1' and\\n        'elasticnet' might bring sparsity to the model (feature selection)\\n        not achievable with 'l2'.\\n\\n- `alpha`: float, default=0.0001\\n        Constant that multiplies the regularization term. The higher the\\n        value, the stronger the regularization.\\n        Also used to compute the learning rate when set to `learning_rate` is\\n        set to 'optimal'.\\n\\n- `l1_ratio`: float, default=0.15\\n        The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1.\\n        l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1.\\n        Only used if `penalty` is 'elasticnet'.\\n\\n- `fit_intercept`: bool, default=True\\n        Whether the intercept should be estimated or not. If False, the\\n        data is assumed to be already centered.\\n\\n- `max_iter`: int, default=1000\\n        The maximum number of passes over the training data (aka epochs).\\n        It only impacts the behavior in the ``fit`` method, and not the\\n        `partial_fit` method.\\n\\n        *Added in 0.19*\\n\\n- `tol`: float, default=1e-3\\n        The stopping criterion. If it is not None, training will stop\\n        when (loss > best_loss - tol) for ``n_iter_no_change`` consecutive\\n        epochs.\\n\\n        *Added in 0.19*\\n\\n- `shuffle`: bool, default=True\\n        Whether or not the training data should be shuffled after each epoch.\\n\\n- `verbose`: int, default=0\\n        The verbosity level.\\n\\n- `epsilon`: float, default=0.1\\n        Epsilon in the epsilon-insensitive loss functions; only if `loss` is\\n        'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'.\\n        For 'huber', determines the threshold at which it becomes less\\n        important to get the prediction exactly right.\\n        For epsilon-insensitive, any differences between the current prediction\\n        and the correct label are ignored if they are less than this threshold.\\n\\n- `random_state`: int, RandomState instance, default=None\\n        Used for shuffling the data, when ``shuffle`` is set to ``True``.\\n        Pass an int for reproducible output across multiple function calls.\\n        See :term:`Glossary <random_state>`.\\n\\n- `learning_rate`: string, default='invscaling'\\n        The learning rate schedule:\\n\\n        - 'constant': `eta = eta0`\\n        - 'optimal': `eta = 1.0 / (alpha * (t + t0))`\\n          where t0 is chosen by a heuristic proposed by Leon Bottou.\\n        - 'invscaling': `eta = eta0 / pow(t, power_t)`\\n        - 'adaptive': eta = eta0, as long as the training keeps decreasing.\\n          Each time n_iter_no_change consecutive epochs fail to decrease the\\n          training loss by tol or fail to increase validation score by tol if\\n          early_stopping is True, the current learning rate is divided by 5.\\n\\n            *Added in 0.20*\\n                Added 'adaptive' option\\n\\n- `eta0`: double, default=0.01\\n        The initial learning rate for the 'constant', 'invscaling' or\\n        'adaptive' schedules. The default value is 0.01.\\n\\n- `power_t`: double, default=0.25\\n        The exponent for inverse scaling learning rate.\\n\\n- `early_stopping`: bool, default=False\\n        Whether to use early stopping to terminate training when validation\\n        score is not improving. If set to True, it will automatically set aside\\n        a fraction of training data as validation and terminate\\n        training when validation score returned by the `score` method is not\\n        improving by at least `tol` for `n_iter_no_change` consecutive\\n        epochs.\\n\\n        *Added in 0.20*\\n            Added 'early_stopping' option\\n\\n- `validation_fraction`: float, default=0.1\\n        The proportion of training data to set aside as validation set for\\n        early stopping. Must be between 0 and 1.\\n        Only used if `early_stopping` is True.\\n\\n        *Added in 0.20*\\n            Added 'validation_fraction' option\\n\\n- `n_iter_no_change`: int, default=5\\n        Number of iterations with no improvement to wait before early stopping.\\n\\n        *Added in 0.20*\\n            Added 'n_iter_no_change' option\\n\\n- `warm_start`: bool, default=False\\n        When set to True, reuse the solution of the previous call to fit as\\n        initialization, otherwise, just erase the previous solution.\\n        See :term:`the Glossary <warm_start>`.\\n\\n        Repeatedly calling fit or partial_fit when warm_start is True can\\n        result in a different solution than when calling fit a single time\\n        because of the way the data is shuffled.\\n        If a dynamic learning rate is used, the learning rate is adapted\\n        depending on the number of samples already seen. Calling ``fit`` resets\\n        this counter, while ``partial_fit``  will result in increasing the\\n        existing counter.\\n\\n- `average`: bool or int, default=False\\n        When set to True, computes the averaged SGD weights accross all\\n        updates and stores the result in the ``coef_`` attribute. If set to\\n        an int greater than 1, averaging will begin once the total number of\\n        samples seen reaches `average`. So ``average=10`` will begin\\n        averaging after seeing 10 samples.\\n\\n    Attributes\\n    ----------\\n- `coef_`: ndarray of shape (n_features,)\\n        Weights assigned to the features.\\n\\n- `intercept_`: ndarray of shape (1,)\\n        The intercept term.\\n\\n- `average_coef_`: ndarray of shape (n_features,)\\n        Averaged weights assigned to the features. Only available\\n        if ``average=True``.\\n\\n        *Deprecated since 0.23*\\n            Attribute ``average_coef_`` was deprecated\\n            in version 0.23 and will be removed in 0.25.\\n\\n- `average_intercept_`: ndarray of shape (1,)\\n        The averaged intercept term. Only available if ``average=True``.\\n\\n        *Deprecated since 0.23*\\n            Attribute ``average_intercept_`` was deprecated\\n            in version 0.23 and will be removed in 0.25.\\n\\n- `n_iter_`: int\\n        The actual number of iterations before reaching the stopping criterion.\\n\\n- `t_`: int\\n        Number of weight updates performed during training.\\n        Same as ``(n_iter_ * n_samples)``.\\n\\n    Examples\\n    --------\\n    >>> import numpy as np\\n    >>> from sklearn.linear_model import SGDRegressor\\n    >>> from sklearn.pipeline import make_pipeline\\n    >>> from sklearn.preprocessing import StandardScaler\\n    >>> n_samples, n_features = 10, 5\\n    >>> rng = np.random.RandomState(0)\\n    >>> y = rng.randn(n_samples)\\n    >>> X = rng.randn(n_samples, n_features)\\n    >>> # Always scale the input. The most convenient way is to use a pipeline.\\n    >>> reg = make_pipeline(StandardScaler(),\\n    ...                     SGDRegressor(max_iter=1000, tol=1e-3))\\n    >>> reg.fit(X, y)\\n    Pipeline(steps=[('standardscaler', StandardScaler()),\\n                    ('sgdregressor', SGDRegressor())])\\n\\n    See also\\n    --------\\n    Ridge, ElasticNet, Lasso, sklearn.svm.SVR\\n\\n    \"]] [:hr] [:hr]] [:div [:h3 \":sklearn.regression/svr\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [11 2]:\\n\\n|       :name | :default |\\n|-------------|----------|\\n|     :kernel |      rbf |\\n|      :gamma |    scale |\\n|     :degree |        3 |\\n|        :tol | 0.001000 |\\n|  :shrinking |     true |\\n|          :c |    1.000 |\\n|   :max-iter |       -1 |\\n|     :coef-0 |    0.000 |\\n| :cache-size |      200 |\\n|    :verbose |    false |\\n|    :epsilon |   0.1000 |\\n\"]]] [:span [:p/markdown \"Epsilon-Support Vector Regression.\\n\\n    The free parameters in the model are C and epsilon.\\n\\n    The implementation is based on libsvm. The fit time complexity\\n    is more than quadratic with the number of samples which makes it hard\\n    to scale to datasets with more than a couple of 10000 samples. For large\\n    datasets consider using `sklearn.svm.LinearSVR` or\\n    `sklearn.linear_model.SGDRegressor` instead, possibly after a\\n    `sklearn.kernel_approximation.Nystroem` transformer.\\n\\n    Read more in the User Guide: `svm_regression`.\\n\\n    Parameters\\n    ----------\\n- `kernel`: {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'}, default='rbf'\\n         Specifies the kernel type to be used in the algorithm.\\n         It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or\\n         a callable.\\n         If none is given, 'rbf' will be used. If a callable is given it is\\n         used to precompute the kernel matrix.\\n\\n- `degree`: int, default=3\\n        Degree of the polynomial kernel function ('poly').\\n        Ignored by all other kernels.\\n\\n- `gamma`: {'scale', 'auto'} or float, default='scale'\\n        Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.\\n\\n        - if ``gamma='scale'`` (default) is passed then it uses\\n          1 / (n_features * X.var()) as value of gamma,\\n        - if 'auto', uses 1 / n_features.\\n\\n        *Changed in 0.22*\\n           The default value of ``gamma`` changed from 'auto' to 'scale'.\\n\\n- `coef0`: float, default=0.0\\n        Independent term in kernel function.\\n        It is only significant in 'poly' and 'sigmoid'.\\n\\n- `tol`: float, default=1e-3\\n        Tolerance for stopping criterion.\\n\\n- `C`: float, default=1.0\\n        Regularization parameter. The strength of the regularization is\\n        inversely proportional to C. Must be strictly positive.\\n        The penalty is a squared l2 penalty.\\n\\n- `epsilon`: float, default=0.1\\n         Epsilon in the epsilon-SVR model. It specifies the epsilon-tube\\n         within which no penalty is associated in the training loss function\\n         with points predicted within a distance epsilon from the actual\\n         value.\\n\\n- `shrinking`: bool, default=True\\n        Whether to use the shrinking heuristic.\\n        See the User Guide: `shrinking_svm`.\\n\\n- `cache_size`: float, default=200\\n        Specify the size of the kernel cache (in MB).\\n\\n- `verbose`: bool, default=False\\n        Enable verbose output. Note that this setting takes advantage of a\\n        per-process runtime setting in libsvm that, if enabled, may not work\\n        properly in a multithreaded context.\\n\\n- `max_iter`: int, default=-1\\n        Hard limit on iterations within solver, or -1 for no limit.\\n\\n    Attributes\\n    ----------\\n- `support_`: ndarray of shape (n_SV,)\\n        Indices of support vectors.\\n\\n- `support_vectors_`: ndarray of shape (n_SV, n_features)\\n        Support vectors.\\n\\n- `dual_coef_`: ndarray of shape (1, n_SV)\\n        Coefficients of the support vector in the decision function.\\n\\n- `coef_`: ndarray of shape (1, n_features)\\n        Weights assigned to the features (coefficients in the primal\\n        problem). This is only available in the case of a linear kernel.\\n\\n        `coef_` is readonly property derived from `dual_coef_` and\\n        `support_vectors_`.\\n\\n- `fit_status_`: int\\n        0 if correctly fitted, 1 otherwise (will raise warning)\\n\\n- `intercept_`: ndarray of shape (1,)\\n        Constants in decision function.\\n\\n    Examples\\n    --------\\n    >>> from sklearn.svm import SVR\\n    >>> from sklearn.pipeline import make_pipeline\\n    >>> from sklearn.preprocessing import StandardScaler\\n    >>> import numpy as np\\n    >>> n_samples, n_features = 10, 5\\n    >>> rng = np.random.RandomState(0)\\n    >>> y = rng.randn(n_samples)\\n    >>> X = rng.randn(n_samples, n_features)\\n    >>> regr = make_pipeline(StandardScaler(), SVR(C=1.0, epsilon=0.2))\\n    >>> regr.fit(X, y)\\n    Pipeline(steps=[('standardscaler', StandardScaler()),\\n                    ('svr', SVR(epsilon=0.2))])\\n\\n\\n    See also\\n    --------\\n    NuSVR\\n        Support Vector Machine for regression implemented using libsvm\\n        using a parameter to control the number of support vectors.\\n\\n    LinearSVR\\n        Scalable Linear Support Vector Machine for regression\\n        implemented using liblinear.\\n\\n    Notes\\n    -----\\n    **References:**\\n    [LIBSVM: A Library for Support Vector Machines\\n    ](http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf)\\n    \"]] [:hr] [:hr]] [:div [:h3 \":sklearn.regression/theil-sen-regressor\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [9 2]:\\n\\n|              :name |  :default |\\n|--------------------|-----------|\\n| :max-subpopulation | 1.000E+04 |\\n|               :tol |  0.001000 |\\n|      :n-subsamples |           |\\n|          :max-iter |     300.0 |\\n|            :n-jobs |           |\\n|      :random-state |           |\\n|            :copy-x |      true |\\n|     :fit-intercept |      true |\\n|           :verbose |     false |\\n\"]]] [:span [:p/markdown \"Theil-Sen Estimator: robust multivariate regression model.\\n\\n    The algorithm calculates least square solutions on subsets with size\\n    n_subsamples of the samples in X. Any value of n_subsamples between the\\n    number of features and samples leads to an estimator with a compromise\\n    between robustness and efficiency. Since the number of least square\\n    solutions is \\\"n_samples choose n_subsamples\\\", it can be extremely large\\n    and can therefore be limited with max_subpopulation. If this limit is\\n    reached, the subsets are chosen randomly. In a final step, the spatial\\n    median (or L1 median) is calculated of all least square solutions.\\n\\n    Read more in the User Guide: `theil_sen_regression`.\\n\\n    Parameters\\n    ----------\\n- `fit_intercept`: boolean, optional, default True\\n        Whether to calculate the intercept for this model. If set\\n        to false, no intercept will be used in calculations.\\n\\n- `copy_X`: boolean, optional, default True\\n        If True, X will be copied; else, it may be overwritten.\\n\\n- `max_subpopulation`: int, optional, default 1e4\\n        Instead of computing with a set of cardinality 'n choose k', where n is\\n        the number of samples and k is the number of subsamples (at least\\n        number of features), consider only a stochastic subpopulation of a\\n        given maximal size if 'n choose k' is larger than max_subpopulation.\\n        For other than small problem sizes this parameter will determine\\n        memory usage and runtime if n_subsamples is not changed.\\n\\n- `n_subsamples`: int, optional, default None\\n        Number of samples to calculate the parameters. This is at least the\\n        number of features (plus 1 if fit_intercept=True) and the number of\\n        samples as a maximum. A lower number leads to a higher breakdown\\n        point and a low efficiency while a high number leads to a low\\n        breakdown point and a high efficiency. If None, take the\\n        minimum number of subsamples leading to maximal robustness.\\n        If n_subsamples is set to n_samples, Theil-Sen is identical to least\\n        squares.\\n\\n- `max_iter`: int, optional, default 300\\n        Maximum number of iterations for the calculation of spatial median.\\n\\n- `tol`: float, optional, default 1.e-3\\n        Tolerance when calculating spatial median.\\n\\n- `random_state`: int, RandomState instance, default=None\\n        A random number generator instance to define the state of the random\\n        permutations generator. Pass an int for reproducible output across\\n        multiple function calls.\\n        See :term:`Glossary <random_state>`\\n\\n- `n_jobs`: int or None, optional (default=None)\\n        Number of CPUs to use during the cross validation.\\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\\n        for more details.\\n\\n- `verbose`: boolean, optional, default False\\n        Verbose mode when fitting the model.\\n\\n    Attributes\\n    ----------\\n- `coef_`: array, shape = (n_features)\\n        Coefficients of the regression model (median of distribution).\\n\\n- `intercept_`: float\\n        Estimated intercept of regression model.\\n\\n- `breakdown_`: float\\n        Approximated breakdown point.\\n\\n- `n_iter_`: int\\n        Number of iterations needed for the spatial median.\\n\\n- `n_subpopulation_`: int\\n        Number of combinations taken into account from 'n choose k', where n is\\n        the number of samples and k is the number of subsamples.\\n\\n    Examples\\n    --------\\n    >>> from sklearn.linear_model import TheilSenRegressor\\n    >>> from sklearn.datasets import make_regression\\n    >>> X, y = make_regression(\\n    ...     n_samples=200, n_features=2, noise=4.0, random_state=0)\\n    >>> reg = TheilSenRegressor(random_state=0).fit(X, y)\\n    >>> reg.score(X, y)\\n    0.9884...\\n    >>> reg.predict(X[:1,])\\n    array([-31.5871...])\\n\\n    References\\n    ----------\\n    - Theil-Sen Estimators in a Multiple Linear Regression Model, 2009\\n      Xin Dang, Hanxiang Peng, Xueqin Wang and Heping Zhang\\n      http://home.olemiss.edu/~xdang/papers/MTSE.pdf\\n    \"]] [:hr] [:hr]] [:div [:h3 \":sklearn.regression/transformed-target-regressor\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [5 2]:\\n\\n|          :name | :default |\\n|----------------|----------|\\n| :check-inverse |     true |\\n|          :func |          |\\n|  :inverse-func |          |\\n|     :regressor |          |\\n|   :transformer |          |\\n\"]]] [:span [:p/markdown \"Meta-estimator to regress on a transformed target.\\n\\n    Useful for applying a non-linear transformation to the target ``y`` in\\n    regression problems. This transformation can be given as a Transformer\\n    such as the QuantileTransformer or as a function and its inverse such as\\n    ``log`` and ``exp``.\\n\\n    The computation during ``fit`` is\\n\\n```python\\nregressor.fit(X, func(y))\\n\\n\\n\\nregressor.fit(X, transformer.transform(y))\\n\\ncomputation during ``predict`` is::\\n\\ninverse_func(regressor.predict(X))\\n\\n\\n\\ntransformer.inverse_transform(regressor.predict(X))\\n\\n more in the :ref:`User Guide <transformed_target_regressor>`.\\n\\nersionadded:: 0.20\\n\\nmeters\\n------\\nessor : object, default=None\\nRegressor object such as derived from ``RegressorMixin``. This\\nregressor will automatically be cloned each time prior to fitting.\\nIf regressor is ``None``, ``LinearRegression()`` is created and used.\\n\\nsformer : object, default=None\\nEstimator object such as derived from ``TransformerMixin``. Cannot be\\nset at the same time as ``func`` and ``inverse_func``. If\\n``transformer`` is ``None`` as well as ``func`` and ``inverse_func``,\\nthe transformer will be an identity transformer. Note that the\\ntransformer will be cloned during fitting. Also, the transformer is\\nrestricting ``y`` to be a numpy array.\\n\\n : function, default=None\\nFunction to apply to ``y`` before passing to ``fit``. Cannot be set at\\nthe same time as ``transformer``. The function needs to return a\\n2-dimensional array. If ``func`` is ``None``, the function used will be\\nthe identity function.\\n\\nrse_func : function, default=None\\nFunction to apply to the prediction of the regressor. Cannot be set at\\nthe same time as ``transformer`` as well. The function needs to return\\na 2-dimensional array. The inverse function is used to return\\npredictions to the same space of the original training labels.\\n\\nk_inverse : bool, default=True\\nWhether to check that ``transform`` followed by ``inverse_transform``\\nor ``func`` followed by ``inverse_func`` leads to the original targets.\\n\\nibutes\\n------\\nessor_ : object\\nFitted regressor.\\n\\nsformer_ : object\\nTransformer used in ``fit`` and ``predict``.\\n\\nples\\n----\\nimport numpy as np\\nfrom sklearn.linear_model import LinearRegression\\nfrom sklearn.compose import TransformedTargetRegressor\\ntt = TransformedTargetRegressor(regressor=LinearRegression(),\\n                                func=np.log, inverse_func=np.exp)\\nX = np.arange(4).reshape(-1, 1)\\ny = np.exp(2 * X).ravel()\\ntt.fit(X, y)\\nsformedTargetRegressor(...)\\ntt.score(X, y)\\n\\ntt.regressor_.coef_\\ny([2.])\\n\\ns\\n-\\nrnally, the target ``y`` is always converted into a 2-dimensional array\\ne used by scikit-learn transformers. At the time of prediction, the\\nut will be reshaped to a have the same number of dimensions as ``y``.\\n\\n:ref:`examples/compose/plot_transformed_target.py\\nx_glr_auto_examples_compose_plot_transformed_target.py>`.\\n\\n```\\n\"]] [:hr] [:hr]] [:div [:h3 \":sklearn.regression/tweedie-regressor\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [8 2]:\\n\\n|          :name |  :default |\\n|----------------|-----------|\\n|         :alpha |     1.000 |\\n| :fit-intercept |      true |\\n|          :link |      auto |\\n|      :max-iter |       100 |\\n|         :power |     0.000 |\\n|           :tol | 0.0001000 |\\n|       :verbose |         0 |\\n|    :warm-start |     false |\\n\"]]] [:span [:p/markdown \"Generalized Linear Model with a Tweedie distribution.\\n\\n    This estimator can be used to model different GLMs depending on the\\n    ``power`` parameter, which determines the underlying distribution.\\n\\n    Read more in the User Guide: `Generalized_linear_regression`.\\n\\n    Parameters\\n    ----------\\n- `power`: float, default=0\\n            The power determines the underlying target distribution according\\n            to the following table:\\n\\n            +-------+------------------------+\\n            | Power | Distribution           |\\n            +=======+========================+\\n            | 0     | Normal                 |\\n            +-------+------------------------+\\n            | 1     | Poisson                |\\n            +-------+------------------------+\\n            | (1,2) | Compound Poisson Gamma |\\n            +-------+------------------------+\\n            | 2     | Gamma                  |\\n            +-------+------------------------+\\n            | 3     | Inverse Gaussian       |\\n            +-------+------------------------+\\n\\n            For ``0 < power < 1``, no distribution exists.\\n\\n- `alpha`: float, default=1\\n        Constant that multiplies the penalty term and thus determines the\\n        regularization strength. ``alpha = 0`` is equivalent to unpenalized\\n        GLMs. In this case, the design matrix `X` must have full column rank\\n        (no collinearities).\\n\\n- `link`: {'auto', 'identity', 'log'}, default='auto'\\n        The link function of the GLM, i.e. mapping from linear predictor\\n        `X @ coeff + intercept` to prediction `y_pred`. Option 'auto' sets\\n        the link depending on the chosen family as follows:\\n\\n        - 'identity' for Normal distribution\\n        - 'log' for Poisson,  Gamma and Inverse Gaussian distributions\\n\\n- `fit_intercept`: bool, default=True\\n        Specifies if a constant (a.k.a. bias or intercept) should be\\n        added to the linear predictor (X @ coef + intercept).\\n\\n- `max_iter`: int, default=100\\n        The maximal number of iterations for the solver.\\n\\n- `tol`: float, default=1e-4\\n        Stopping criterion. For the lbfgs solver,\\n        the iteration will stop when ``max{|g_j|, j = 1, ..., d} <= tol``\\n        where ``g_j`` is the j-th component of the gradient (derivative) of\\n        the objective function.\\n\\n- `warm_start`: bool, default=False\\n        If set to ``True``, reuse the solution of the previous call to ``fit``\\n        as initialization for ``coef_`` and ``intercept_`` .\\n\\n- `verbose`: int, default=0\\n        For the lbfgs solver set verbose to any positive number for verbosity.\\n\\n    Attributes\\n    ----------\\n- `coef_`: array of shape (n_features,)\\n        Estimated coefficients for the linear predictor (`X @ coef_ +\\n        intercept_`) in the GLM.\\n\\n- `intercept_`: float\\n        Intercept (a.k.a. bias) added to linear predictor.\\n\\n- `n_iter_`: int\\n        Actual number of iterations used in the solver.\\n    \"]] [:hr] [:hr]])], \"1202\" [:div [:p] [:div [:p/code {:code \"(ml/transform-pipe\\n (:test (first  iris-split))\\n pipe-fn\\n fitted-ctx\\n )\", :bg-class \"bg-light\"}]] nil [:p/code {:code \"{:metamorph/data Group: 0 [50 5]:\\n\\n| :sepal_length | :sepal_width | :petal_length | :petal_width | :species |\\n|--------------:|-------------:|--------------:|-------------:|---------:|\\n|           6.4 |          2.8 |           5.6 |          2.2 |      2.0 |\\n|           6.1 |          2.8 |           4.7 |          1.2 |      1.0 |\\n|           5.0 |          2.3 |           3.3 |          1.0 |      1.0 |\\n|           6.2 |          2.9 |           4.3 |          1.3 |      1.0 |\\n|           5.9 |          3.0 |           5.1 |          1.8 |      2.0 |\\n|           6.7 |          3.0 |           5.2 |          2.3 |      2.0 |\\n|           4.7 |          3.2 |           1.3 |          0.2 |      0.0 |\\n|           4.9 |          2.5 |           4.5 |          1.7 |      1.0 |\\n|           6.0 |          2.2 |           4.0 |          1.0 |      1.0 |\\n|           5.7 |          4.4 |           1.5 |          0.4 |      0.0 |\\n|           6.1 |          2.9 |           4.7 |          1.4 |      1.0 |\\n|           6.0 |          2.7 |           5.1 |          1.6 |      2.0 |\\n|           5.2 |          3.5 |           1.5 |          0.2 |      0.0 |\\n|           5.6 |          2.5 |           3.9 |          1.1 |      1.0 |\\n|           4.5 |          2.3 |           1.3 |          0.3 |      0.0 |\\n|           6.1 |          3.0 |           4.6 |          1.4 |      1.0 |\\n|           5.8 |          2.7 |           5.1 |          1.9 |      2.0 |\\n|           5.5 |          2.4 |           3.7 |          1.0 |      1.0 |\\n|           4.8 |          3.4 |           1.9 |          0.2 |      0.0 |\\n|           7.2 |          3.2 |           6.0 |          1.8 |      2.0 |\\n|           5.7 |          3.0 |           4.2 |          1.2 |      1.0 |\\n|           7.7 |          3.8 |           6.7 |          2.2 |      2.0 |\\n|           7.4 |          2.8 |           6.1 |          1.9 |      2.0 |\\n|           5.4 |          3.4 |           1.7 |          0.2 |      0.0 |\\n|           6.2 |          2.2 |           4.5 |          1.5 |      1.0 |\\n,\\n :metamorph/mode :transform,\\n #uuid \\\"6d53a6fe-39b0-4a01-992f-725d2d7eb216\\\"\\n {:model-data SVC(),\\n  :options {:model-type :sklearn.classification/svc},\\n  :id #uuid \\\"9d0cc522-e33d-4166-9e7e-b51b31f1bf28\\\",\\n  :feature-columns\\n  [:sepal_length :sepal_width :petal_length :petal_width],\\n  :target-columns [:species],\\n  :target-categorical-maps\\n  {:species\\n   {:lookup-table {0 0, 1 1, 2 2},\\n    :src-column :species,\\n    :result-datatype :int16}}},\\n :scicloj.metamorph.ml/feature-ds Group: 0 [50 4]:\\n\\n| :sepal_length | :sepal_width | :petal_length | :petal_width |\\n|--------------:|-------------:|--------------:|-------------:|\\n|           6.4 |          2.8 |           5.6 |          2.2 |\\n|           6.1 |          2.8 |           4.7 |          1.2 |\\n|           5.0 |          2.3 |           3.3 |          1.0 |\\n|           6.2 |          2.9 |           4.3 |          1.3 |\\n|           5.9 |          3.0 |           5.1 |          1.8 |\\n|           6.7 |          3.0 |           5.2 |          2.3 |\\n|           4.7 |          3.2 |           1.3 |          0.2 |\\n|           4.9 |          2.5 |           4.5 |          1.7 |\\n|           6.0 |          2.2 |           4.0 |          1.0 |\\n|           5.7 |          4.4 |           1.5 |          0.4 |\\n|           6.1 |          2.9 |           4.7 |          1.4 |\\n|           6.0 |          2.7 |           5.1 |          1.6 |\\n|           5.2 |          3.5 |           1.5 |          0.2 |\\n|           5.6 |          2.5 |           3.9 |          1.1 |\\n|           4.5 |          2.3 |           1.3 |          0.3 |\\n|           6.1 |          3.0 |           4.6 |          1.4 |\\n|           5.8 |          2.7 |           5.1 |          1.9 |\\n|           5.5 |          2.4 |           3.7 |          1.0 |\\n|           4.8 |          3.4 |           1.9 |          0.2 |\\n|           7.2 |          3.2 |           6.0 |          1.8 |\\n|           5.7 |          3.0 |           4.2 |          1.2 |\\n|           7.7 |          3.8 |           6.7 |          2.2 |\\n|           7.4 |          2.8 |           6.1 |          1.9 |\\n|           5.4 |          3.4 |           1.7 |          0.2 |\\n|           6.2 |          2.2 |           4.5 |          1.5 |\\n,\\n :scicloj.metamorph.ml/target-ds Group: 0 [50 1]:\\n\\n| :species |\\n|---------:|\\n|        2 |\\n|        1 |\\n|        1 |\\n|        1 |\\n|        2 |\\n|        2 |\\n|        0 |\\n|        2 |\\n|        1 |\\n|        0 |\\n|        1 |\\n|        1 |\\n|        0 |\\n|        1 |\\n|        0 |\\n|        1 |\\n|        2 |\\n|        1 |\\n|        0 |\\n|        2 |\\n|        1 |\\n|        2 |\\n|        2 |\\n|        0 |\\n|        1 |\\n}\\n\"}]]}}"</script>
    <script src="gorilla-notes/js/compiled/main.js"></script>
</html>
